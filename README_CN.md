<p align="left">
    ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>&nbsp ï½œ &nbsp<a href="README_JA.md">æ—¥æœ¬èª</a> ï½œ &nbsp<a href="README_FR.md">FranÃ§ais</a> ï½œ &nbsp<a href="README_ES.md">EspaÃ±ol</a>
</p>
<br><br>

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg" width="400"/>
<p>
<br>

<p align="center">
        ğŸ¤— <a href="https://huggingface.co/Qwen">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href="https://modelscope.cn/organization/qwen">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href="https://arxiv.org/abs/2309.16609">Paper</a> &nbsp&nbsp ï½œ &nbsp&nbspğŸ–¥ï¸ <a href="https://modelscope.cn/studios/qwen/Qwen-72B-Chat-Demo/summary">Demo</a>
<br>
<a href="assets/wechat.png">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp&nbsp ï½œ  &nbsp&nbsp<a href="https://dashscope.aliyun.com">API</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://qianwen.aliyun.com">Web</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://apps.apple.com/cn/app/%E9%80%9A%E4%B9%89%E5%8D%83%E9%97%AE/id6466733523">APP</a>
</p>
<br><br>

> [!Important]
> Qwen2å·²å¼€ï¼Œæ¬¢è¿å…³æ³¨ï¼çœ‹è¿™é‡Œï¼š[QwenLM/Qwen2](https://github.com/QwenLM/Qwen2)
>
> Qwen2æ¨¡å‹ä»£ç å’Œç”¨æ³•ç›¸æ¯”æ­¤å‰ç‰ˆæœ¬æœ‰è¾ƒå¤§ä¸åŒï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨æ–°çš„repoè¿›è¡Œç»´æŠ¤ã€‚æ­¤repo ([QwenLM/Qwen](https://github.com/QwenLM/Qwen)) å·²åœæ­¢ä¸»è¦æ›´æ–°ç»´æŠ¤ã€‚

> [!Warning]
> è¯·å‹¿æ··ç”¨[Qwen](https://github.com/QwenLM/Qwen)å’Œ[Qwen2](https://github.com/QwenLM/Qwen2)ä»£ç ï¼Œä¸¤è€…å¹¶ä¸å…¼å®¹ã€‚
<br>

|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                        Qwen-Chat (Int8)                         |                                                            Qwen                                                            |
|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|
| 1.8B  |  <a href="https://modelscope.cn/models/qwen/Qwen-1_8B-Chat/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-1_8B-Chat">ğŸ¤—</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int4/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int4">ğŸ¤—</a>  | <a href="https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int8/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int8">ğŸ¤—</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-1_8B/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-1_8B">ğŸ¤—</a>  |
| 7B  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat">ğŸ¤—</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int4">ğŸ¤—</a>  | <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int8">ğŸ¤—</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B">ğŸ¤—</a>  |
| 14B | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int4">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int8">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B">ğŸ¤—</a> |
| 72B | <a href="https://modelscope.cn/models/qwen/Qwen-72B-Chat/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-72B-Chat">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int4/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-72B-Chat-Int4">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int8/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-72B-Chat-Int8">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-72B/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-72B">ğŸ¤—</a> |


  
æˆ‘ä»¬å¼€æºäº†**Qwen**ï¼ˆé€šä¹‰åƒé—®ï¼‰ç³»åˆ—å·¥ä½œï¼Œå½“å‰å¼€æºæ¨¡å‹çš„å‚æ•°è§„æ¨¡ä¸º18äº¿ï¼ˆ1.8Bï¼‰ã€70äº¿ï¼ˆ7Bï¼‰ã€140äº¿ï¼ˆ14Bï¼‰å’Œ720äº¿ï¼ˆ72Bï¼‰ã€‚æœ¬æ¬¡å¼€æºåŒ…æ‹¬åŸºç¡€æ¨¡å‹**Qwen**ï¼Œå³**Qwen-1.8B**ã€**Qwen-7B**ã€**Qwen-14B**ã€**Qwen-72B**ï¼Œä»¥åŠå¯¹è¯æ¨¡å‹**Qwen-Chat**ï¼Œå³**Qwen-1.8B-Chat**ã€**Qwen-7B-Chat**ã€**Qwen-14B-Chat**å’Œ**Qwen-72B-Chat**ã€‚æ¨¡å‹é“¾æ¥åœ¨è¡¨æ ¼ä¸­ï¼Œè¯·ç‚¹å‡»äº†è§£è¯¦æƒ…ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å…¬å¼€äº†æˆ‘ä»¬çš„<b><a href="https://arxiv.org/abs/2309.16609">æŠ€æœ¯æŠ¥å‘Š</a></b>ï¼Œè¯·ç‚¹å‡»ä¸Šæ–¹è®ºæ–‡é“¾æ¥æŸ¥çœ‹ã€‚

å½“å‰åŸºç¡€æ¨¡å‹å·²ç»ç¨³å®šè®­ç»ƒäº†å¤§è§„æ¨¡é«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„æ•°æ®ï¼Œè¦†ç›–å¤šè¯­è¨€ï¼ˆå½“å‰ä»¥ä¸­æ–‡å’Œè‹±æ–‡ä¸ºä¸»ï¼‰ï¼Œæ€»é‡é«˜è¾¾3ä¸‡äº¿tokenã€‚åœ¨ç›¸å…³åŸºå‡†è¯„æµ‹ä¸­ï¼ŒQwenç³»åˆ—æ¨¡å‹æ‹¿å‡ºéå¸¸æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼Œæ˜¾è‘—è¶…å‡ºåŒè§„æ¨¡æ¨¡å‹å¹¶ç´§è¿½ä¸€ç³»åˆ—æœ€å¼ºçš„é—­æºæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨SFTå’ŒRLHFæŠ€æœ¯å®ç°å¯¹é½ï¼Œä»åŸºåº§æ¨¡å‹è®­ç»ƒå¾—åˆ°å¯¹è¯æ¨¡å‹ã€‚Qwen-Chatå…·å¤‡èŠå¤©ã€æ–‡å­—åˆ›ä½œã€æ‘˜è¦ã€ä¿¡æ¯æŠ½å–ã€ç¿»è¯‘ç­‰èƒ½åŠ›ï¼ŒåŒæ—¶è¿˜å…·å¤‡ä¸€å®šçš„ä»£ç ç”Ÿæˆå’Œç®€å•æ•°å­¦æ¨ç†çš„èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬é’ˆå¯¹LLMå¯¹æ¥å¤–éƒ¨ç³»ç»Ÿç­‰æ–¹é¢é’ˆå¯¹æ€§åœ°åšäº†ä¼˜åŒ–ï¼Œå½“å‰å…·å¤‡è¾ƒå¼ºçš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œä»¥åŠæœ€è¿‘å¤‡å—å…³æ³¨çš„Code Interpreterçš„èƒ½åŠ›å’Œæ‰®æ¼”Agentçš„èƒ½åŠ›ã€‚æˆ‘ä»¬å°†å„ä¸ªå¤§å°æ¨¡å‹çš„ç‰¹ç‚¹åˆ—åˆ°äº†ä¸‹è¡¨ã€‚

| æ¨¡å‹        |   å¼€æºæ—¥æœŸ   | æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ | System Promptå¼ºåŒ– | é¢„è®­ç»ƒtokenæ•° | å¾®è°ƒï¼ˆQ-Loraï¼‰æœ€å°GPUç”¨é‡ | ç”Ÿæˆ2048ä¸ªtokençš„æœ€å°æ˜¾å­˜å ç”¨ï¼ˆInt4ï¼‰ | å·¥å…·è°ƒç”¨ |
|:----------|:--------:|:-------:|:---------------:|:---------:|:-----------------:|:-------------------:|:----:|
| Qwen-1.8B | 23.11.30 |   32K   |        âœ…        |   2.2T    |       5.8GB       |        2.9GB        |  âœ…   |  
| Qwen-7B   | 23.08.03 |   32K   |        â        |   2.4T    |      11.5GB       |        8.2GB        |  âœ…   |   
| Qwen-14B  | 23.09.25 |   8K    |        â        |   3.0T    |      18.7GB       |       13.0GB        |  âœ…   |
| Qwen-72B  | 23.11.30 |   32K   |        âœ…        |   3.0T    |      61.4GB       |       48.9GB        |  âœ…   |   

  
åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œä½ å¯ä»¥äº†è§£åˆ°ä»¥ä¸‹å†…å®¹

* å¿«é€Ÿä¸Šæ‰‹Qwen-Chatæ•™ç¨‹ï¼Œç©è½¬å¤§æ¨¡å‹æ¨ç†
* é‡åŒ–æ¨¡å‹ç›¸å…³ç»†èŠ‚ï¼ŒåŒ…æ‹¬GPTQå’ŒKV cacheé‡åŒ–
* æ¨ç†æ€§èƒ½æ•°æ®ï¼ŒåŒ…æ‹¬æ¨ç†é€Ÿåº¦å’Œæ˜¾å­˜å ç”¨
* å¾®è°ƒçš„æ•™ç¨‹ï¼Œå¸®ä½ å®ç°å…¨å‚æ•°å¾®è°ƒã€LoRAä»¥åŠQ-LoRA
* éƒ¨ç½²æ•™ç¨‹ï¼Œä»¥vLLMå’ŒFastChatä¸ºä¾‹
* æ­å»ºDemoçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬WebUIå’ŒCLI Demo
* æ­å»ºAPIçš„æ–¹æ³•ï¼Œæˆ‘ä»¬æä¾›çš„ç¤ºä¾‹ä¸ºOpenAIé£æ ¼çš„API
* æ›´å¤šå…³äºQwenåœ¨å·¥å…·è°ƒç”¨ã€Code Interpreterã€Agentæ–¹é¢çš„å†…å®¹
* é•¿åºåˆ—ç†è§£èƒ½åŠ›åŠè¯„æµ‹
* ä½¿ç”¨åè®®
* ...

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·ä¼˜å…ˆè€ƒè™‘æŸ¥è¯¢[FAQ](FAQ.md)ã€‚å¦‚ä»æœªè§£å†³ï¼Œéšæ—¶æå‡ºissueï¼ˆä½†å»ºè®®ä½¿ç”¨è‹±è¯­æˆ–æä¾›ç¿»è¯‘ï¼Œæœ‰åŠ©äºå¸®åŠ©æ›´å¤šç”¨æˆ·ï¼‰ã€‚å¦‚æœæƒ³å¸®åŠ©æˆ‘ä»¬æå‡ï¼Œæ¬¢è¿æäº¤Pull Requestsï¼

æƒ³å’Œæˆ‘ä»¬ä¸€èµ·è®¨è®ºå’ŒèŠå¤©çš„è¯ï¼Œèµ¶ç´§åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤å’ŒDiscord serverï¼ˆå…¥å£è§æ–‡æ¡£å¼€å¤´éƒ¨åˆ†ï¼‰ï¼
<br><br>

## æ–°é—»

* 2023.11.30 ğŸ”¥ æˆ‘ä»¬æ¨å‡º **Qwen-72B** å’Œ **Qwen-72B-Chat**ï¼Œå®ƒä»¬åœ¨ 3T tokensä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶æ”¯æŒ 32k ä¸Šä¸‹æ–‡ã€‚åŒæ—¶ä¹Ÿå‘å¸ƒäº† **Qwen-1.8B** å’Œ **Qwen-1.8B-Chat**ã€‚æˆ‘ä»¬è¿˜å¢å¼ºäº† Qwen-72B-Chat å’Œ Qwen-1.8B-Chat çš„ç³»ç»ŸæŒ‡ä»¤ï¼ˆSystem Promptï¼‰åŠŸèƒ½ï¼Œè¯·å‚é˜…[ç¤ºä¾‹æ–‡æ¡£](examples/system_prompt.md)ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯¹**æ˜‡è…¾910**ä»¥åŠ**æµ·å…‰DCU**å®ç°äº†æ¨ç†çš„æ”¯æŒï¼Œè¯¦æƒ…è¯·æŸ¥çœ‹`ascend-support`åŠ`dcu-support`æ–‡ä»¶å¤¹ã€‚
* 2023å¹´10æœˆ17æ—¥ æˆ‘ä»¬æ¨å‡ºäº†Int8é‡åŒ–æ¨¡å‹**Qwen-7B-Chat-Int8**å’Œ**Qwen-14B-Chat-Int8**ã€‚
* 2023å¹´9æœˆ25æ—¥ åœ¨é­”æ­ç¤¾åŒºï¼ˆModelScopeï¼‰å’ŒHugging Faceæ¨å‡º**Qwen-14B**å’Œ**Qwen-14B-Chat**æ¨¡å‹ï¼Œå¹¶å¼€æº [qwen.cpp](https://github.com/QwenLM/qwen.cpp) å’Œ [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent)ã€‚**Qwen-7B**å’Œ**Qwen-7B-Chat**çš„ä»£ç å’Œæ¨¡å‹ä¹ŸåŒæ­¥å¾—åˆ°æ›´æ–°ã€‚**è¯·ä½¿ç”¨æœ€æ–°çš„ä»£ç å’Œæ¨¡å‹ï¼**
    - ç›¸æ¯”åŸç‰ˆQwen-7Bï¼Œæ–°ç‰ˆç”¨äº†æ›´å¤šè®­ç»ƒæ•°æ®ï¼ˆä»2.2Tå¢åŠ åˆ°2.4T tokensï¼‰ï¼Œåºåˆ—é•¿åº¦ä»2048æ‰©å±•è‡³8192ã€‚æ•´ä½“ä¸­æ–‡èƒ½åŠ›ä»¥åŠä»£ç èƒ½åŠ›å‡æœ‰æ‰€æå‡ã€‚
* 2023å¹´9æœˆ12æ—¥ æ”¯æŒQwen-7Bå’ŒQwen-7B-Chatçš„å¾®è°ƒï¼Œå…¶ä¸­åŒ…æ‹¬å…¨å‚æ•°å¾®è°ƒã€LoRAä»¥åŠQ-LoRAã€‚
* 2023å¹´8æœˆ21æ—¥ å‘å¸ƒQwen-7B-Chatçš„Int4é‡åŒ–æ¨¡å‹ï¼ŒQwen-7B-Chat-Int4ã€‚è¯¥æ¨¡å‹æ˜¾å­˜å ç”¨ä½ï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åŠç²¾åº¦æ¨¡å‹æ˜¾è‘—æå‡ï¼Œåœ¨åŸºå‡†è¯„æµ‹ä¸Šæ•ˆæœæŸå¤±è¾ƒå°ã€‚
* 2023å¹´8æœˆ3æ—¥ åœ¨é­”æ­ç¤¾åŒºï¼ˆModelScopeï¼‰å’ŒHugging FaceåŒæ­¥æ¨å‡ºQwen-7Bå’ŒQwen-7B-Chatæ¨¡å‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æŠ€æœ¯å¤‡å¿˜å½•ï¼Œä»‹ç»äº†ç›¸å…³çš„è®­ç»ƒç»†èŠ‚å’Œæ¨¡å‹è¡¨ç°ã€‚
<br>

## è¯„æµ‹è¡¨ç°

Qwenç³»åˆ—æ¨¡å‹ç›¸æ¯”åŒè§„æ¨¡æ¨¡å‹å‡å®ç°äº†æ•ˆæœçš„æ˜¾è‘—æå‡ã€‚æˆ‘ä»¬è¯„æµ‹çš„æ•°æ®é›†åŒ…æ‹¬MMLUã€C-Evalã€ GSM8Kã€ MATHã€HumanEvalã€MBPPã€BBHç­‰æ•°æ®é›†ï¼Œè€ƒå¯Ÿçš„èƒ½åŠ›åŒ…æ‹¬è‡ªç„¶è¯­è¨€ç†è§£ã€çŸ¥è¯†ã€æ•°å­¦è®¡ç®—å’Œæ¨ç†ã€ä»£ç ç”Ÿæˆã€é€»è¾‘æ¨ç†ç­‰ã€‚Qwen-72Båœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šå‡è¶…è¶Šäº†LLaMA2-70Bçš„æ€§èƒ½ï¼ŒåŒæ—¶åœ¨10é¡¹ä»»åŠ¡ä¸­çš„7é¡¹ä»»åŠ¡ä¸­è¶…è¶ŠGPT-3.5.

<p align="left">
    <img src="assets/radar_72b.jpg" width="600"/>
<p>
<br>

| Model              |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP   |   BBH    |  CMMLU   |
|:-------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:|
|                    |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot  |  3-shot  |  5-shot  |
| LLaMA2-7B          |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8   |   38.2   |   31.8   |
| LLaMA2-13B         |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3   |   45.6   |   38.4   |
| LLaMA2-34B         |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0   |   44.1   |    -     |
| ChatGLM2-6B        |   47.9   |   51.7   |   32.4   |   6.5    |     -     |    -     |   33.7   |    -     |
| InternLM-7B        |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0   |   37.0   |   51.8   |
| InternLM-20B       |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6   |   52.5   |   59.0   |
| Baichuan2-7B       |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2   |   41.6   |   57.1   |
| Baichuan2-13B      |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2   |   49.0   |   62.0   |
| Yi-34B      	  	 |   76.3   |   81.8   |   67.9   |   15.9   |   26.2    |   38.2   |   66.4   |   82.6   |
| XVERSE-65B      	 |   70.8   |   68.6   |   60.3   |   -      |   26.3    |   -      |  -       |   -      |
| **Qwen-1.8B**      |   45.3   |   56.1   |   32.3   |   2.3    |   15.2    |   14.2   |   22.3   |   52.1   |
| **Qwen-7B**        |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6   |   45.0   |   62.2   |
| **Qwen-14B**       |   66.3   |   72.1   |   61.3   |   24.8   |   32.3    |   40.8   |   53.4   |   71.0   |
| **Qwen-72B**       | **77.4** | **83.3** | **78.9** | **35.2** | **35.4**  | **52.2** | **67.7** | **83.6** |


å¯¹äºä»¥ä¸Šæ‰€æœ‰å¯¹æ¯”æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ—å‡ºäº†å…¶å®˜æ–¹æ±‡æŠ¥ç»“æœä¸[OpenCompass](https://opencompass.org.cn/leaderboard-llm)ç»“æœä¹‹é—´çš„æœ€ä½³åˆ†æ•°ã€‚

æ›´å¤šçš„å®éªŒç»“æœå’Œç»†èŠ‚è¯·æŸ¥çœ‹æˆ‘ä»¬çš„æŠ€æœ¯å¤‡å¿˜å½•ã€‚ç‚¹å‡»[è¿™é‡Œ](https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf)ã€‚
<br><br>

## è¦æ±‚

* python 3.8åŠä»¥ä¸Šç‰ˆæœ¬
* pytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œæ¨è2.0åŠä»¥ä¸Šç‰ˆæœ¬
* transformers 4.32åŠä»¥ä¸Šç‰ˆæœ¬
* å»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·ã€flash-attentionç”¨æˆ·ç­‰éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰
<br>

## å¿«é€Ÿä½¿ç”¨

æˆ‘ä»¬æä¾›ç®€å•çš„ç¤ºä¾‹æ¥è¯´æ˜å¦‚ä½•åˆ©ç”¨ğŸ¤– ModelScopeå’ŒğŸ¤— Transformerså¿«é€Ÿä½¿ç”¨Qwen-7Bå’ŒQwen-7B-Chatã€‚

ä½ å¯ä»¥ä½¿ç”¨æˆ‘ä»¬é¢„æ„å»ºå¥½çš„Dockeré•œåƒï¼Œçœå»å¤§éƒ¨åˆ†é…ç½®ç¯å¢ƒçš„æ“ä½œï¼Œè¯¦æƒ…è§[â€œä½¿ç”¨é¢„æ„å»ºçš„dockeré•œåƒâ€](#-ä½¿ç”¨é¢„æ„å»ºçš„dockeré•œåƒ)ä¸€èŠ‚ã€‚

å¦‚ä¸ä½¿ç”¨Dockerï¼Œè¯·ç¡®ä¿ä½ å·²ç»é…ç½®å¥½ç¯å¢ƒå¹¶å®‰è£…å¥½ç›¸å…³çš„ä»£ç åŒ…ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œç¡®ä¿ä½ æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œç„¶åå®‰è£…ç›¸å…³çš„ä¾èµ–åº“ã€‚

```bash
pip install -r requirements.txt
```

å¦‚æœä½ çš„æ˜¾å¡æ”¯æŒfp16æˆ–bf16ç²¾åº¦ï¼Œæˆ‘ä»¬è¿˜æ¨èå®‰è£…[flash-attention](https://github.com/Dao-AILab/flash-attention)ï¼ˆ**å½“å‰å·²æ”¯æŒflash attention 2**ï¼‰æ¥æé«˜ä½ çš„è¿è¡Œæ•ˆç‡ä»¥åŠé™ä½æ˜¾å­˜å ç”¨ã€‚(**flash-attentionåªæ˜¯å¯é€‰é¡¹ï¼Œä¸å®‰è£…ä¹Ÿå¯æ­£å¸¸è¿è¡Œè¯¥é¡¹ç›®**)

```bash
git clone https://github.com/Dao-AILab/flash-attention
cd flash-attention && pip install .
# ä¸‹æ–¹å®‰è£…å¯é€‰ï¼Œå®‰è£…å¯èƒ½æ¯”è¾ƒç¼“æ…¢ã€‚
# pip install csrc/layer_norm
# å¦‚æœflash-attnç‰ˆæœ¬é«˜äº2.1.1ï¼Œä¸‹æ–¹æ— éœ€å®‰è£…ã€‚
# pip install csrc/rotary
```

æ¥ä¸‹æ¥ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨Transformersæˆ–è€…ModelScopeæ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚

### ğŸ¤— Transformers

å¦‚å¸Œæœ›ä½¿ç”¨Qwen-chatè¿›è¡Œæ¨ç†ï¼Œæ‰€éœ€è¦å†™çš„åªæ˜¯å¦‚ä¸‹æ‰€ç¤ºçš„æ•°è¡Œä»£ç ã€‚**è¯·ç¡®ä¿ä½ ä½¿ç”¨çš„æ˜¯æœ€æ–°ä»£ç ï¼Œå¹¶æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹åç§°å’Œè·¯å¾„ï¼Œå¦‚`Qwen/Qwen-7B-Chat`å’Œ`Qwen/Qwen-14B-Chat`**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# å¯é€‰çš„æ¨¡å‹åŒ…æ‹¬: "Qwen/Qwen-7B-Chat", "Qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤ä½¿ç”¨è‡ªåŠ¨æ¨¡å¼ï¼Œæ ¹æ®è®¾å¤‡è‡ªåŠ¨é€‰æ‹©ç²¾åº¦
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True).eval()

# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# ç¬¬ä¸€è½®å¯¹è¯
response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚

# ç¬¬äºŒè½®å¯¹è¯
response, history = model.chat(tokenizer, "ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚", history=history)
print(response)
# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚
# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚
# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚
# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚
# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚
# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚

# ç¬¬ä¸‰è½®å¯¹è¯
response, history = model.chat(tokenizer, "ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜", history=history)
print(response)
# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹
```

è¿è¡ŒQwenåŒæ ·éå¸¸ç®€å•ã€‚

<details>
  <summary>è¿è¡ŒQwen</summary>

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# å¯é€‰çš„æ¨¡å‹åŒ…æ‹¬: "Qwen/Qwen-7B", "Qwen/Qwen-14B"
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)

# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤ä½¿ç”¨è‡ªåŠ¨æ¨¡å¼ï¼Œæ ¹æ®è®¾å¤‡è‡ªåŠ¨é€‰æ‹©ç²¾åº¦
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True).eval()

# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)

inputs = tokenizer('è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯', return_tensors='pt')
inputs = inputs.to(model.device)
pred = model.generate(**inputs)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))
# è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯äºšçš„æ–¯äºšè´å·´ï¼ˆAddis Ababaï¼‰...
```

</details>

<p id="DownloadModel">
è‹¥åœ¨ä½¿ç”¨ä¸Šè¿°ä»£ç æ—¶ç”±äºå„ç§åŸå› æ— æ³•ä» HuggingFace æ‹‰å–æ¨¡å‹å’Œä»£ç ï¼Œå¯ä»¥å…ˆä» ModelScope ä¸‹è½½æ¨¡å‹åŠä»£ç è‡³æœ¬åœ°ï¼Œå†ä»æœ¬åœ°åŠ è½½æ¨¡å‹ï¼š
</p>

```python
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer

# Downloading model checkpoint to a local dir model_dir
# model_dir = snapshot_download('qwen/Qwen-7B')
# model_dir = snapshot_download('qwen/Qwen-7B-Chat')
# model_dir = snapshot_download('qwen/Qwen-14B')
model_dir = snapshot_download('qwen/Qwen-14B-Chat')

# Loading local checkpoints
# trust_remote_code is still set as True since we still load codes from local dir instead of transformers
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    device_map="auto",
    trust_remote_code=True
).eval()
```

### ğŸ¤– ModelScope

é­”æ­ï¼ˆModelScopeï¼‰æ˜¯å¼€æºçš„æ¨¡å‹å³æœåŠ¡å…±äº«å¹³å°ï¼Œä¸ºæ³›AIå¼€å‘è€…æä¾›çµæ´»ã€æ˜“ç”¨ã€ä½æˆæœ¬çš„ä¸€ç«™å¼æ¨¡å‹æœåŠ¡äº§å“ã€‚ä½¿ç”¨ModelScopeåŒæ ·éå¸¸ç®€å•ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from modelscope import AutoModelForCausalLM, AutoTokenizer
from modelscope import GenerationConfig

# å¯é€‰çš„æ¨¡å‹åŒ…æ‹¬: "qwen/Qwen-7B-Chat", "qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("qwen/Qwen-7B-Chat", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚

response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
response, history = model.chat(tokenizer, "æµ™æ±Ÿçš„çœä¼šåœ¨å“ªé‡Œï¼Ÿ", history=history) 
print(response)
response, history = model.chat(tokenizer, "å®ƒæœ‰ä»€ä¹ˆå¥½ç©çš„æ™¯ç‚¹", history=history)
print(response)
```

### Batchæ¨ç†
åƒé—®æ”¯æŒbatchæ‰¹é‡æ¨ç†ã€‚åœ¨å¼€å¯flash-attentionçš„çŠ¶æ€ä¸‹ï¼Œä½¿ç”¨batchæ¨ç†å¯ä»¥çº¦40%çš„æé€Ÿã€‚ç¤ºä¾‹ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import GenerationConfig
from qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids

tokenizer = AutoTokenizer.from_pretrained(
    './',
    pad_token='<|extra_0|>',
    eos_token='<|endoftext|>',
    padding_side='left',
    trust_remote_code=True
)
model = AutoModelForCausalLM.from_pretrained(
    './',
    pad_token_id=tokenizer.pad_token_id,
    device_map="auto",
    trust_remote_code=True
).eval()
model.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)

all_raw_text = ["æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚", "ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹", "æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°"]
batch_raw_text = []
for q in all_raw_text:
    raw_text, _ = make_context(
        tokenizer,
        q,
        system="You are a helpful assistant.",
        max_window_size=model.generation_config.max_window_size,
        chat_format=model.generation_config.chat_format,
    )
    batch_raw_text.append(raw_text)

batch_input_ids = tokenizer(batch_raw_text, padding='longest')
batch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)
batch_out_ids = model.generate(
    batch_input_ids,
    return_dict_in_generate=False,
    generation_config=model.generation_config
)
padding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]

batch_response = [
    decode_tokens(
        batch_out_ids[i][padding_lens[i]:],
        tokenizer,
        raw_text_len=len(batch_raw_text[i]),
        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),
        chat_format="chatml",
        verbose=False,
        errors='replace'
    ) for i in range(len(all_raw_text))
]
print(batch_response)

response, _ = model.chat(tokenizer, "æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚", history=None)
print(response)

response, _ = model.chat(tokenizer, "ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹", history=None)
print(response)

response, _ = model.chat(tokenizer, "æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°", history=None)
print(response)
```

### CPU

æˆ‘ä»¬æ¨èä½ ä½¿ç”¨ [qwen.cpp](https://github.com/QwenLM/qwen.cpp) æ¥å®ç°CPUéƒ¨ç½²å’Œæ¨ç†ã€‚qwen.cppæ˜¯Qwenå’Œtiktokençš„C++å®ç°ã€‚ä½ å¯ä»¥ç‚¹å‡»é“¾æ¥è¿›å…¥repoäº†è§£è¯¦æƒ…ã€‚

å½“ç„¶ï¼Œç›´æ¥åœ¨CPUä¸Šè¿è¡Œæ¨¡å‹ä¹Ÿæ˜¯å¯ä»¥çš„ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
```

ä½†æ˜¯ï¼Œè¿™æ ·çš„æ¨ç†æ•ˆç‡å¤§æ¦‚ç‡ä¼šéå¸¸ä½ã€‚

### å¤šGPU

å¦‚æœä½ é‡åˆ°æ˜¾å­˜ä¸è¶³çš„é—®é¢˜è€Œå¸Œæœ›ä½¿ç”¨å¤šå¼ GPUè¿›è¡Œæ¨ç†ï¼Œå¯ä»¥ä½¿ç”¨ä¸Šè¿°çš„é»˜è®¤çš„ä½¿ç”¨æ–¹æ³•è¯»å–æ¨¡å‹ã€‚æ­¤å‰æä¾›çš„è„šæœ¬`utils.py`å·²åœæ­¢ç»´æŠ¤ã€‚

å°½ç®¡è¿™ä¸ªæ–¹æ³•å¾ˆç®€å•ï¼Œä½†å®ƒçš„æ•ˆç‡ç›¸å¯¹è¾ƒä½ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨vLLMå’ŒFastChatå¹¶è¯·é˜…è¯»éƒ¨ç½²ç« èŠ‚ã€‚

### x86 å¹³å°
åœ¨ é…·ç¿â„¢/è‡³å¼ºÂ® å¯æ‰©å±•å¤„ç†å™¨æˆ– Arcâ„¢ GPU ä¸Šéƒ¨ç½²é‡åŒ–æ¨¡å‹æ—¶ï¼Œå»ºè®®ä½¿ç”¨ [OpenVINOâ„¢ Toolkit](https://docs.openvino.ai/2023.3/gen_ai_guide.html) ä»¥å……åˆ†åˆ©ç”¨ç¡¬ä»¶ï¼Œå®ç°æ›´å¥½çš„æ¨ç†æ€§èƒ½ã€‚æ‚¨å¯ä»¥å®‰è£…å¹¶è¿è¡Œæ­¤[example notebook](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot)ã€‚ç›¸å…³é—®é¢˜ï¼Œæ‚¨å¯åœ¨ [OpenVINO repo](https://github.com/openvinotoolkit/openvino_notebooks/issues)ä¸­æäº¤ã€‚


### é˜¿é‡Œäº‘çµç§¯ï¼ˆDashScopeï¼‰APIæœåŠ¡
æœ€ç®€å•çš„ä½¿ç”¨Qwenæ¨¡å‹APIæœåŠ¡çš„æ–¹æ³•å°±æ˜¯é€šè¿‡DashScopeï¼ˆé˜¿é‡Œäº‘çµç§¯APIæ¨¡å‹æœåŠ¡ï¼‰ã€‚æˆ‘ä»¬æä¾›äº†ç®€å•ä»‹ç»è¯´æ˜ä½¿ç”¨æ–¹æ³•ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†è‡ªå·±éƒ¨ç½²OpenAIæ ¼å¼çš„APIçš„æ–¹æ³•ã€‚

DashScopeæ˜¯é˜¿é‡Œäº‘æä¾›çš„å¤§è¯­è¨€æ¨¡å‹çš„APIæœåŠ¡ï¼Œç›®å‰æ”¯æŒQwenã€‚ä½†è¯·æ³¨æ„ï¼Œç›®å‰æä¾›æœåŠ¡çš„Qwenæ¨¡å‹ä¸ºå†…éƒ¨æ¨¡å‹ï¼Œæš‚æ— æ›´å¤šå…·ä½“ç»†èŠ‚å¯¹å¤–é€éœ²ã€‚æ¨¡å‹æœåŠ¡åŒ…æ‹¬`qwen-turbo`ã€`qwen-plus`å’Œ`qwen-max`ï¼Œ`qwen-turbo`é€Ÿåº¦æ›´å¿«ï¼Œ`qwen-plus`æ•ˆæœæ›´ä¼˜ï¼Œ`qwen-max`æ˜¯æœ€æ–°å‘å¸ƒçš„åƒäº¿çº§é€šä¹‰åƒé—®2.0æ¨¡å‹ã€‚è¯¦æƒ…è¯·æŸ¥çœ‹[æ–‡æ¡£](https://dashscope.aliyun.com)ã€‚

è¯·é¦–å…ˆå‰å¾€[å®˜ç½‘](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn)å¼€é€šDashScopeï¼Œè·å¾—API Keyï¼ˆAKï¼‰ã€‚å»ºè®®é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®AKï¼š
```bash
export DASHSCOPE_API_KEY="YOUR_DASHSCOPE_API_KEY"
```
éšåå®‰è£…ç›¸å…³ä»£ç åŒ…ï¼Œç‚¹å‡»[æ­¤å¤„](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk)æŸ¥çœ‹å®‰è£…æ–‡æ¡£ã€‚å¦‚ä½¿ç”¨pythonï¼Œåˆ™ç›´æ¥é€šè¿‡pipå®‰è£…ï¼š
```bash
pip install dashscope
```
å¦‚å®‰è£…JAVA SDKï¼Œåˆ™é€šè¿‡å¦‚ä¸‹å‘½ä»¤å®‰è£…ï¼š
```xml
<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->
<dependency>
    <groupId>com.alibaba</groupId>
    <artifactId>dashscope-sdk-java</artifactId>
    <version>the-latest-version</version>
</dependency>
```
æœ€ç®€å•çš„ä½¿ç”¨æ–¹æ³•å°±æ˜¯é€šè¿‡messagesè°ƒç”¨ï¼Œç”¨æ³•ç±»ä¼¼OpenAI APIã€‚ç¤ºä¾‹å¦‚ä¸‹ï¼š
```python
import random
from http import HTTPStatus
from dashscope import Generation


def call_with_messages():
    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},
                {'role': 'user', 'content': 'å¦‚ä½•åšè¥¿çº¢æŸ¿é¸¡è›‹ï¼Ÿ'}]
    gen = Generation()
    response = gen.call(
        Generation.Models.qwen_turbo,
        messages=messages,
        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set
        result_format='message',  # set the result to be "message" format.
    )
    return response


if __name__ == '__main__':
    response = call_with_messages()
    if response.status_code == HTTPStatus.OK:
        print(response)
    else:
        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (
            response.request_id, response.status_code,
            response.code, response.message
        ))
```
æ›´å¤šç”¨æ³•è¯·æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£äº†è§£è¯¦æƒ…ã€‚
<br><br>


## é‡åŒ–

### GPTQ

æˆ‘ä»¬æä¾›äº†åŸºäº[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)çš„é‡åŒ–æ–¹æ¡ˆï¼Œå¹¶å¼€æºäº†Int4å’ŒInt8é‡åŒ–æ¨¡å‹ã€‚é‡åŒ–æ¨¡å‹çš„æ•ˆæœæŸå¤±å¾ˆå°ï¼Œä½†èƒ½æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨å¹¶æå‡æ¨ç†é€Ÿåº¦ã€‚

ä»¥ä¸‹æˆ‘ä»¬æä¾›ç¤ºä¾‹è¯´æ˜å¦‚ä½•ä½¿ç”¨Int4é‡åŒ–æ¨¡å‹ã€‚åœ¨å¼€å§‹ä½¿ç”¨å‰ï¼Œè¯·å…ˆä¿è¯æ»¡è¶³è¦æ±‚ï¼ˆå¦‚torch 2.0åŠä»¥ä¸Šï¼Œtransformersç‰ˆæœ¬ä¸º4.32.0åŠä»¥ä¸Šï¼Œç­‰ç­‰ï¼‰ï¼Œå¹¶å®‰è£…æ‰€éœ€å®‰è£…åŒ…ï¼š

```bash
pip install auto-gptq optimum
```

å¦‚å®‰è£…`auto-gptq`é‡åˆ°é—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨åˆ°å®˜æ–¹[repo](https://github.com/PanQiWei/AutoGPTQ)æœç´¢åˆé€‚çš„wheelã€‚

> æ³¨æ„ï¼šé¢„ç¼–è¯‘çš„`auto-gptq`ç‰ˆæœ¬å¯¹`torch`ç‰ˆæœ¬åŠå…¶CUDAç‰ˆæœ¬è¦æ±‚ä¸¥æ ¼ã€‚åŒæ—¶ï¼Œç”±äº
> å…¶è¿‘æœŸæ›´æ–°ï¼Œä½ å¯èƒ½ä¼šé‡åˆ°`transformers`ã€`optimum`æˆ–`peft`æŠ›å‡ºçš„ç‰ˆæœ¬é”™è¯¯ã€‚
> æˆ‘ä»¬å»ºè®®ä½¿ç”¨ç¬¦åˆä»¥ä¸‹è¦æ±‚çš„æœ€æ–°ç‰ˆæœ¬ï¼š
> - torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1
> - torch>=2.0,<2.1 auto-gptq<0.5.0 transformers<4.35.0 optimum<1.14.0 peft>=0.5.0,<0.6.0

éšåå³å¯ä½¿ç”¨å’Œä¸Šè¿°ä¸€è‡´çš„ç”¨æ³•è°ƒç”¨é‡åŒ–æ¨¡å‹ï¼š

```python
# å¯é€‰æ¨¡å‹åŒ…æ‹¬ï¼š"Qwen/Qwen-7B-Chat-Int4", "Qwen/Qwen-14B-Chat-Int4"
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat-Int4",
    device_map="auto",
    trust_remote_code=True
).eval()
response, history = model.chat(tokenizer, "Hi", history=None)
```

æˆ‘ä»¬å¯¹BF16ï¼ŒInt8å’ŒInt4æ¨¡å‹åœ¨åŸºå‡†è¯„æµ‹ä¸Šåšäº†æµ‹è¯•ï¼Œå‘ç°é‡åŒ–æ¨¡å‹æ•ˆæœæŸå¤±è¾ƒå°ï¼Œç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |
|----------------------|:----:|:-----------:|:-----:|:---------:|
| Qwen-1.8B-Chat (BF16)| 43.3 |    55.6     | 33.7  |   26.2    |
| Qwen-1.8B-Chat (Int8)| 43.1 |    55.8     | 33.0  |   27.4    |
| Qwen-1.8B-Chat (Int4)| 42.9 |    52.8     | 31.2  |   25.0    |
| Qwen-7B-Chat (BF16)  | 55.8 |    59.7     | 50.3  |   37.2    |
| Qwen-7B-Chat (Int8)  | 55.4 |    59.4     | 48.3  |   34.8    |
| Qwen-7B-Chat (Int4)  | 55.1 |    59.2     | 49.7  |   29.9    |
| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 60.1  |   43.9    |
| Qwen-14B-Chat (Int8) | 63.6 |    68.6     | 60.0  |   48.2    |
| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |
| Qwen-72B-Chat (BF16) | 74.4 |    80.1     | 76.4  |   64.6    |
| Qwen-72B-Chat (Int8) | 73.5 |    80.1     | 73.5  |   62.2    |
| Qwen-72B-Chat (Int4) | 73.4 |    80.1     | 75.3  |   61.6    |
<br>


### KV cacheé‡åŒ–

> æ³¨æ„ï¼šç”±äºHugging Faceçš„å†…éƒ¨å®ç°ï¼Œæœ¬åŠŸèƒ½çš„æ”¯æŒæ–‡ä»¶`cache_autogptq_cuda_256.cpp`ä¸`cache_autogptq_cuda_kernel_256.cu`å¯èƒ½æ²¡è¢«ä¸‹è½½ã€‚å¦‚éœ€å¼€å¯ä½¿ç”¨ï¼Œè¯·æ‰‹åŠ¨ä»ç›¸å…³ä½ç½®ä¸‹è½½ï¼Œå¹¶æ”¾ç½®åˆ°ç›¸åº”æ–‡ä»¶ä¸­ã€‚

åœ¨æ¨¡å‹æ¨ç†æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸­é—´ç»“æœkeyä»¥åŠvalueçš„å€¼é‡åŒ–åå‹ç¼©å­˜å‚¨ï¼Œè¿™æ ·ä¾¿å¯ä»¥åœ¨ç›¸åŒçš„å¡ä¸Šå­˜å‚¨æ›´å¤šçš„keyä»¥åŠvalueï¼Œå¢åŠ æ ·æœ¬ååã€‚

æˆ‘ä»¬åœ¨`config.json`é‡Œæä¾›äº†`use_cache_quantization`å’Œ`use_cache_kernel`ä¸¤ä¸ªå‚æ•°æ¥æ§åˆ¶æ˜¯å¦å¯ç”¨KV cacheé‡åŒ–ï¼Œå…·ä½“ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š
```python
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat",
     device_map="auto",
     trust_remote_code=True,
     use_cache_quantization=True,
     use_cache_kernel=True,
     use_flash_attn=False
)
```
æ³¨æ„ï¼šå½“å‰è¯¥åŠŸèƒ½ä¸æ”¯æŒä¸flash attentionåŒæ—¶å¼€å¯ï¼Œå¦‚æœä½ å¼€äº†KV cacheé‡åŒ–çš„åŒæ—¶åˆå¼€äº†flash attentionï¼ˆ`use_flash_attn=True`ï¼Œ `use_cache_quantization=True`, `use_cache_kernel=True`ï¼‰ï¼Œç¨‹åºé»˜è®¤å°†å…³é—­`use_flash_attn`ã€‚

æ•ˆæœæ–¹é¢ï¼Œæˆ‘ä»¬éªŒè¯è¿‡Int8 KV Cacheçš„ä½¿ç”¨å¯¹æ¨¡å‹æ•´ä½“çš„ç²¾åº¦æŒ‡æ ‡åŸºæœ¬æ— æŸã€‚æˆ‘ä»¬åšäº†é’ˆå¯¹æ˜¾å­˜å ç”¨çš„æ€§èƒ½æµ‹è¯•ã€‚è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼Œæ¨¡å‹é»˜è®¤ä½¿ç”¨BF16æ ¼å¼ï¼Œé»˜è®¤ç”Ÿæˆ1024ä¸ªtokenï¼Œå…¶ä¸­OOMè¡¨ç¤ºå†…å­˜ä¸è¶³ã€‚

å¼€å¯äº†KV cacheé‡åŒ–ä¹‹åï¼Œæ¨¡å‹åœ¨æ¨ç†çš„æ—¶å€™å¯ä»¥å¼€å¯æ›´å¤§çš„batch size (bs)ã€‚

| USE KV Cache |  bs=1  |  bs=4  | bs=16  | bs=32  | bs=64  | bs=100 |
|--------------|:------:|:------:|:------:|:------:|:------:|:------:|
| No           | 16.3GB | 24.1GB | 31.7GB | 48.7GB |  oom   |  oom   |
| Yes          | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB |


å¼€å¯äº†KV cacheé‡åŒ–ä¹‹åï¼Œæ¨¡å‹åœ¨æ¨ç†æ—¶å¯åœ¨ç”Ÿæˆæ›´é•¿çš„åºåˆ—ï¼ˆslï¼Œç”Ÿæˆçš„tokenæ•°ï¼‰æ—¶ï¼ŒèŠ‚çº¦æ›´å¤šçš„æ˜¾å­˜ã€‚

| USE KV Cache | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |
|--------------|:------:|:-------:|:-------:|:-------:|:-------:|
| no           | 15.2GB | 16.3GB  | 17.6GB  | 19.5GB  | 23.2GB  |
| yes          |  15GB  | 15.5GB  | 15.8GB  | 16.6GB  | 17.6GB  |


å¼€å¯KV cacheé‡åŒ–åï¼Œæ¨¡å‹åœ¨æ¨ç†æ—¶ä¼šå°†åŸå§‹å­˜è¿›`layer-past`çš„floatæ ¼å¼çš„key/valueè½¬æ¢æˆint8æ ¼å¼ï¼ŒåŒæ—¶å­˜å‚¨é‡åŒ–éƒ¨åˆ†çš„å‚æ•°ã€‚

å…·ä½“æ“ä½œå¦‚ä¸‹ï¼š

1. å°†key/valueè¿›è¡Œé‡åŒ–æ“ä½œ
```
    qv,scale,zero_point=quantize_cache_v(v)
```
2. å­˜å…¥`layer_past`ä¸­:

é‡åŒ–æ ¼å¼çš„`layer-past`:
```
    layer_past=((q_key,key_scale,key_zero_point),
                (q_value,value_scale,value_zero_point))
```
åŸå§‹æ ¼å¼çš„`layer-past`:
```
    layer_past=(key,value)
```
å¦‚æœéœ€è¦å°†`layer-past`ä¸­å­˜å¥½çš„keyï¼Œvalueç›´æ¥å–å‡ºä½¿ç”¨ï¼Œå¯ä»¥ä½¿ç”¨åé‡åŒ–æ“ä½œå°†Int8æ ¼å¼çš„key/valueè½¬å›floatæ ¼å¼ï¼š
```
    v=dequantize_cache_torch(qv,scale,zero_point)
```
<br>

### æ¨ç†æ€§èƒ½
è¿™ä¸€éƒ¨åˆ†å°†ä»‹ç»æ¨¡å‹æ¨ç†çš„é€Ÿåº¦å’Œæ˜¾å­˜å ç”¨çš„ç›¸å…³æ•°æ®ã€‚ä¸‹æ–‡çš„æ€§èƒ½æµ‹ç®—ä½¿ç”¨ [æ­¤è„šæœ¬](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py) å®Œæˆã€‚

æˆ‘ä»¬æµ‹ç®—äº†BF16ã€Int8å’ŒInt4æ¨¡å‹åœ¨ç”Ÿæˆ2048ä¸ªtokenæ—¶çš„å¹³å‡æ¨ç†é€Ÿåº¦ï¼ˆtokens/sï¼‰å’Œæ˜¾å­˜ä½¿ç”¨ã€‚ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

<table>
    <tr>
        <td>Model Size</td>
        <td>Quantization</td>
        <td>Speed (Tokens/s)</td>
        <td>GPU Memory Usage</td>
    </tr>
    <tr>
        <td rowspan="3">1.8B</td>
        <td>BF16</td>
        <td>54.09</td>
        <td>4.23GB</td>
    </tr>
    <tr>
        <td>Int8</td>
        <td>55.56</td>
        <td>3.48GB</td>
    </tr>
    <tr>
        <td>Int4</td>
        <td>71.07</td>
        <td>2.91GB</td>
    </tr>
    <tr>
        <td rowspan="3">7B</td>
        <td>BF16</td>
        <td>40.93</td>
        <td>16.99GB</td>
    </tr>
    <tr>
        <td>Int8</td>
        <td>37.47</td>
        <td>11.20GB</td>
    </tr>
    <tr>
        <td>Int4</td>
        <td>50.09</td>
        <td>8.21GB</td>
    </tr>
    <tr>
        <td rowspan="3">14B</td>
        <td>BF16</td>
        <td>32.22</td>
        <td>30.15GB</td>
    </tr>
    <tr>
        <td>Int8</td>
        <td>29.28</td>
        <td>18.81GB</td>
    </tr>
    <tr>
        <td>Int4</td>
        <td>38.72</td>
        <td>13.01GB</td>
    </tr>
    <tr>
        <td rowspan="3">72B</td>
        <td>BF16</td>
        <td>8.48</td>
        <td>144.69GB (2xA100)</td>
    </tr>
    <tr>
        <td>Int8</td>
        <td>9.05</td>
        <td>81.27GB (2xA100)</td>
    </tr>
    <tr>
        <td>Int4</td>
        <td>11.32</td>
        <td>48.86GB</td>
    </tr>
    <tr>
        <td>72B + vLLM</td>
        <td>BF16</td>
        <td>17.60</td>
        <td>2xA100</td>
    </tr>
</table>

è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼ˆé™¤éæåˆ°ä½¿ç”¨2xA100ï¼‰ï¼Œä½¿ç”¨PyTorch 2.0.1ã€CUDA 11.8å’ŒFlash-Attention2ã€‚(72B + vLLM ä½¿ç”¨ PyTorch 2.1.0å’ŒCuda 11.8.)æ¨ç†é€Ÿåº¦æ˜¯ç”Ÿæˆ2048ä¸ªtokençš„é€Ÿåº¦å‡å€¼ã€‚

æ³¨æ„ï¼šä»¥ä¸ŠInt4/Int8æ¨¡å‹ç”Ÿæˆé€Ÿåº¦ä½¿ç”¨autogptqåº“ç»™å‡ºï¼Œå½“å‰``AutoModelForCausalLM.from_pretrained``è½½å…¥çš„æ¨¡å‹ç”Ÿæˆé€Ÿåº¦ä¼šæ…¢å¤§çº¦20%ã€‚æˆ‘ä»¬å·²ç»å°†è¯¥é—®é¢˜æ±‡æŠ¥ç»™HuggingFaceå›¢é˜Ÿï¼Œè‹¥æœ‰è§£å†³æ–¹æ¡ˆå°†å³æ—¶æ›´æ–°ã€‚

æˆ‘ä»¬è¿˜æµ‹é‡äº†ä¸åŒä¸Šä¸‹æ–‡é•¿åº¦ã€ç”Ÿæˆé•¿åº¦ã€Flash-Attentionç‰ˆæœ¬çš„æ¨ç†é€Ÿåº¦å’Œ GPU å†…å­˜ä½¿ç”¨æƒ…å†µã€‚å¯ä»¥åœ¨ Hugging Face æˆ– ModelScope ä¸Šçš„ç›¸åº”çš„æ¨¡å‹ä»‹ç»é¡µé¢æ‰¾åˆ°ç»“æœã€‚

## å¾®è°ƒ

### ä½¿ç”¨æ–¹æ³•
æˆ‘ä»¬æä¾›äº†`finetune.py`è¿™ä¸ªè„šæœ¬ä¾›ç”¨æˆ·å®ç°åœ¨è‡ªå·±çš„æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒçš„åŠŸèƒ½ï¼Œä»¥æ¥å…¥ä¸‹æ¸¸ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†shellè„šæœ¬å‡å°‘ç”¨æˆ·çš„å·¥ä½œé‡ã€‚è¿™ä¸ªè„šæœ¬æ”¯æŒ [DeepSpeed](https://github.com/microsoft/DeepSpeed) å’Œ [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/) ã€‚æˆ‘ä»¬æä¾›çš„shellè„šæœ¬ä½¿ç”¨äº†DeepSpeedï¼Œå› æ­¤å»ºè®®æ‚¨ç¡®ä¿å·²ç»å®‰è£…DeepSpeedå’ŒPeftï¼ˆæ³¨æ„ï¼šDeepSpeedå¯èƒ½ä¸å…¼å®¹æœ€æ–°çš„pydanticç‰ˆæœ¬ï¼Œè¯·ç¡®ä¿`pydantic<2.0`ï¼‰ã€‚ä½ å¯ä»¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å®‰è£…ï¼š
```bash
pip install "peft<0.8.0" deepspeed
```

é¦–å…ˆï¼Œä½ éœ€è¦å‡†å¤‡ä½ çš„è®­ç»ƒæ•°æ®ã€‚ä½ éœ€è¦å°†æ‰€æœ‰æ ·æœ¬æ”¾åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­å¹¶å­˜å…¥jsonæ–‡ä»¶ä¸­ã€‚æ¯ä¸ªæ ·æœ¬å¯¹åº”ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«idå’Œconversationï¼Œå…¶ä¸­åè€…ä¸ºä¸€ä¸ªåˆ—è¡¨ã€‚ç¤ºä¾‹å¦‚ä¸‹æ‰€ç¤ºï¼š
```json
[
  {
    "id": "identity_0",
    "conversations": [
      {
        "from": "user",
        "value": "ä½ å¥½"
      },
      {
        "from": "assistant",
        "value": "æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚"
      }
    ]
  }
]
```

å‡†å¤‡å¥½æ•°æ®åï¼Œä½ å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æä¾›çš„shellè„šæœ¬å®ç°å¾®è°ƒã€‚æ³¨æ„ï¼Œä½ éœ€è¦åœ¨è„šæœ¬ä¸­æŒ‡å®šä½ çš„æ•°æ®çš„è·¯å¾„ã€‚

å¾®è°ƒè„šæœ¬èƒ½å¤Ÿå¸®ä½ å®ç°ï¼š
- å…¨å‚æ•°å¾®è°ƒ
- LoRA
- Q-LoRA

å…¨å‚æ•°å¾®è°ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚ä½ å¯ä»¥è¿è¡Œè¿™ä¸ªè„šæœ¬å¼€å§‹è®­ç»ƒï¼š

```bash
# åˆ†å¸ƒå¼è®­ç»ƒã€‚ç”±äºæ˜¾å­˜é™åˆ¶å°†å¯¼è‡´å•å¡è®­ç»ƒå¤±è´¥ï¼Œæˆ‘ä»¬ä¸æä¾›å•å¡è®­ç»ƒè„šæœ¬ã€‚
bash finetune/finetune_ds.sh
```

å°¤å…¶æ³¨æ„ï¼Œä½ éœ€è¦åœ¨è„šæœ¬ä¸­æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹åç§°æˆ–è·¯å¾„ã€æ•°æ®è·¯å¾„ã€ä»¥åŠæ¨¡å‹è¾“å‡ºçš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚åœ¨è¿™ä¸ªè„šæœ¬ä¸­æˆ‘ä»¬ä½¿ç”¨äº†DeepSpeed ZeRO 3ã€‚å¦‚æœä½ æƒ³ä¿®æ”¹è¿™ä¸ªé…ç½®ï¼Œå¯ä»¥åˆ é™¤æ‰`--deepspeed`è¿™ä¸ªè¾“å…¥æˆ–è€…è‡ªè¡Œæ ¹æ®éœ€æ±‚ä¿®æ”¹DeepSpeedé…ç½®jsonæ–‡ä»¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒï¼Œå› æ­¤ä½ å¯ä»¥è®¾ç½®`--bf16 True`æˆ–è€…`--fp16 True`ã€‚åœ¨ä½¿ç”¨fp16æ—¶ï¼Œè¯·ä½¿ç”¨DeepSpeedæ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒã€‚ç»éªŒä¸Šï¼Œå¦‚æœä½ çš„æœºå™¨æ”¯æŒbf16ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨bf16ï¼Œè¿™æ ·å¯ä»¥å’Œæˆ‘ä»¬çš„é¢„è®­ç»ƒå’Œå¯¹é½è®­ç»ƒä¿æŒä¸€è‡´ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬æŠŠé»˜è®¤é…ç½®è®¾ä¸ºå®ƒçš„åŸå› ã€‚

è¿è¡ŒLoRAçš„æ–¹æ³•ç±»ä¼¼å…¨å‚æ•°å¾®è°ƒã€‚ä½†åœ¨å¼€å§‹å‰ï¼Œè¯·ç¡®ä¿å·²ç»å®‰è£…`peft`ä»£ç åº“ã€‚å¦å¤–ï¼Œè®°ä½è¦è®¾ç½®æ­£ç¡®çš„æ¨¡å‹ã€æ•°æ®å’Œè¾“å‡ºè·¯å¾„ã€‚æˆ‘ä»¬å»ºè®®ä½ ä¸ºæ¨¡å‹è·¯å¾„ä½¿ç”¨ç»å¯¹è·¯å¾„ã€‚è¿™æ˜¯å› ä¸ºLoRAä»…å­˜å‚¨adapteréƒ¨åˆ†å‚æ•°ï¼Œè€Œadapteré…ç½®jsonæ–‡ä»¶è®°å½•äº†é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ï¼Œç”¨äºè¯»å–é¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€‚åŒæ ·ï¼Œä½ å¯ä»¥è®¾ç½®bf16æˆ–è€…fp16ã€‚

```bash
# å•å¡è®­ç»ƒ
bash finetune/finetune_lora_single_gpu.sh
# åˆ†å¸ƒå¼è®­ç»ƒ
bash finetune/finetune_lora_ds.sh
```

ä¸å…¨å‚æ•°å¾®è°ƒä¸åŒï¼ŒLoRA ([è®ºæ–‡](https://arxiv.org/abs/2106.09685)) åªæ›´æ–°adapterå±‚çš„å‚æ•°è€Œæ— éœ€æ›´æ–°åŸæœ‰è¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚è¿™ç§æ–¹æ³•å…è®¸ç”¨æˆ·ç”¨æ›´ä½çš„æ˜¾å­˜å¼€é”€æ¥è®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿæ„å‘³ç€æ›´å°çš„è®¡ç®—å¼€é”€ã€‚

æ³¨æ„ï¼Œå¦‚æœä½ ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒLoRAå¾®è°ƒï¼Œè€Œéchatæ¨¡å‹ï¼Œæ¨¡å‹çš„embeddingå’Œè¾“å‡ºå±‚çš„å‚æ•°å°†è¢«è®¾ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚è¿™æ˜¯å› ä¸ºé¢„è®­ç»ƒæ¨¡å‹æ²¡æœ‰å­¦ä¹ è¿‡ChatMLæ ¼å¼ä¸­çš„ç‰¹æ®Štokenï¼Œå› æ­¤éœ€è¦å°†è¿™éƒ¨åˆ†å‚æ•°è®¾ä¸ºå¯è®­ç»ƒæ‰èƒ½è®©æ¨¡å‹å­¦ä¼šç†è§£å’Œé¢„æµ‹è¿™äº›tokenã€‚è¿™ä¹Ÿæ„å‘³ç€ï¼Œå‡å¦‚ä½ çš„è®­ç»ƒå¼•å…¥æ–°çš„ç‰¹æ®Štokenï¼Œä½ éœ€è¦é€šè¿‡ä»£ç ä¸­çš„`modules_to_save`å°†è¿™äº›å‚æ•°è®¾ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚æ­¤å¤–ï¼Œè¿™éƒ¨åˆ†è®­ç»ƒå‚æ•°çš„å¼•å…¥ä¼šå½±å“ZeRO 3çš„ä½¿ç”¨ï¼Œå› æ­¤æˆ‘ä»¬é»˜è®¤æ¨èä½¿ç”¨ZeRO 2ã€‚å½“ç„¶ï¼Œå¦‚æœä½ ä¸éœ€è¦å¼•å…¥è¿™éƒ¨åˆ†è®­ç»ƒå‚æ•°ï¼Œä½ å¯ä»¥é€šè¿‡æ›¿æ¢DeepSpeedçš„é…ç½®æ–‡ä»¶æ¥ä½¿ç”¨ZeRO 3ã€‚å¦‚æœä½ æƒ³èŠ‚çœæ˜¾å­˜å ç”¨ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨chatæ¨¡å‹è¿›è¡ŒLoRAå¾®è°ƒï¼Œæ˜¾å­˜å ç”¨å°†å¤§å¹…åº¦é™ä½ã€‚ä¸‹æ–‡çš„æ˜¾å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦çš„è®°å½•å°†è¯¦ç»†ä»‹ç»è¿™éƒ¨åˆ†ç»†èŠ‚ã€‚

å¦‚æœä½ ä¾ç„¶é‡åˆ°æ˜¾å­˜ä¸è¶³çš„é—®é¢˜ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨Q-LoRA ([è®ºæ–‡](https://arxiv.org/abs/2305.14314)) ã€‚è¯¥æ–¹æ³•ä½¿ç”¨4æ¯”ç‰¹é‡åŒ–æ¨¡å‹ä»¥åŠpaged attentionç­‰æŠ€æœ¯å®ç°æ›´å°çš„æ˜¾å­˜å¼€é”€ã€‚

æ³¨æ„ï¼šå¦‚ä½ ä½¿ç”¨å•å¡Q-LoRAï¼Œä½ å¯èƒ½éœ€è¦å®‰è£…`mpi4py`ã€‚ä½ å¯ä»¥é€šè¿‡`pip`æˆ–è€…`conda`æ¥å®‰è£…ã€‚

è¿è¡ŒQ-LoRAä½ åªéœ€è¿è¡Œå¦‚ä¸‹è„šæœ¬ï¼š

```bash
# å•å¡è®­ç»ƒ
bash finetune/finetune_qlora_single_gpu.sh
# åˆ†å¸ƒå¼è®­ç»ƒ
bash finetune/finetune_qlora_ds.sh
```

æˆ‘ä»¬å»ºè®®ä½ ä½¿ç”¨æˆ‘ä»¬æä¾›çš„Int4é‡åŒ–æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå³Qwen-7B-Chat-Int4ã€‚è¯·**ä¸è¦ä½¿ç”¨**éé‡åŒ–æ¨¡å‹ï¼ä¸å…¨å‚æ•°å¾®è°ƒä»¥åŠLoRAä¸åŒï¼ŒQ-LoRAä»…æ”¯æŒfp16ã€‚æ³¨æ„ï¼Œç”±äºæˆ‘ä»¬å‘ç°torch ampæ”¯æŒçš„fp16æ··åˆç²¾åº¦è®­ç»ƒå­˜åœ¨é—®é¢˜ï¼Œå› æ­¤å½“å‰çš„å•å¡è®­ç»ƒQ-LoRAå¿…é¡»ä½¿ç”¨DeepSpeedã€‚æ­¤å¤–ï¼Œä¸Šè¿°LoRAå…³äºç‰¹æ®Štokençš„é—®é¢˜åœ¨Q-LoRAä¾ç„¶å­˜åœ¨ã€‚å¹¶ä¸”ï¼ŒInt4æ¨¡å‹çš„å‚æ•°æ— æ³•è¢«è®¾ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚æ‰€å¹¸çš„æ˜¯ï¼Œæˆ‘ä»¬åªæä¾›äº†Chatæ¨¡å‹çš„Int4æ¨¡å‹ï¼Œå› æ­¤ä½ ä¸ç”¨æ‹…å¿ƒè¿™ä¸ªé—®é¢˜ã€‚ä½†æ˜¯ï¼Œå¦‚æœä½ æ‰§æ„è¦åœ¨Q-LoRAä¸­å¼•å…¥æ–°çš„ç‰¹æ®Štokenï¼Œå¾ˆæŠ±æ­‰ï¼Œæˆ‘ä»¬æ— æ³•ä¿è¯ä½ èƒ½æˆåŠŸè®­ç»ƒã€‚

> æ³¨æ„ï¼šç”±äºHugging Faceçš„å†…éƒ¨å®ç°ï¼Œæ¨¡å‹åœ¨ä¿å­˜æ—¶ï¼Œä¸€äº›éPythonæ–‡ä»¶æœªä¿å­˜ï¼ˆä¾‹å¦‚`*.cpp`ä¸`*.cu`ï¼‰ï¼Œå¦‚éœ€è¦æ”¯æŒç›¸å…³åŠŸèƒ½ï¼Œè¯·æ‰‹åŠ¨å¤åˆ¶æœ‰å…³æ–‡ä»¶ã€‚

ä¸å…¨å‚æ•°å¾®è°ƒä¸åŒï¼ŒLoRAå’ŒQ-LoRAçš„è®­ç»ƒåªéœ€å­˜å‚¨adapteréƒ¨åˆ†çš„å‚æ•°ã€‚å‡å¦‚ä½ éœ€è¦ä½¿ç”¨LoRAè®­ç»ƒåçš„æ¨¡å‹ï¼Œä½ éœ€è¦ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ã€‚å‡è®¾ä½ ä½¿ç”¨Qwen-7Bè®­ç»ƒæ¨¡å‹ï¼Œä½ å¯ä»¥ç”¨å¦‚ä¸‹ä»£ç è¯»å–æ¨¡å‹ï¼š

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()
```

> æ³¨æ„: å¦‚æœ`peft>=0.8.0`ï¼ŒåŠ è½½æ¨¡å‹åŒæ—¶ä¼šå°è¯•åŠ è½½tokenizerï¼Œä½†peftå†…éƒ¨æœªç›¸åº”è®¾ç½®`trust_remote_code=True`ï¼Œå¯¼è‡´`ValueError: Tokenizer class QWenTokenizer does not exist or is not currently imported.`è¦é¿è¿‡è¿™ä¸€é—®é¢˜ï¼Œä½ å¯ä»¥é™çº§`peft<0.8.0`æˆ–å°†tokenizerç›¸å…³æ–‡ä»¶ç§»åˆ°å…¶å®ƒæ–‡ä»¶å¤¹ã€‚


å¦‚æœä½ è§‰å¾—è¿™æ ·ä¸€æ­¥åˆ°ä½çš„æ–¹å¼è®©ä½ å¾ˆä¸å®‰å¿ƒæˆ–è€…å½±å“ä½ æ¥å…¥ä¸‹æ¸¸åº”ç”¨ï¼Œä½ å¯ä»¥é€‰æ‹©å…ˆåˆå¹¶å¹¶å­˜å‚¨æ¨¡å‹ï¼ˆLoRAæ”¯æŒåˆå¹¶ï¼ŒQ-LoRAä¸æ”¯æŒï¼‰ï¼Œå†ç”¨å¸¸è§„æ–¹å¼è¯»å–ä½ çš„æ–°æ¨¡å‹ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()

merged_model = model.merge_and_unload()
# max_shard_size and safe serialization are not necessary. 
# They respectively work for sharding checkpoint and save the model to safetensors
merged_model.save_pretrained(new_model_directory, max_shard_size="2048MB", safe_serialization=True)
```

`new_model_directory`ç›®å½•å°†åŒ…å«åˆå¹¶åçš„æ¨¡å‹å‚æ•°ä¸ç›¸å…³æ¨¡å‹ä»£ç ã€‚è¯·æ³¨æ„`*.cu`å’Œ`*.cpp`æ–‡ä»¶å¯èƒ½æ²¡è¢«ä¿å­˜ï¼Œè¯·æ‰‹åŠ¨å¤åˆ¶ã€‚å¦å¤–ï¼Œ`merge_and_unload`ä»…ä¿å­˜æ¨¡å‹ï¼Œå¹¶æœªä¿å­˜tokenizerï¼Œå¦‚æœ‰éœ€è¦ï¼Œè¯·å¤åˆ¶ç›¸å…³æ–‡ä»¶æˆ–ä½¿ç”¨ä»¥ä»¥ä¸‹ä»£ç ä¿å­˜
```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(
    path_to_adapter, # path to the output directory
    trust_remote_code=True
)
tokenizer.save_pretrained(new_model_directory)
```


æ³¨æ„ï¼šåˆ†å¸ƒå¼è®­ç»ƒéœ€è¦æ ¹æ®ä½ çš„éœ€æ±‚å’Œæœºå™¨æŒ‡å®šæ­£ç¡®çš„åˆ†å¸ƒå¼è®­ç»ƒè¶…å‚æ•°ã€‚æ­¤å¤–ï¼Œä½ éœ€è¦æ ¹æ®ä½ çš„æ•°æ®ã€æ˜¾å­˜æƒ…å†µå’Œè®­ç»ƒé€Ÿåº¦é¢„æœŸï¼Œä½¿ç”¨`--model_max_length`è®¾å®šä½ çš„æ•°æ®é•¿åº¦ã€‚

### é‡åŒ–å¾®è°ƒåæ¨¡å‹

è¿™ä¸€å°èŠ‚ç”¨äºé‡åŒ–å…¨å‚/LoRAå¾®è°ƒåçš„æ¨¡å‹ã€‚ï¼ˆæ³¨æ„ï¼šä½ ä¸éœ€è¦é‡åŒ–Q-LoRAæ¨¡å‹å› ä¸ºå®ƒæœ¬èº«å°±æ˜¯é‡åŒ–è¿‡çš„ã€‚ï¼‰
å¦‚æœä½ éœ€è¦é‡åŒ–LoRAå¾®è°ƒåçš„æ¨¡å‹ï¼Œè¯·å…ˆæ ¹æ®ä¸Šæ–¹è¯´æ˜å»åˆå¹¶ä½ çš„æ¨¡å‹æƒé‡ã€‚

æˆ‘ä»¬æ¨èä½¿ç”¨[auto_gptq](https://github.com/PanQiWei/AutoGPTQ)å»é‡åŒ–ä½ çš„æ¨¡å‹ã€‚

```bash
pip install auto-gptq optimum
```

æ³¨æ„: å½“å‰AutoGPTQæœ‰ä¸ªbugï¼Œå¯ä»¥åœ¨è¯¥[issue](https://github.com/PanQiWei/AutoGPTQ/issues/370)æŸ¥çœ‹ã€‚è¿™é‡Œæœ‰ä¸ª[ä¿®æ”¹PR](https://github.com/PanQiWei/AutoGPTQ/pull/495)ï¼Œä½ å¯ä»¥ä½¿ç”¨è¯¥åˆ†æ”¯ä»ä»£ç è¿›è¡Œå®‰è£…ã€‚

é¦–å…ˆï¼Œå‡†å¤‡æ ¡å‡†é›†ã€‚ä½ å¯ä»¥é‡ç”¨å¾®è°ƒä½ çš„æ•°æ®ï¼Œæˆ–è€…æŒ‰ç…§å¾®è°ƒç›¸åŒçš„æ–¹å¼å‡†å¤‡å…¶ä»–æ•°æ®ã€‚

ç¬¬äºŒæ­¥ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```bash
python run_gptq.py \
    --model_name_or_path $YOUR_LORA_MODEL_PATH \
    --data_path $DATA \
    --out_path $OUTPUT_PATH \
    --bits 4 # 4 for int4; 8 for int8
```

è¿™ä¸€æ­¥éœ€è¦ä½¿ç”¨GPUï¼Œæ ¹æ®ä½ çš„æ ¡å‡†é›†å¤§å°å’Œæ¨¡å‹å¤§å°ï¼Œå¯èƒ½ä¼šæ¶ˆè€—æ•°ä¸ªå°æ—¶ã€‚

æ¥ä¸‹æ¥, å°†åŸæ¨¡å‹ä¸­æ‰€æœ‰ `*.py`, `*.cu`, `*.cpp` æ–‡ä»¶å’Œ `generation_config.json` æ–‡ä»¶å¤åˆ¶åˆ°è¾“å‡ºæ¨¡å‹ç›®å½•ä¸‹ã€‚åŒæ—¶ï¼Œä½¿ç”¨å®˜æ–¹å¯¹åº”ç‰ˆæœ¬çš„é‡åŒ–æ¨¡å‹çš„ `config.json` æ–‡ä»¶è¦†ç›–è¾“å‡ºæ¨¡å‹ç›®å½•ä¸‹çš„æ–‡ä»¶
(ä¾‹å¦‚, å¦‚æœä½ å¾®è°ƒäº† `Qwen-7B-Chat`å’Œ`--bits 4`, é‚£ä¹ˆä½ å¯ä»¥ä» [Qwen-7B-Chat-Int4](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4/blob/main/config.json) ä»“åº“ä¸­æ‰¾åˆ°å¯¹åº”çš„`config.json` )ã€‚
å¹¶ä¸”ï¼Œä½ éœ€è¦å°† ``gptq.safetensors`` é‡å‘½åä¸º ``model.safetensors``ã€‚

æœ€åï¼Œåƒå®˜æ–¹é‡åŒ–æ¨¡å‹ä¸€æ ·æµ‹è¯•ä½ çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("/path/to/your/model", trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    "/path/to/your/model",
    device_map="auto",
    trust_remote_code=True
).eval()

response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
```

### å¤šæœºå¾®è°ƒ

æˆ‘ä»¬æä¾›çš„è„šæœ¬æ”¯æŒå¤šæœºå¾®è°ƒï¼Œå¯ä»¥å‚è€ƒ[è„šæœ¬](./finetune/finetune_lora_ds.sh)ä¸­çš„æ³¨é‡Šï¼Œåœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šæ­£ç¡®è®¾ç½®ç›¸åº”çš„å‚æ•°å¹¶å¯åŠ¨è®­ç»ƒè„šæœ¬ã€‚å…³äºå¤šæœºåˆ†å¸ƒå¼è®­ç»ƒçš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ[torchrun](https://pytorch.org/docs/stable/elastic/run.html)ã€‚

æ³¨æ„ï¼š DeepSpeed ZeRO 3 å¯¹èŠ‚ç‚¹é—´é€šä¿¡é€Ÿç‡çš„è¦æ±‚è¿œå¤§äº ZeRO 2ï¼Œåœ¨å¤šæœºå¾®è°ƒçš„æƒ…å†µä¸‹ä¼šå¤§å¹…é™ä½è®­ç»ƒé€Ÿåº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸å»ºè®®åœ¨å¤šæœºå¾®è°ƒçš„æƒ…å†µä¸‹ä½¿ç”¨ DeepSpeed ZeRO 3 é…ç½®ã€‚

### æ˜¾å­˜å ç”¨åŠè®­ç»ƒé€Ÿåº¦

ä¸‹é¢è®°å½•7Bå’Œ14Bæ¨¡å‹åœ¨å•GPUä½¿ç”¨LoRAï¼ˆLoRA (emb)æŒ‡çš„æ˜¯embeddingå’Œè¾“å‡ºå±‚å‚ä¸è®­ç»ƒï¼Œè€ŒLoRAåˆ™ä¸ä¼˜åŒ–è¿™éƒ¨åˆ†å‚æ•°ï¼‰å’ŒQLoRAæ—¶å¤„ç†ä¸åŒé•¿åº¦è¾“å…¥çš„æ˜¾å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦çš„æƒ…å†µã€‚æœ¬æ¬¡è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼Œä½¿ç”¨CUDA 11.8å’ŒPytorch 2.0ï¼Œå¹¶ä½¿ç”¨äº†flash attention 2ã€‚æˆ‘ä»¬ç»Ÿä¸€ä½¿ç”¨batch sizeä¸º1ï¼Œgradient accumulationä¸º8çš„è®­ç»ƒé…ç½®ï¼Œè®°å½•è¾“å…¥é•¿åº¦åˆ†åˆ«ä¸º256ã€512ã€1024ã€2048ã€4096å’Œ8192çš„æ˜¾å­˜å ç”¨ï¼ˆGBï¼‰å’Œè®­ç»ƒé€Ÿåº¦ï¼ˆs/iterï¼‰ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨2å¼ A100æµ‹äº†Qwen-7Bçš„å…¨å‚æ•°å¾®è°ƒã€‚å—é™äºæ˜¾å­˜å¤§å°ï¼Œæˆ‘ä»¬ä»…æµ‹è¯•äº†256ã€512å’Œ1024tokençš„æ€§èƒ½ã€‚

å¯¹äº Qwen-7Bï¼Œæˆ‘ä»¬é¢å¤–æµ‹è¯•äº†å¤šæœºå¾®è°ƒçš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨ä¸¤å°æœåŠ¡å™¨ä¸Šè¿è¡Œè¯„æµ‹ï¼Œæ¯å°æœåŠ¡å™¨åŒ…å«ä¸¤å¼ A100-SXM4-80G GPUï¼Œå…¶ä½™é…ç½®ä¸Qwen-7Bçš„å…¶ä»–è¯„æµ‹ç›¸åŒã€‚å¤šæœºå¾®è°ƒçš„ç»“æœåœ¨è¡¨ä¸­ä»¥ LoRA (multinode) æ ‡ç¤ºã€‚

å¯¹äº Qwen-72Bï¼Œæˆ‘ä»¬æµ‹è¯•äº†ä¸¤ç§æ–¹æ¡ˆï¼š1ï¼‰ä½¿ç”¨4ä¸ª A100-SXM4-80G GPUsï¼Œé€šè¿‡ Lora + DeepSpeed ZeRO 3 å¾®è°ƒå’Œ2ï¼‰ä½¿ç”¨å•å¼ A100-SXM4-80G GPUï¼Œé€šè¿‡ QLora (int4) å¾®è°ƒã€‚è¯·æ³¨æ„ï¼Œä½¿ç”¨ LoRA (emb) å¾®è°ƒå’Œä¸å¸¦ DeepSpeed ZeRO 3 çš„ LoRA å¾®è°ƒåœ¨4ä¸ªA100-SXM4-80G GPUs ä¸Šéƒ½ä¼šå‡ºç°OOMï¼ˆä½ å¯ä»¥é€šè¿‡å°†`--deepspeed finetune/ds_config_zero3.json`å‚æ•°ä¼ ç»™[`finetune/finetune_lora_ds.sh`](finetune/finetune_lora_ds.sh)æ¥æ‰“å¼€ DeepSpeed ZeRO 3 é…ç½®ï¼‰ã€‚

å…·ä½“æ•°å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š


<table>
    <tr>
      <th rowspan="2">Model Size</th><th rowspan="2">Method</th><th rowspan="2">#Nodes</th><th rowspan="2">#GPUs per node</th><th colspan="6" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">256</th><th align="center">512</th><th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th>
    </tr>
    <tr>
        <th rowspan="4">1.8B</th><td>LoRA</td>
        <td>1</td><td>1</td>
        <td align="center">6.7G / 1.0s/it</td><td align="center">7.4G / 1.0s/it</td><td align="center">8.4G / 1.1s/it</td><td align="center">11.0G / 1.7s/it</td><td align="center">16.2G / 3.3s/it</td><td align="center">21.8G / 6.8s/it</td>
    </tr>
    <tr>
        <td>LoRA (emb)</td>
        <td>1</td><td>1</td>
        <td align="center">13.7G / 1.0s/it</td><td align="center">14.0G / 1.0s/it</td><td align="center">14.0G / 1.1s/it</td><td align="center">15.1G / 1.8s/it</td><td align="center">19.7G / 3.4s/it</td><td align="center">27.7G / 7.0s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td>
        <td>1</td><td>1</td>
        <td align="center">5.8G / 1.4s/it</td><td align="center">6.0G / 1.4s/it</td><td align="center">6.6G / 1.4s/it</td><td align="center">7.8G / 2.0s/it</td><td align="center">10.2G / 3.4s/it</td><td align="center">15.8G / 6.5s/it</td>
    </tr>
    <tr>
        <td>Full-parameter</td>
        <td>1</td><td>1</td>
        <td align="center">43.5G / 2.1s/it</td><td align="center">43.5G / 2.2s/it</td><td align="center">43.5G / 2.2s/it</td><td align="center">43.5G / 2.3s/it</td><td align="center">47.1G / 2.8s/it</td><td align="center">48.3G / 5.6s/it</td>
    </tr>
    <tr>
        <th rowspan="5">7B</th>
        <td>LoRA</td>
        <td>1</td><td>1</td>
        <td align="center">20.1G / 1.2s/it</td><td align="center">20.4G / 1.5s/it</td><td align="center">21.5G / 2.8s/it</td><td align="center">23.8G / 5.2s/it</td><td align="center">29.7G / 10.1s/it</td><td align="center">36.6G / 21.3s/it</td>
    </tr>
    <tr>
        <td>LoRA (emb)</td>
        <td>1</td><td>1</td>
        <td align="center">33.7G / 1.4s/it</td><td align="center">34.1G / 1.6s/it</td><td align="center">35.2G / 2.9s/it</td><td align="center">35.1G / 5.3s/it</td><td align="center">39.2G / 10.3s/it</td><td align="center">48.5G / 21.7s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td>
        <td>1</td><td>1</td>
        <td align="center">11.5G / 3.0s/it</td><td align="center">11.5G / 3.0s/it</td><td align="center">12.3G / 3.5s/it</td><td align="center">13.9G / 7.0s/it</td><td align="center">16.9G / 11.6s/it</td><td align="center">23.5G / 22.3s/it</td>
    </tr>
    <tr>
        <td>Full-parameter</td>
<td>1</td><td>2</td>
<td align="center">139.2G / 4.0s/it</td><td align="center">148.0G / 4.0s/it</td><td align="center">162.0G / 4.5s/it</td><td align="center">-</td><td align="center">-</td><td align="center">-</td>
    </tr>
    <tr>
        <td>LoRA (multinode)</td>
        <td>2</td><td>2</td>
        <td align="center">74.7G / 2.09s/it</td><td align="center">77.6G / 3.16s/it</td><td align="center">84.9G / 5.17s/it</td><td align="center">95.1G / 9.25s/it</td><td align="center">121.1G / 18.1s/it</td><td align="center">155.5G / 37.4s/it</td>
    </tr>
    <tr>
        <th rowspan="3">14B</th>
        <td>LoRA</td>
        <td>1</td><td>1</td>
        <td align="center">34.6G / 1.6s/it</td><td align="center">35.1G / 2.4s/it</td><td align="center">35.3G / 4.4s/it</td><td align="center">37.4G / 8.4s/it</td><td align="center">42.5G / 17.0s/it</td><td align="center">55.2G / 36.0s/it</td>
    </tr>
    <tr>
        <td>LoRA (emb)</td>
        <td>1</td><td>1</td>
        <td align="center">51.2 / 1.7s/it</td><td align="center">51.1G / 2.6s/it</td><td align="center">51.5G / 4.6s/it</td><td align="center">54.1G / 8.6s/it</td><td align="center">56.8G / 17.2s/it</td><td align="center">67.7G / 36.3s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td>
        <td>1</td><td>1</td>
        <td align="center">18.7G / 5.3s/it</td><td align="center">18.4G / 6.3s/it</td><td align="center">18.9G / 8.2s/it</td><td align="center">19.9G / 11.8s/it</td><td align="center">23.0G / 20.1s/it</td><td align="center">27.9G / 38.3s/it</td>
    </tr>
    <tr>
        <th rowspan="2">72B</th>
        <td>LoRA + Deepspeed Zero3</td>
        <td>1</td><td>4</td>
        <td align="center">215.4G / 17.6s/it</td><td align="center">217.7G / 20.5s/it</td><td align="center">222.6G / 29.4s/it</td><td align="center">228.8G / 45.7s/it</td><td align="center">249.0G / 83.4s/it</td><td align="center">289.2G / 161.5s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td>
        <td>1</td><td>1</td>
        <td align="center">61.4G / 27.4s/it</td><td align="center">61.4G / 31.5s/it</td><td align="center">62.9G / 41.4s/it</td><td align="center">64.1G / 59.5s/it</td><td align="center">68.0G / 97.7s/it</td><td align="center">75.6G / 179.8s/it</td>
    </tr>
</table>

<br>

## éƒ¨ç½²

### vLLM
å¦‚å¸Œæœ›éƒ¨ç½²åŠåŠ é€Ÿæ¨ç†ï¼Œæˆ‘ä»¬å»ºè®®ä½ ä½¿ç”¨vLLMã€‚

å¦‚æœä½ ä½¿ç”¨**CUDA 12.1å’ŒPyTorch 2.1**ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…vLLMã€‚

```bash
pip install vllm
```

å¦åˆ™è¯·å‚è€ƒvLLMå®˜æ–¹çš„[å®‰è£…è¯´æ˜](https://docs.vllm.ai/en/latest/getting_started/installation.html)ã€‚

#### vLLM + ç±»Transformeræ¥å£

è¯·ä¸‹è½½[æ¥å£å°è£…ä»£ç ](examples/vllm_wrapper.py)åˆ°å½“å‰æ–‡ä»¶å¤¹ï¼Œå¹¶æ‰§è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œå¤šè½®å¯¹è¯äº¤äº’ã€‚ï¼ˆæ³¨æ„ï¼šè¯¥æ–¹æ³•å½“å‰åªæ”¯æŒ``model.chat()``æ¥å£ã€‚ï¼‰

```python
from vllm_wrapper import vLLMWrapper

model = vLLMWrapper('Qwen/Qwen-7B-Chat', tensor_parallel_size=1)

response, history = model.chat(query="ä½ å¥½", history=None)
print(response)
response, history = model.chat(query="ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚", history=history)
print(response)
response, history = model.chat(query="ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜", history=history)
print(response)
```

#### vLLM + ç½‘é¡µDemo / ç±»OpenAI API

ä½ å¯ä»¥ä½¿ç”¨FastChatå»æ­å»ºä¸€ä¸ªç½‘é¡µDemoæˆ–ç±»OpenAI APIæœåŠ¡å™¨ã€‚é¦–å…ˆï¼Œè¯·å®‰è£…FastChatï¼š

```bash
pip install "fschat[model_worker,webui]"
```

ä½¿ç”¨vLLMå’ŒFastChatè¿è¡ŒQwenä¹‹å‰ï¼Œé¦–å…ˆå¯åŠ¨ä¸€ä¸ªcontrollerï¼š
```bash
python -m fastchat.serve.controller
```

ç„¶åå¯åŠ¨model workerè¯»å–æ¨¡å‹ã€‚å¦‚ä½¿ç”¨å•å¡æ¨ç†ï¼Œè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š
```bash
python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype bfloat16
# python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --dtype float16 # è¿è¡Œint4æ¨¡å‹
```
ç„¶è€Œï¼Œå¦‚æœä½ å¸Œæœ›ä½¿ç”¨å¤šGPUåŠ é€Ÿæ¨ç†æˆ–è€…å¢å¤§æ˜¾å­˜ï¼Œä½ å¯ä»¥ä½¿ç”¨vLLMæ”¯æŒçš„æ¨¡å‹å¹¶è¡Œæœºåˆ¶ã€‚å‡è®¾ä½ éœ€è¦åœ¨4å¼ GPUä¸Šè¿è¡Œä½ çš„æ¨¡å‹ï¼Œå‘½ä»¤å¦‚ä¸‹æ‰€ç¤ºï¼š
```bash
python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4 --dtype bfloat16
# python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4 --dtype float16 # è¿è¡Œint4æ¨¡å‹
```

å¯åŠ¨model workeråï¼Œä½ å¯ä»¥å¯åŠ¨ä¸€ä¸ªï¼š

* Web UI Demo
```bash
python -m fastchat.serve.gradio_web_server
```

* OpenAI API

ä½¿ç”¨OpenAI APIå‰ï¼Œè¯·é˜…è¯»æˆ‘ä»¬çš„APIç« èŠ‚é…ç½®å¥½ç¯å¢ƒï¼Œç„¶åè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š
```bash
python -m fastchat.serve.openai_api_server --host localhost --port 8000
```

ç„¶è€Œï¼Œå¦‚æœä½ è§‰å¾—ä½¿ç”¨vLLMå’ŒFastChatæ¯”è¾ƒå›°éš¾ï¼Œä½ ä¹Ÿå¯ä»¥å°è¯•ä»¥ä¸‹æˆ‘ä»¬æä¾›çš„æœ€ç®€å•çš„æ–¹å¼éƒ¨ç½²Web Demoã€CLI Demoå’ŒOpenAI APIã€‚
<br>


### Web UI

æˆ‘ä»¬æä¾›äº†Web UIçš„demoä¾›ç”¨æˆ·ä½¿ç”¨ (æ„Ÿè°¢ @wysaid æ”¯æŒ)ã€‚åœ¨å¼€å§‹å‰ï¼Œç¡®ä¿å·²ç»å®‰è£…å¦‚ä¸‹ä»£ç åº“ï¼š

```bash
pip install -r requirements_web_demo.txt
```

éšåè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå¹¶ç‚¹å‡»ç”Ÿæˆé“¾æ¥ï¼š

```bash
python web_demo.py
```

<p align="center">
    <br>
    <img src="assets/web_demo.gif" width="600" />
    <br>
<p>

### äº¤äº’å¼Demo

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€å•çš„äº¤äº’å¼Demoç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹`cli_demo.py`ã€‚å½“å‰æ¨¡å‹å·²ç»æ”¯æŒæµå¼è¾“å‡ºï¼Œç”¨æˆ·å¯é€šè¿‡è¾“å…¥æ–‡å­—çš„æ–¹å¼å’ŒQwen-7B-Chatäº¤äº’ï¼Œæ¨¡å‹å°†æµå¼è¾“å‡ºè¿”å›ç»“æœã€‚è¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š

```bash
python cli_demo.py
```

<p align="center">
    <br>
    <img src="assets/cli_demo.gif" width="600" />
    <br>
<p>
<br>

### API

æˆ‘ä»¬æä¾›äº†OpenAI APIæ ¼å¼çš„æœ¬åœ°APIéƒ¨ç½²æ–¹æ³•ï¼ˆæ„Ÿè°¢@hanpenggitï¼‰ã€‚åœ¨å¼€å§‹ä¹‹å‰å…ˆå®‰è£…å¿…è¦çš„ä»£ç åº“ï¼š

```bash
pip install fastapi uvicorn "openai<1.0" pydantic sse_starlette
```

éšåå³å¯è¿è¡Œä»¥ä¸‹å‘½ä»¤éƒ¨ç½²ä½ çš„æœ¬åœ°APIï¼š

```bash
python openai_api.py
```

ä½ ä¹Ÿå¯ä»¥ä¿®æ”¹å‚æ•°ï¼Œæ¯”å¦‚`-c`æ¥ä¿®æ”¹æ¨¡å‹åç§°æˆ–è·¯å¾„, `--cpu-only`æ”¹ä¸ºCPUéƒ¨ç½²ç­‰ç­‰ã€‚å¦‚æœéƒ¨ç½²å‡ºç°é—®é¢˜ï¼Œæ›´æ–°ä¸Šè¿°ä»£ç åº“å¾€å¾€å¯ä»¥è§£å†³å¤§å¤šæ•°é—®é¢˜ã€‚

ä½¿ç”¨APIåŒæ ·éå¸¸ç®€å•ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
import openai
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "none"

# ä½¿ç”¨æµå¼å›å¤çš„è¯·æ±‚
for chunk in openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ],
    stream=True
    # æµå¼è¾“å‡ºçš„è‡ªå®šä¹‰stopwordsåŠŸèƒ½å°šæœªæ”¯æŒï¼Œæ­£åœ¨å¼€å‘ä¸­
):
    if hasattr(chunk.choices[0].delta, "content"):
        print(chunk.choices[0].delta.content, end="", flush=True)

# ä¸ä½¿ç”¨æµå¼å›å¤çš„è¯·æ±‚
response = openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ],
    stream=False,
    stop=[] # åœ¨æ­¤å¤„æ·»åŠ è‡ªå®šä¹‰çš„stop words ä¾‹å¦‚ReAct promptingæ—¶éœ€è¦å¢åŠ ï¼š stop=["Observation:"]ã€‚
)
print(response.choices[0].message.content)
```

<p align="center">
    <br>
    <img src="assets/openai_api.gif" width="600" />
    <br>
<p>

è¯¥æ¥å£ä¹Ÿæ”¯æŒå‡½æ•°è°ƒç”¨ï¼ˆ**Function Calling**ï¼‰ï¼Œä½†æš‚æ—¶ä»…é™ `stream=False` æ—¶èƒ½ç”Ÿæ•ˆã€‚ç”¨æ³•è§[å‡½æ•°è°ƒç”¨ç¤ºä¾‹](examples/function_call_examples.py)ã€‚
<br><br>

## ğŸ³ ä½¿ç”¨é¢„æ„å»ºçš„Dockeré•œåƒ

ä¸ºç®€åŒ–éƒ¨ç½²æµç¨‹ï¼Œæˆ‘ä»¬æä¾›äº†é¢„é…ç½®å¥½ç›¸åº”ç¯å¢ƒçš„Dockeré•œåƒï¼š[qwenllm/qwen](https://hub.docker.com/r/qwenllm/qwen)ï¼Œåªéœ€å®‰è£…é©±åŠ¨ã€ä¸‹è½½æ¨¡å‹æ–‡ä»¶å³å¯å¯åŠ¨Demoã€éƒ¨ç½²OpenAI APIä»¥åŠè¿›è¡Œå¾®è°ƒã€‚

### å‡†å¤‡æ“ä½œ

1. æ ¹æ®éœ€è¦ä½¿ç”¨çš„é•œåƒç‰ˆæœ¬ï¼Œå®‰è£…ç›¸åº”ç‰ˆæœ¬çš„Nvidiaé©±åŠ¨ï¼š
  - `qwenllm/qwen:cu117`ï¼ˆ**æ¨è**ï¼‰ï¼š`>= 515.48.07`
  - `qwenllm/qwen:cu114`ï¼ˆä¸æ”¯æŒflash-attentionï¼‰ï¼š`>= 470.82.01`
  - `qwenllm/qwen:cu121`ï¼š`>= 530.30.02`
  - `qwenllm/qwen:latest`ï¼šä¸`qwenllm/qwen:cu117`ç›¸åŒ

2. å®‰è£…å¹¶é…ç½®[docker](https://docs.docker.com/engine/install/)å’Œ[nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)ï¼š

```bash
# é…ç½®docker
sudo systemctl start docker
# æµ‹è¯•dockeræ˜¯å¦å®‰è£…æ­£ç¡®
sudo docker run hello-world

# é…ç½®nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
# æµ‹è¯•nvidia-container-toolkitæ˜¯å¦å®‰è£…æ­£ç¡®
sudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi
```

3. ä¸‹è½½æ¨¡å‹åŠä»£ç è‡³æœ¬åœ°ï¼ˆå‚è€ƒ[æ­¤å¤„è¯´æ˜](#DownloadModel)ï¼‰

### éƒ¨ç½²

ä¸‹é¢æˆ‘ä»¬ä»¥Qwen-7B-Chatä¸ºä¾‹ã€‚åœ¨å¯åŠ¨Web Demoæˆ–è€…éƒ¨ç½²APIå‰ï¼Œè¯·å…ˆå‚ç…§ä¸‹æ–¹ä»£ç å®Œæˆé…ç½®å·¥ä½œï¼š

```bash
IMAGE_NAME=qwenllm/qwen:cu117
PORT=8901
CHECKPOINT_PATH=/path/to/Qwen-7B-Chat   # ä¸‹è½½åˆ°æœ¬åœ°çš„æ¨¡å‹åŠä»£ç è·¯å¾„
```

å¦‚ä¸‹è„šæœ¬å¯ä»¥å¸®ä½ éƒ¨ç½²:

* OpenAI API
```bash
bash docker/docker_openai_api.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}
```

* Web UI
```bash
bash docker/docker_web_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}
```

* äº¤äº’å¼Demo
```bash
bash docker/docker_cli_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH}
```

è¿™äº›å‘½ä»¤å°†è‡ªåŠ¨ä¸‹è½½æ‰€éœ€é•œåƒä»¥åŠåå°å¯åŠ¨Web UI Demoã€‚ä½ å¯ä»¥æ‰“å¼€`http://localhost:${PORT}` æ¥ä½¿ç”¨è¯¥Demoã€‚

å¦‚æœè¾“å‡ºå¦‚ä¸‹å†…å®¹ï¼Œåˆ™è¯´æ˜Demoå¯åŠ¨æˆåŠŸï¼š

```text
Successfully started web demo. Open '...' to try!
Run `docker logs ...` to check demo status.
Run `docker rm -f ...` to stop and remove the demo.
```

å¦‚æœä½ æƒ³æŸ¥çœ‹Demoçš„çŠ¶æ€ï¼Œä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ªå‘½ä»¤æ¥å±•ç¤ºè¾“å‡ºç»“æœï¼š`docker logs qwen`ã€‚

ä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ªå‘½ä»¤`docker rm -f qwen`æ¥åœæ­¢æœåŠ¡å¹¶åˆ é™¤å®¹å™¨ã€‚

### å¾®è°ƒ

ä½¿ç”¨é¢„é…ç½®å¥½çš„Dockeré•œåƒè¿›è¡Œå¾®è°ƒçš„æ–¹æ³•ä¸[ä¸Šä¸€ç« ](#å¾®è°ƒ)åŸºæœ¬ä¸€è‡´ï¼ˆæˆ‘ä»¬å·²ç»åœ¨é•œåƒä¸­å®‰è£…äº†ç›¸å…³ä¾èµ–ï¼‰ï¼š

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå•å¡LoRAå¾®è°ƒçš„ç¤ºä¾‹ï¼š
```bash
IMAGE_NAME=qwenllm/qwen:cu117
CHECKPOINT_PATH=/path/to/Qwen-7B                # ä¸‹è½½çš„æ¨¡å‹å’Œä»£ç è·¯å¾„
#CHECKPOINT_PATH=/path/to/Qwen-7B-Chat-Int4     # ä¸‹è½½çš„æ¨¡å‹å’Œä»£ç è·¯å¾„ (Q-LoRA)
DATA_PATH=/path/to/data/root                    # å‡†å¤‡å¾®è°ƒæ•°æ®æ”¾åœ¨ ${DATA_PATH}/example.json
OUTPUT_PATH=/path/to/output/checkpoint          # å¾®è°ƒè¾“å‡ºè·¯å¾„

# é»˜è®¤ä½¿ç”¨ä¸»æœºæ‰€æœ‰GPU
DEVICE=all
# å¦‚æœéœ€è¦æŒ‡å®šç”¨äºè®­ç»ƒçš„GPUï¼ŒæŒ‰ç…§ä»¥ä¸‹æ–¹å¼è®¾ç½®deviceï¼ˆæ³¨æ„ï¼šå†…å±‚çš„å¼•å·ä¸å¯çœç•¥ï¼‰
#DEVICE='"device=0,1,2,3"'

mkdir -p ${OUTPUT_PATH}

# å•å¡LoRAå¾®è°ƒ
docker run --gpus ${DEVICE} --rm --name qwen \
    --mount type=bind,source=${CHECKPOINT_PATH},target=/data/shared/Qwen/Qwen-7B \
    --mount type=bind,source=${DATA_PATH},target=/data/shared/Qwen/data \
    --mount type=bind,source=${OUTPUT_PATH},target=/data/shared/Qwen/output_qwen \
    --shm-size=2gb \
    -it ${IMAGE_NAME} \
    bash finetune/finetune_lora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B/ -d /data/shared/Qwen/data/example.json
```

å¦‚éœ€ä¿®æ”¹ä¸ºå•å¡Q-LoRAå¾®è°ƒç¤ºä¾‹ï¼Œåªè¦ä¿®æ”¹`docker run`ä¸­çš„bashå‘½ä»¤ï¼š
```bash
bash finetune/finetune_qlora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B-Chat-Int4/ -d /data/shared/Qwen/data/example.json
```
<br>

## ğŸ”¥ ç³»ç»ŸæŒ‡ä»¤ (System Prompt)
Qwen-1.8-Chat å’Œ Qwen-72B-Chat é€šä¹‰åƒé—®åœ¨å¤šæ ·ä¸”å­˜åœ¨å¤šè½®å¤æ‚äº¤äº’çš„ç³»ç»ŸæŒ‡ä»¤ä¸Šè¿›è¡Œäº†å……åˆ†è®­ç»ƒï¼Œä½¿æ¨¡å‹å¯ä»¥è·Ÿéšå¤šæ ·çš„ç³»ç»ŸæŒ‡ä»¤ï¼Œå®ç°ä¸Šä¸‹æ–‡(in-context)ä¸­çš„æ¨¡å‹å®šåˆ¶åŒ–ï¼Œè¿›ä¸€æ­¥æå‡äº†é€šä¹‰åƒé—®çš„å¯æ‰©å±•æ€§ã€‚

é€šè¿‡ç³»ç»ŸæŒ‡ä»¤ï¼ŒQwen-Chatèƒ½å¤Ÿå®ç°**è§’è‰²æ‰®æ¼”**ï¼Œ**è¯­è¨€é£æ ¼è¿ç§»**ï¼Œ**ä»»åŠ¡è®¾å®š**ï¼Œå’Œ**è¡Œä¸ºè®¾å®š**ç­‰èƒ½åŠ›ã€‚

![](assets/system_prompt_language_style.png)

![](assets/system_prompt_role_play_en.png)

æ›´å¤šå…³äºç³»ç»ŸæŒ‡ä»¤çš„ä»‹ç»ä¿¡æ¯å¯ä»¥å‚è€ƒ[ç¤ºä¾‹æ–‡æ¡£](examples/system_prompt.md).


## å·¥å…·è°ƒç”¨

Qwen-Chaté’ˆå¯¹å·¥å…·ä½¿ç”¨ã€å‡½æ•°è°ƒç”¨èƒ½åŠ›è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç”¨æˆ·å¯ä»¥å¼€å‘åŸºäºQwençš„Agentã€LangChainåº”ç”¨ã€ç”šè‡³Code Interpreterã€‚

æˆ‘ä»¬æä¾›äº†æ–‡æ¡£è¯´æ˜å¦‚ä½•æ ¹æ®ReAct Promptingçš„åŸç†å®ç°å·¥å…·è°ƒç”¨ï¼Œè¯·å‚è§[ReActç¤ºä¾‹](examples/react_prompt.md)ã€‚åŸºäºè¯¥åŸç†ï¼Œæˆ‘ä»¬åœ¨ [openai_api.py](openai_api.py) é‡Œæä¾›äº†å‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰çš„æ”¯æŒã€‚
æˆ‘ä»¬åœ¨å·²å¼€æºçš„ä¸­æ–‡[è¯„æµ‹æ•°æ®é›†](eval/EVALUATION.md)ä¸Šæµ‹è¯•æ¨¡å‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œå¹¶å‘ç°Qwen-Chatèƒ½å¤Ÿå–å¾—ç¨³å®šçš„è¡¨ç°ï¼š

<table>
    <tr>
        <th colspan="4" align="center">ä¸­æ–‡å·¥å…·è°ƒç”¨è¯„æµ‹åŸºå‡†ï¼ˆç‰ˆæœ¬ 20231206ï¼‰</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selection (Acc.â†‘)</th><th align="center">Tool Input (Rouge-Lâ†‘)</th><th align="center">False Positive Errorâ†“</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">98.0%</td><td align="center">0.953</td><td align="center">23.9%</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">74.5%</td><td align="center">0.807</td><td align="center">80.6%</td>
    </tr>
    <tr>
        <td>Qwen-1_8B-Chat</td><td align="center">85.0%</td><td align="center">0.839</td><td align="center">27.6%</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">95.5%</td><td align="center">0.900</td><td align="center">11.6%</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">96.9%</td><td align="center">0.917</td><td align="center">5.6%</td>
    </tr>
    <tr>
        <td>Qwen-72B-Chat</td><td align="center">98.2%</td><td align="center">0.927</td><td align="center">1.1%</td>
    </tr>
</table>

ä¸ºäº†è€ƒå¯ŸQwenä½¿ç”¨Python Code Interpreterå®Œæˆæ•°å­¦è§£é¢˜ã€æ•°æ®å¯è§†åŒ–ã€åŠæ–‡ä»¶å¤„ç†ä¸çˆ¬è™«ç­‰ä»»åŠ¡çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸“é—¨å»ºè®¾å¹¶å¼€æºäº†ä¸€ä¸ªè¯„æµ‹è¿™æ–¹é¢èƒ½åŠ›çš„[è¯„æµ‹åŸºå‡†](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark)ã€‚
æˆ‘ä»¬å‘ç°Qwenåœ¨ç”Ÿæˆä»£ç çš„å¯æ‰§è¡Œç‡ã€ç»“æœæ­£ç¡®æ€§ä¸Šå‡è¡¨ç°è¾ƒå¥½ï¼š

<table>
    <tr>
        <th colspan="5" align="center">Code Interpreter Benchmark (Version 20231206)</th>
    </tr>
    <tr>
        <th rowspan="2" align="center">Model</th>
        <th colspan="3" align="center">ä»£ç æ‰§è¡Œç»“æœæ­£ç¡®æ€§ (%)</th>
        <th colspan="1" align="center">ç”Ÿæˆä»£ç çš„å¯æ‰§è¡Œç‡ (%)</th>
    </tr>
    <tr>
        <th align="center">Mathâ†‘</th><th align="center">Visualization-Hardâ†‘</th><th align="center">Visualization-Easyâ†‘</th><th align="center">Generalâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td>
        <td align="center">82.8</td>
        <td align="center">66.7</td>
        <td align="center">60.8</td>
        <td align="center">82.8</td>
    </tr>
    <tr>
        <td>GPT-3.5</td>
        <td align="center">47.3</td>
        <td align="center">33.3</td>
        <td align="center">55.7</td>
        <td align="center">74.1</td>
    </tr>
    <tr>
        <td>LLaMA2-13B-Chat</td>
        <td align="center">8.3</td>
        <td align="center">1.2</td>
        <td align="center">15.2</td>
        <td align="center">48.3</td>
    </tr>
    <tr>
        <td>CodeLLaMA-13B-Instruct</td>
        <td align="center">28.2</td>
        <td align="center">15.5</td>
        <td align="center">21.5</td>
        <td align="center">74.1</td>
    </tr>
    <tr>
        <td>InternLM-20B-Chat</td>
        <td align="center">34.6</td>
        <td align="center">10.7</td>
        <td align="center">25.1</td>
        <td align="center">65.5</td>
    </tr>
    <tr>
        <td>ChatGLM3-6B</td>
        <td align="center">54.2</td>
        <td align="center">4.8</td>
        <td align="center">15.2</td>
        <td align="center">67.1</td>
    </tr>
    <tr>
        <td>Qwen-1.8B-Chat</td>
        <td align="center">25.6</td>
        <td align="center">21.4</td>
        <td align="center">22.8</td>
        <td align="center">65.5</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td>
        <td align="center">41.9</td>
        <td align="center">23.8</td>
        <td align="center">38.0</td>
        <td align="center">67.2</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td>
        <td align="center">58.4</td>
        <td align="center">31.0</td>
        <td align="center">45.6</td>
        <td align="center">65.5</td>
    </tr>
    <tr>
        <td>Qwen-72B-Chat</td>
        <td align="center">72.7</td>
        <td align="center">41.7</td>
        <td align="center">43.0</td>
        <td align="center">82.8</td>
    </tr>
</table>

<p align="center">
    <br>
    <img src="assets/code_interpreter_showcase_001.jpg" />
    <br>
<p>

<br>

## é•¿æ–‡æœ¬ç†è§£

æˆ‘ä»¬å¼•å…¥äº†NTKæ’å€¼ã€çª—å£æ³¨æ„åŠ›ã€LogNæ³¨æ„åŠ›ç¼©æ”¾ç­‰æŠ€æœ¯æ¥æå‡æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦å¹¶çªç ´è®­ç»ƒåºåˆ—é•¿åº¦çš„é™åˆ¶ï¼ŒåŸç”Ÿé•¿åº¦ä¸º2Kçš„Qwen-14Bå¯ä»¥æ‰©å±•åˆ°8Kçš„åºåˆ—é•¿åº¦ï¼Œè€ŒåŸç”Ÿé•¿åº¦8Kçš„Qwen-1.8B/7Bèƒ½å¤Ÿåœ¨32Ké•¿åºåˆ—çš„è®¾ç½®ä¸‹å–å¾—ä¸é”™çš„è¡¨ç°ã€‚

å¯¹äºQwen-72Bï¼Œæˆ‘ä»¬åŸºäºRoPEé‡‡ç”¨æ›´å¤§çš„æ—‹è½¬Baseæ¥é€‚åº”æ›´é•¿çš„ä¸Šä¸‹æ–‡ã€‚Qwen-72Bæ”¯æŒ32Kçš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚

é€šè¿‡arXivæ•°æ®é›†ä¸Šçš„è¯­è¨€æ¨¡å‹å®éªŒï¼Œå‘ç° Qwen åœ¨é•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸‹å¯ä»¥è¾¾åˆ°å‡ºè‰²çš„æ€§èƒ½ã€‚ç»“æœå¦‚ä¸‹ï¼š

<table>
    <tr>
        <th rowspan="2">Model</th><th colspan="6" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th><th align="center">16384</th><th align="center">32768</th>
    </tr>
     <tr>
        <td>Qwen-7B (original)</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">39.35</td><td align="center">469.81</td><td align="center">2645.09</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.59</td><td align="center">3.66</td><td align="center">5.71</td><td align="center">-</td>
    </tr>
    <tr>
            <td>Qwen-1.8B</td><td align="center"><b>5.00</b></td><td align="center"><b>4.48</b></td><td align="center"><b>4.13</b></td><td align="center"><b>3.89</b></td><td align="center">17.42</td><td align="center">433.85</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>5.00</b></td><td align="center"><b>4.48</b></td><td align="center"><b>4.14</b></td><td align="center"><b>3.93</b></td><td align="center"><b>3.82</b></td><td align="center"><b>3.83</b></td>
    </tr>
    <tr>
        <td>Qwen-7B</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.31</b></td><td align="center">7.27</td><td align="center">181.49</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.31</b></td><td align="center"><b>3.23</b></td><td align="center">3.33</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.33</b></td><td align="center"><b>3.22</b></td><td align="center"><b>3.17</b></td>
    </tr>
    <tr>
        <td>Qwen-14B</td><td align="center"><b>-</b></td><td align="center"><b>3.46</b></td><td align="center">22.79</td><td align="center">334.65</td><td align="center">3168.35</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>-</b></td><td align="center"><b>3.46</b></td><td align="center"><b>3.29</b></td><td align="center"><b>3.18</b></td><td align="center">3.42</td><td align="center">-</td>
    </tr>
    <tr>
        <td>Qwen-72B</td><td align="center"><b>-</b></td><td align="center"><b>-</b></td><td align="center">-</td><td align="center"><b>2.83</b></td><td align="center"><b>2.73</b></td><td align="center"><b>2.72</b></td>
    </tr>
</table>

è¿›ä¸€æ­¥ï¼Œæˆ‘ä»¬ä¸ºäº†éªŒè¯Qwen-72B-Chatåœ¨é•¿æ–‡æœ¬ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œåœ¨[L-Eval](https://arxiv.org/abs/2307.11088)å®¢è§‚é¢˜ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œè¯„åˆ†ç»“æœå¦‚ä¸‹ï¼š

| Model             | Input Length | Average   |  Coursera  |    GSM     |   QuALITY  |    TOEFL   |   CodeU    |  SFcition  |
|:------------------|:------------:|:---------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|
| ChatGPT-3.5-16k   |     16K      |   60.73   | **63.51**  | **84.00**  |   61.38    |    78.43   | **12.22**  |    64.84   |
| **Qwen-72B-Chat** |     32K      | **62.30** |   58.13    |   76.00    | **77.22**  |  **86.24** |    6.66    |  **69.53** |


æˆ‘ä»¬è¿›ä¸€æ­¥è¿›è¡Œäº†â€œå¤§æµ·æé’ˆâ€å®éªŒï¼ˆæƒ³æ³•æ¥è‡ªäº[@Greg Kamradt](https://twitter.com/GregKamradt/status/1727018183608193393)ï¼‰ï¼Œæµ‹è¯•æ¨¡å‹åœ¨ä¸åŒé•¿åº¦çš„è¾“å…¥ä¸‹ï¼Œæ˜¯å¦èƒ½æ£€ç´¢åˆ°æ–‡ç« ä¸åŒä½ç½®çš„ä¿¡æ¯ï¼Œç»“æœå¦‚ä¸‹ï¼š

![](assets/qwen_72b_needle_in_a_haystack.png)

ä»¥ä¸Šç»“æœè¯´æ˜ï¼ŒQwen-72B-Chatå¯ä»¥èƒ½å‡†ç¡®æ£€ç´¢åˆ°32Kä»¥å†…çš„è¾“å…¥é•¿åº¦ä¸­æ”¾åœ¨å„ç§ä½ç½®çš„ä¿¡æ¯ï¼Œè¯æ˜äº†å…¶å…·æœ‰ä¼˜ç§€çš„é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›ã€‚

## Tokenizer

> æ³¨ï¼šä½œä¸ºæœ¯è¯­çš„â€œtokenizerâ€åœ¨ä¸­æ–‡ä¸­å°šæ— å…±è¯†çš„æ¦‚å¿µå¯¹åº”ï¼Œæœ¬æ–‡æ¡£é‡‡ç”¨è‹±æ–‡è¡¨è¾¾ä»¥åˆ©è¯´æ˜ã€‚

åŸºäºtiktokençš„tokenizeræœ‰åˆ«äºå…¶ä»–åˆ†è¯å™¨ï¼Œæ¯”å¦‚sentencepiece tokenizerã€‚å°¤å…¶åœ¨å¾®è°ƒé˜¶æ®µï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„ç‰¹æ®Štokençš„ä½¿ç”¨ã€‚å…³äºtokenizerçš„æ›´å¤šä¿¡æ¯ï¼Œä»¥åŠå¾®è°ƒæ—¶æ¶‰åŠçš„ç›¸å…³ä½¿ç”¨ï¼Œè¯·å‚é˜…[æ–‡æ¡£](tokenization_note_zh.md)ã€‚
<br><br>

## å¤ç°

æˆ‘ä»¬æä¾›äº†è¯„æµ‹è„šæœ¬ä»¥ä¾›å¤ç°æˆ‘ä»¬çš„å®éªŒç»“æœã€‚æ³¨æ„ï¼Œç”±äºå†…éƒ¨ä»£ç å’Œå¼€æºä»£ç å­˜åœ¨å°‘è®¸å·®å¼‚ï¼Œè¯„æµ‹ç»“æœå¯èƒ½ä¸æ±‡æŠ¥ç»“æœå­˜åœ¨ç»†å¾®çš„ç»“æœä¸ä¸€è‡´ã€‚è¯·é˜…è¯»[eval/EVALUATION.md](eval/EVALUATION.md)äº†è§£æ›´å¤šä¿¡æ¯ã€‚
<br><br>

## FAQ

å¦‚é‡åˆ°é—®é¢˜ï¼Œæ•¬è¯·æŸ¥é˜…[FAQ](FAQ_zh.md)ä»¥åŠissueåŒºï¼Œå¦‚ä»æ— æ³•è§£å†³å†æäº¤issueã€‚
<br><br>

## å¼•ç”¨
å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ï¼

```
@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
```
<br>

## ä½¿ç”¨åè®®

<https://github.com/QwenLM/Qwen>ä¸­çš„æºä»£ç é‡‡ç”¨[Apache 2.0åè®®](./LICENSE)æˆæƒï¼Œæ‚¨å¯åœ¨è¯¥ä»“åº“æ ¹ç›®å½•æ‰¾åˆ°åè®®å…¨æ–‡ã€‚

ç ”ç©¶äººå‘˜ä¸å¼€å‘è€…å¯ä½¿ç”¨Qwenå’ŒQwen-Chatæˆ–è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€‚å¯¹äºå•†ä¸šä½¿ç”¨ï¼Œè¯·æŸ¥çœ‹æ¨¡å‹å„è‡ªçš„LICENSEã€‚

- Qwen-72Bã€Qwen-14Bå’ŒQwen-7Bé‡‡ç”¨[Tongyi Qianwen LICENSE AGREEMENT](./Tongyi%20Qianwen%20LICENSE%20AGREEMENT)æˆæƒï¼Œæ‚¨å¯åœ¨ç›¸åº”æ¨¡å‹çš„HuggingFaceæˆ–ModelScopeä»“åº“æ‰¾åˆ°åè®®åŸæ–‡ã€‚å¦‚éœ€å•†ç”¨ï¼Œæ‚¨åªéœ€éµå¾ªä½¿ç”¨åè®®è¿›è¡Œå•†ç”¨å³å¯ï¼Œæˆ‘ä»¬æ¬¢è¿æ‚¨å¡«å†™é—®å·([72B](https://dashscope.console.aliyun.com/openModelApply/Qwen-72B-Chat)ã€[14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat)ã€[7B](https://dashscope.console.aliyun.com/openModelApply/qianwen))ã€‚

- Qwen-1.8Bé‡‡ç”¨[Tongyi Qianwen RESEARCH LICENSE AGREEMENT](./Tongyi%20Qianwen%20RESEARCH%20LICENSE%20AGREEMENT)æˆæƒï¼Œæ‚¨å¯åœ¨ç›¸åº”æ¨¡å‹çš„HuggingFaceæˆ–ModelScopeä»“åº“æ‰¾åˆ°åè®®åŸæ–‡ã€‚å¦‚éœ€å•†ç”¨ï¼Œè¯·è”ç³»æˆ‘ä»¬ã€‚

<br><br>

## è”ç³»æˆ‘ä»¬

å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œæ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤å’ŒDiscord serverã€‚å½“ç„¶ä¹Ÿå¯ä»¥é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚

