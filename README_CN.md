<p align="left">
        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>&nbsp ï½œ &nbsp<a href="README_JA.md">æ—¥æœ¬èª</a>
</p>
<br><br>

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg" width="400"/>
<p>
<br>

<p align="center">
        ğŸ¤— <a href="https://huggingface.co/Qwen">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href="https://modelscope.cn/models/qwen">é­”æ­ç¤¾åŒº</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf">è®ºæ–‡</a> &nbsp&nbsp ï½œ &nbsp&nbspğŸ–¥ï¸ <a href="https://modelscope.cn/studios/qwen/Qwen-14B-Chat-Demo/summary">Demo</a>
<br>
<a href="assets/wechat.png">å¾®ä¿¡</a>&nbsp&nbsp ï½œ &nbsp&nbsp é’‰é’‰ &nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp&nbsp
</p>
<br><br>

|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                                                            Qwen                                                            |
|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|
| 7B  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat">ğŸ¤—</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int4">ğŸ¤—</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B">ğŸ¤—</a>  |
| 14B | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int4">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B">ğŸ¤—</a> |

æˆ‘ä»¬å¼€æºäº†**Qwen**ï¼ˆé€šä¹‰åƒé—®ï¼‰ç³»åˆ—å·¥ä½œï¼Œå½“å‰å¼€æºæ¨¡å‹çš„å‚æ•°è§„æ¨¡ä¸º70äº¿ï¼ˆ7Bï¼‰å’Œ140äº¿ï¼ˆ14Bï¼‰ã€‚æœ¬æ¬¡å¼€æºåŒ…æ‹¬åŸºç¡€æ¨¡å‹**Qwen**ï¼Œå³**Qwen-7B**å’Œ**Qwen-14B**ï¼Œä»¥åŠå¯¹è¯æ¨¡å‹**Qwen-Chat**ï¼Œå³**Qwen-7B-Chat**å’Œ**Qwen-14B-Chat**ã€‚æ¨¡å‹é“¾æ¥åœ¨è¡¨æ ¼ä¸­ï¼Œè¯·ç‚¹å‡»äº†è§£è¯¦æƒ…ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å…¬å¼€äº†æˆ‘ä»¬çš„**[æŠ€æœ¯æŠ¥å‘Š](https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf)**ï¼Œè¯·ç‚¹å‡»ä¸Šæ–¹è®ºæ–‡é“¾æ¥æŸ¥çœ‹ã€‚

å½“å‰åŸºç¡€æ¨¡å‹å·²ç»ç¨³å®šè®­ç»ƒäº†å¤§è§„æ¨¡é«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„æ•°æ®ï¼Œè¦†ç›–å¤šè¯­è¨€ï¼ˆå½“å‰ç»ä»¥ä¸­æ–‡å’Œè‹±æ–‡ä¸ºä¸»ï¼‰ï¼Œæ€»é‡é«˜è¾¾3ä¸‡äº¿tokenã€‚åœ¨ç›¸å…³åŸºå‡†è¯„æµ‹ä¸­ï¼ŒQwenç³»åˆ—æ¨¡å‹æ‹¿å‡ºéå¸¸æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼Œæ˜¾è‘—è¶…å‡ºåŒè§„æ¨¡æ¨¡å‹å¹¶ç´§è¿½ä¸€ç³»åˆ—æœ€å¼ºçš„é—­æºæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨SFTå’ŒRLHFæŠ€æœ¯å®ç°å¯¹é½ï¼Œä»åŸºåº§æ¨¡å‹è®­ç»ƒå¾—åˆ°å¯¹è¯æ¨¡å‹ã€‚Qwen-Chatå…·å¤‡èŠå¤©ã€æ–‡å­—åˆ›ä½œã€æ‘˜è¦ã€ä¿¡æ¯æŠ½å–ã€ç¿»è¯‘ç­‰èƒ½åŠ›ï¼ŒåŒæ—¶è¿˜å…·å¤‡ä¸€å®šçš„ä»£ç ç”Ÿæˆå’Œç®€å•æ•°å­¦æ¨ç†çš„èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬é’ˆå¯¹LLMå¯¹æ¥å¤–éƒ¨ç³»ç»Ÿç­‰æ–¹é¢é’ˆå¯¹æ€§åœ°åšäº†ä¼˜åŒ–ï¼Œå½“å‰å…·å¤‡è¾ƒå¼ºçš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œä»¥åŠæœ€è¿‘å¤‡å—å…³æ³¨çš„Code Interpreterçš„èƒ½åŠ›å’Œæ‰®æ¼”Agentçš„èƒ½åŠ›ã€‚

åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œä½ å¯ä»¥äº†è§£åˆ°ä»¥ä¸‹å†…å®¹

* å¿«é€Ÿä¸Šæ‰‹Qwen-Chatæ•™ç¨‹ï¼Œç©è½¬å¤§æ¨¡å‹æ¨ç†.
* é‡åŒ–æ¨¡å‹ç›¸å…³ç»†èŠ‚ï¼ŒåŒ…æ‹¬ç”¨æ³•ã€æ˜¾å­˜å ç”¨ã€æ¨ç†æ€§èƒ½ç­‰ã€‚è¿™éƒ¨åˆ†è¿˜æä¾›äº†å’Œéé‡åŒ–æ¨¡å‹çš„å¯¹æ¯”ã€‚
* å¾®è°ƒçš„æ•™ç¨‹ï¼Œå¸®ä½ å®ç°å…¨å‚æ•°å¾®è°ƒã€LoRAä»¥åŠQ-LoRAã€‚
* æ­å»ºDemoçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬WebUIå’ŒCLI Demo
* æ›´å¤šå…³äºQwenåœ¨å·¥å…·è°ƒç”¨ã€Code Interpreterã€Agentæ–¹é¢çš„å†…å®¹
* é•¿åºåˆ—ç†è§£èƒ½åŠ›åŠè¯„æµ‹
* ä½¿ç”¨åè®®
* ...

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·ä¼˜å…ˆè€ƒè™‘æŸ¥è¯¢[FAQ](FAQ.md)ã€‚å¦‚ä»æœªè§£å†³ï¼Œéšæ—¶æå‡ºissueï¼ˆä½†å»ºè®®ä½¿ç”¨è‹±è¯­æˆ–æä¾›ç¿»è¯‘ï¼Œæœ‰åŠ©äºå¸®åŠ©æ›´å¤šç”¨æˆ·ï¼‰ã€‚å¦‚æœæƒ³å¸®åŠ©æˆ‘ä»¬æå‡ï¼Œæ¬¢è¿æäº¤Pull Requestsï¼

æƒ³å’Œæˆ‘ä»¬ä¸€èµ·è®¨è®ºå’ŒèŠå¤©çš„è¯ï¼Œèµ¶ç´§åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤å’ŒDiscord serverï¼ˆå…¥å£è§æ–‡æ¡£å¼€å¤´éƒ¨åˆ†ï¼‰ï¼
<br><br>

## æ–°é—»

* 2023å¹´9æœˆ25æ—¥ ğŸ”¥ åœ¨é­”æ­ç¤¾åŒºï¼ˆModelScopeï¼‰å’ŒHugging Faceæ¨å‡º**Qwen-14B**å’Œ**Qwen-14B-Chat**æ¨¡å‹ï¼Œå¹¶å¼€æº [qwen.cpp](https://github.com/QwenLM/qwen.cpp) å’Œ [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent)ã€‚**Qwen-7B**å’Œ**Qwen-7B-Chat**çš„ä»£ç å’Œæ¨¡å‹ä¹ŸåŒæ­¥å¾—åˆ°æ›´æ–°ã€‚**è¯·ä½¿ç”¨æœ€æ–°çš„ä»£ç å’Œæ¨¡å‹ï¼**
    - ç›¸æ¯”åŸç‰ˆQwen-7Bï¼Œæ–°ç‰ˆç”¨äº†æ›´å¤šè®­ç»ƒæ•°æ®ï¼ˆä»2.2Tå¢åŠ åˆ°2.4T tokensï¼‰ï¼Œåºåˆ—é•¿åº¦ä»2048æ‰©å±•è‡³8192ã€‚æ•´ä½“ä¸­æ–‡èƒ½åŠ›ä»¥åŠä»£ç èƒ½åŠ›å‡æœ‰æ‰€æå‡ã€‚
* 2023å¹´9æœˆ12æ—¥ æ”¯æŒQwen-7Bå’ŒQwen-7B-Chatçš„å¾®è°ƒï¼Œå…¶ä¸­åŒ…æ‹¬å…¨å‚æ•°å¾®è°ƒã€LoRAä»¥åŠQ-LoRAã€‚
* 2023å¹´8æœˆ21æ—¥ å‘å¸ƒQwen-7B-Chatçš„Int4é‡åŒ–æ¨¡å‹ï¼ŒQwen-7B-Chat-Int4ã€‚è¯¥æ¨¡å‹æ˜¾å­˜å ç”¨ä½ï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åŠç²¾åº¦æ¨¡å‹æ˜¾è‘—æå‡ï¼Œåœ¨åŸºå‡†è¯„æµ‹ä¸Šæ•ˆæœæŸå¤±è¾ƒå°ã€‚
* 2023å¹´8æœˆ3æ—¥ åœ¨é­”æ­ç¤¾åŒºï¼ˆModelScopeï¼‰å’ŒHugging FaceåŒæ­¥æ¨å‡ºQwen-7Bå’ŒQwen-7B-Chatæ¨¡å‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æŠ€æœ¯å¤‡å¿˜å½•ï¼Œä»‹ç»äº†ç›¸å…³çš„è®­ç»ƒç»†èŠ‚å’Œæ¨¡å‹è¡¨ç°ã€‚
<br>

## è¯„æµ‹è¡¨ç°

Qwen-14BåŠQwen-7B (æœ€æ–°ç‰ˆæœ¬ä½¿ç”¨æ›´å¤§é‡çš„tokenè¿›è¡Œé¢„è®­ç»ƒ)ç›¸æ¯”åŒè§„æ¨¡æ¨¡å‹å‡å®ç°äº†æ•ˆæœçš„æ˜¾è‘—æå‡ã€‚æˆ‘ä»¬è¯„æµ‹çš„æ•°æ®é›†åŒ…æ‹¬MMLUã€C-Evalã€ GSM8Kã€ MATHã€HumanEvalã€MBPPã€BBHç­‰æ•°æ®é›†ï¼Œè€ƒå¯Ÿçš„èƒ½åŠ›åŒ…æ‹¬è‡ªç„¶è¯­è¨€ç†è§£ã€çŸ¥è¯†ã€æ•°å­¦è®¡ç®—å’Œæ¨ç†ã€ä»£ç ç”Ÿæˆã€é€»è¾‘æ¨ç†ç­‰ã€‚å½“ç„¶ï¼Œå³ä¾¿Qwen-14Bç›¸æ¯”GPT-3.5å’ŒGPT-4ä»æœ‰å·®è·ã€‚ 

<p align="left">
    <img src="assets/radar_14b.jpg" width="600"/>
<p>
<br>

| Model                  |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP    |   BBH    |  CMMLU   |
|:-----------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:---------:|:--------:|:--------:|
|                        |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot   |  3-shot  |  5-shot  |
| LLaMA2-7B              |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8    |   38.2   |   31.8   |
| LLaMA2-13B             |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3    |   45.6   |   38.4   |
| LLaMA2-34B             |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0    |   44.1   |    -     |
| ChatGLM2-6B            |   47.9   |   51.7   |   32.4   |   6.5    |     -     |     -     |   33.7   |    -     |
| InternLM-7B            |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0    |   37.0   |   51.8   |
| InternLM-20B           |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6    |   52.5   |   59.0   |
| Baichuan2-7B           |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2    |   41.6   |   57.1   |
| Baichuan2-13B          |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2    |   49.0   |   62.0   |
| **Qwen-7B (original)** |   56.7   |   59.6   |   51.6   |     10.4     |   24.4    |   31.2    |   40.6   |   58.8   |
| **Qwen-7B**            |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6    |   45.0   |   62.2   |
| **Qwen-14B**           | **66.3** | **72.1** | **61.3** | **24.8** | **32.3**  | **40.8**  | **53.4** | **71.0** |


å¯¹äºä»¥ä¸Šæ‰€æœ‰å¯¹æ¯”æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ—å‡ºäº†å…¶å®˜æ–¹æ±‡æŠ¥ç»“æœä¸[OpenCompass](https://opencompass.org.cn/leaderboard-llm)ç»“æœä¹‹é—´çš„æœ€ä½³åˆ†æ•°ã€‚

æ›´å¤šçš„å®éªŒç»“æœå’Œç»†èŠ‚è¯·æŸ¥çœ‹æˆ‘ä»¬çš„æŠ€æœ¯å¤‡å¿˜å½•ã€‚ç‚¹å‡»[è¿™é‡Œ](https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf)ã€‚
<br><br>

## è¦æ±‚

* python 3.8åŠä»¥ä¸Šç‰ˆæœ¬
* pytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œæ¨è2.0åŠä»¥ä¸Šç‰ˆæœ¬
* å»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·ã€flash-attentionç”¨æˆ·ç­‰éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰
<br>

## å¿«é€Ÿä½¿ç”¨

æˆ‘ä»¬æä¾›ç®€å•çš„ç¤ºä¾‹æ¥è¯´æ˜å¦‚ä½•åˆ©ç”¨ğŸ¤– ModelScopeå’ŒğŸ¤— Transformerså¿«é€Ÿä½¿ç”¨Qwen-7Bå’ŒQwen-7B-Chatã€‚

åœ¨å¼€å§‹å‰ï¼Œè¯·ç¡®ä¿ä½ å·²ç»é…ç½®å¥½ç¯å¢ƒå¹¶å®‰è£…å¥½ç›¸å…³çš„ä»£ç åŒ…ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œç¡®ä¿ä½ æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œç„¶åå®‰è£…ç›¸å…³çš„ä¾èµ–åº“ã€‚

```bash
pip install -r requirements.txt
```

å¦‚æœä½ çš„æ˜¾å¡æ”¯æŒfp16æˆ–bf16ç²¾åº¦ï¼Œæˆ‘ä»¬è¿˜æ¨èå®‰è£…[flash-attention](https://github.com/Dao-AILab/flash-attention)æ¥æé«˜ä½ çš„è¿è¡Œæ•ˆç‡ä»¥åŠé™ä½æ˜¾å­˜å ç”¨ã€‚(**flash-attentionåªæ˜¯å¯é€‰é¡¹ï¼Œä¸å®‰è£…ä¹Ÿå¯æ­£å¸¸è¿è¡Œè¯¥é¡¹ç›®**)

```bash
git clone -b v1.0.8 https://github.com/Dao-AILab/flash-attention
cd flash-attention && pip install .
# ä¸‹æ–¹å®‰è£…å¯é€‰ï¼Œå®‰è£…å¯èƒ½æ¯”è¾ƒç¼“æ…¢ã€‚
# pip install csrc/layer_norm
# pip install csrc/rotary
```

æ¥ä¸‹æ¥ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨Transformersæˆ–è€…ModelScopeæ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚

#### ğŸ¤— Transformers

å¦‚å¸Œæœ›ä½¿ç”¨Qwen-chatè¿›è¡Œæ¨ç†ï¼Œæ‰€éœ€è¦å†™çš„åªæ˜¯å¦‚ä¸‹æ‰€ç¤ºçš„æ•°è¡Œä»£ç ã€‚**è¯·ç¡®ä¿ä½ ä½¿ç”¨çš„æ˜¯æœ€æ–°ä»£ç ï¼Œå¹¶æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹åç§°å’Œè·¯å¾„ï¼Œå¦‚`Qwen/Qwen-7B-Chat`å’Œ`Qwen/Qwen-14B-Chat`**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# å¯é€‰çš„æ¨¡å‹åŒ…æ‹¬: "Qwen/Qwen-7B-Chat", "Qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤ä½¿ç”¨è‡ªåŠ¨æ¨¡å¼ï¼Œæ ¹æ®è®¾å¤‡è‡ªåŠ¨é€‰æ‹©ç²¾åº¦
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True).eval()

# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# ç¬¬ä¸€è½®å¯¹è¯
response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚

# ç¬¬äºŒè½®å¯¹è¯
response, history = model.chat(tokenizer, "ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚", history=history)
print(response)
# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚
# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚
# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚
# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚
# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚
# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚

# ç¬¬ä¸‰è½®å¯¹è¯
response, history = model.chat(tokenizer, "ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜", history=history)
print(response)
# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹
```

è¿è¡ŒQwenåŒæ ·éå¸¸ç®€å•ã€‚

<details>
  <summary>è¿è¡ŒQwen</summary>

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# å¯é€‰çš„æ¨¡å‹åŒ…æ‹¬: "Qwen/Qwen-7B", "Qwen/Qwen-14B"
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)

# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤ä½¿ç”¨è‡ªåŠ¨æ¨¡å¼ï¼Œæ ¹æ®è®¾å¤‡è‡ªåŠ¨é€‰æ‹©ç²¾åº¦
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True).eval()

# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)

inputs = tokenizer('è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯', return_tensors='pt')
inputs = inputs.to(model.device)
pred = model.generate(**inputs)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))
# è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯äºšçš„æ–¯äºšè´å·´ï¼ˆAddis Ababaï¼‰...
```

</details>

#### ğŸ¤– ModelScope

é­”æ­ï¼ˆModelScopeï¼‰æ˜¯å¼€æºçš„æ¨¡å‹å³æœåŠ¡å…±äº«å¹³å°ï¼Œä¸ºæ³›AIå¼€å‘è€…æä¾›çµæ´»ã€æ˜“ç”¨ã€ä½æˆæœ¬çš„ä¸€ç«™å¼æ¨¡å‹æœåŠ¡äº§å“ã€‚ä½¿ç”¨ModelScopeåŒæ ·éå¸¸ç®€å•ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from modelscope import AutoModelForCausalLM, AutoTokenizer
from modelscope import GenerationConfig

# å¯é€‰çš„æ¨¡å‹åŒ…æ‹¬: "qwen/Qwen-7B-Chat", "qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("qwen/Qwen-7B-Chat", revision='v1.0.5', trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("qwen/Qwen-7B-Chat", revision='v1.0.5', device_map="auto", trust_remote_code=True, fp16=True).eval()
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", revision='v1.0.5', trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚

response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
response, history = model.chat(tokenizer, "æµ™æ±Ÿçš„çœä¼šåœ¨å“ªé‡Œï¼Ÿ", history=history) 
print(response)
response, history = model.chat(tokenizer, "å®ƒæœ‰ä»€ä¹ˆå¥½ç©çš„æ™¯ç‚¹", history=history)
print(response)
```
<br>

## é‡åŒ–

### ç”¨æ³•

**è¯·æ³¨æ„ï¼šæˆ‘ä»¬æ›´æ–°é‡åŒ–æ–¹æ¡ˆä¸ºåŸºäº[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)çš„é‡åŒ–ï¼Œæä¾›Int4é‡åŒ–æ¨¡å‹ï¼ŒåŒ…æ‹¬Qwen-7B-Chat [Click here](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4)å’ŒQwen-14B-Chat [Click here](https://huggingface.co/Qwen/Qwen-14B-Chat-Int4)ã€‚è¯¥æ–¹æ¡ˆåœ¨æ¨¡å‹è¯„æµ‹æ•ˆæœå‡ ä¹æ— æŸï¼Œä¸”å­˜å‚¨éœ€æ±‚æ›´ä½ï¼Œæ¨ç†é€Ÿåº¦æ›´ä¼˜ã€‚**

ä»¥ä¸‹æˆ‘ä»¬æä¾›ç¤ºä¾‹è¯´æ˜å¦‚ä½•ä½¿ç”¨Int4é‡åŒ–æ¨¡å‹ã€‚åœ¨å¼€å§‹ä½¿ç”¨å‰ï¼Œè¯·å…ˆä¿è¯æ»¡è¶³è¦æ±‚ï¼ˆå¦‚torch 2.0åŠä»¥ä¸Šï¼Œtransformersç‰ˆæœ¬ä¸º4.32.0åŠä»¥ä¸Šï¼Œç­‰ç­‰ï¼‰ï¼Œå¹¶å®‰è£…æ‰€éœ€å®‰è£…åŒ…ï¼š

```bash
pip install auto-gptq optimum
```

å¦‚å®‰è£…`auto-gptq`é‡åˆ°é—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨åˆ°å®˜æ–¹[repo](https://github.com/PanQiWei/AutoGPTQ)æœç´¢åˆé€‚çš„wheelã€‚

éšåå³å¯ä½¿ç”¨å’Œä¸Šè¿°ä¸€è‡´çš„ç”¨æ³•è°ƒç”¨é‡åŒ–æ¨¡å‹ï¼š

```python
# å¯é€‰æ¨¡å‹åŒ…æ‹¬ï¼š"Qwen/Qwen-7B-Chat-Int4", "Qwen/Qwen-14B-Chat-Int4"
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat-Int4",
    device_map="auto",
    trust_remote_code=True
).eval()
response, history = model.chat(tokenizer, "Hi", history=None)
```
### æ•ˆæœè¯„æµ‹

æˆ‘ä»¬å¯¹BF16å’ŒInt4æ¨¡å‹åœ¨åŸºå‡†è¯„æµ‹ä¸Šåšäº†æµ‹è¯•ï¼Œå‘ç°é‡åŒ–æ¨¡å‹æ•ˆæœæŸå¤±è¾ƒå°ï¼Œç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |
|----------------------|:----:|:-----------:|:-----:|:---------:|
| Qwen-7B-Chat (BF16)  | 53.9 |    54.2     | 41.1  |   24.4    |
| Qwen-7B-Chat (Int4)  | 52.6 |    52.9     | 38.1  |   23.8    |
| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 61.0  |   43.9    |
| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |

### æ¨ç†é€Ÿåº¦

æˆ‘ä»¬æµ‹ç®—äº†BF16å’ŒInt4æ¨¡å‹ç”Ÿæˆ2048å’Œ8192ä¸ªtokençš„å¹³å‡æ¨ç†é€Ÿåº¦ï¼ˆtokens/sï¼‰ã€‚å¦‚å›¾æ‰€ç¤ºï¼š

| Quantization         | Speed (2048 tokens) | Speed (8192 tokens) |
|----------------------|:-------------------:|:-------------------:|
| Qwen-7B-Chat (BF16)  |        30.34        |        29.32        |
| Qwen-7B-Chat (Int4)  |        43.56        |        33.92        |
| Qwen-14B-Chat (BF16) |        30.70        |        21.73        |
| Qwen-14B-Chat (Int4) |        37.11        |        26.11        |

å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è®°å½•åœ¨é•¿åº¦ä¸º1çš„ä¸Šä¸‹æ–‡çš„æ¡ä»¶ä¸‹ç”Ÿæˆ8192ä¸ªtokençš„æ€§èƒ½ã€‚è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼Œä½¿ç”¨PyTorch 2.0.1å’ŒCUDA 11.4ã€‚æ¨ç†é€Ÿåº¦æ˜¯ç”Ÿæˆ8192ä¸ªtokençš„é€Ÿåº¦å‡å€¼ã€‚

### æ˜¾å­˜ä½¿ç”¨

æˆ‘ä»¬è¿˜æµ‹ç®—äº†BF16å’ŒInt4æ¨¡å‹ç¼–ç 2048ä¸ªtokenåŠç”Ÿæˆ8192ä¸ªtokençš„å³°å€¼æ˜¾å­˜å ç”¨æƒ…å†µã€‚ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

| Quantization         | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |
|----------------------|:-----------------------------------:|:-------------------------------------:|
| Qwen-7B-Chat (BF16)  |               17.66GB               |                22.58GB                |
| Qwen-7B-Chat (Int4)  |               8.21GB                |                13.62GB                |
| Qwen-14B-Chat (BF16) |               30.15GB                 |                38.94GB                  |
| Qwen-14B-Chat (Int4) |               13.00GB                 |                21.79GB                  |

ä¸Šè¿°æ€§èƒ½æµ‹ç®—ä½¿ç”¨[æ­¤è„šæœ¬](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py)å®Œæˆã€‚
<br><br>

## KV cacheé‡åŒ–

åœ¨æ¨¡å‹inferæ—¶ï¼Œå¯ä»¥å°†ä¸­é—´ç»“æœkeyä»¥åŠvalueçš„å€¼é‡åŒ–åå‹ç¼©å­˜å‚¨ï¼Œè¿™æ ·ä¾¿å¯ä»¥åœ¨ç›¸åŒçš„å¡ä¸Šå­˜å‚¨æ›´å¤šçš„keyä»¥åŠvalueï¼Œå¢åŠ æ ·æœ¬ååã€‚

### ä½¿ç”¨æ–¹æ³•
æä¾›use_cache_quantizationä»¥åŠuse_cache_kernelä¸¤ä¸ªå‚æ•°å¯¹æ¨¡å‹æ§åˆ¶ï¼Œå½“use_cache_quantizationä»¥åŠuse_cache_kernelå‡å¼€å¯æ—¶ï¼Œå°†å¯åŠ¨kv-cacheé‡åŒ–çš„åŠŸèƒ½ã€‚å…·ä½“ä½¿ç”¨å¦‚ä¸‹ï¼š
```python
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat",
     device_map="auto",
     trust_remote_code=True,
     use_cache_quantization=True,
     use_cache_kernel=True,
     use_flash_attn=False
)
```
æ³¨æ„ï¼šå½“å‰è¯¥åŠŸèƒ½ç›®å‰ä¸æ”¯æŒä¸flash attnåŒæ—¶å¼€å¯ï¼Œå¦‚æœä½ å¼€äº†kv cacheé‡åŒ–çš„åŒæ—¶åˆå¼€äº†flash attnï¼ˆuse_flash_attn=Trueï¼Œ use_cache_quantization=True, use_cache_kernel=Trueï¼‰ï¼Œä¼šé»˜è®¤å°†use_flash_attnå…³é—­ã€‚

### ç»“æœå¯¹æ¯”
#### æ•ˆæœ
æˆ‘ä»¬éªŒè¯è¿‡int8 kv-cacheçš„ä½¿ç”¨å¯¹æ¨¡å‹æ•´ä½“çš„ç²¾åº¦æŒ‡æ ‡åŸºæœ¬æ— æŸã€‚

#### æ˜¾å­˜å¯¹æ¯”
æœ¬æ¬¡è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼Œæ¨¡å‹é»˜è®¤ä½¿ç”¨BF16æ ¼å¼ï¼Œé»˜è®¤ç”Ÿæˆçš„seq-length=1024ï¼ˆç”Ÿæˆ1024ä¸ªtokenï¼‰ï¼Œå…¶ä¸­oomè¡¨ç¤ºout of memoryã€‚

å¼€å¯äº†kv-cacheé‡åŒ–ä¹‹åï¼Œæ¨¡å‹åœ¨inferçš„æ—¶å€™å¯ä»¥å¼€å¯æ›´å¤§çš„batch size(bs)

| USE KVCache | bs=1 | bs=4 | bs=16 | bs=32 | bs=64 | bs=100 |
| --- | :---: | :---: | :---: | :---: | :---: | :---: |
| no | 16.3GB | 24.1GB | 31.7GB | 48.7GB   | oom  |  oom |
| yes | 15.5GB | 17.2GB | 22.3GB | 30.2GB  | 48.2GB  |  72.4GB |


å¼€å¯äº†kv-cacheé‡åŒ–ä¹‹åï¼Œæ¨¡å‹åœ¨inferæ—¶é¢„æµ‹æ›´é•¿çš„seq-lengthï¼ˆslï¼Œç”Ÿæˆçš„tokenæ•°ï¼‰ç»“æœæ—¶ï¼Œå¯ä»¥èŠ‚çº¦æ›´å¤šçš„æ˜¾å­˜ã€‚

| USE KVCache | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |
| --- | :---: | :---: | :---: | :---: | :---: |
| no | 15.2GB | 16.3GB | 17.6GB | 19.5GB  | 23.2GB  |
| yes | 15GB | 15.5GB | 15.8GB | 16.6GB  | 17.6GB  |


### å­˜å‚¨æ ¼å¼åŒºåˆ«
æ¨¡å‹å¼€å¯kv cacheé‡åŒ–åå†æ¨¡å‹inferçš„æ—¶å€™ï¼Œä¼šå°†åŸå§‹å­˜è¿›layer_pastçš„floatæ ¼å¼çš„key/valueå˜æˆint8æ ¼å¼çš„qkey/qvalueå’Œç›¸å¯¹åº”çš„é‡åŒ–å‚æ•°ã€‚
å…·ä½“æ“ä½œå¦‚ä¸‹ï¼š
1ã€å°†key/valueè¿›è¡Œé‡åŒ–æ“ä½œ
```
    qv,scale,zero_point=quantize_cache_v(v)
```
2ã€å­˜å…¥layer_pastä¸­:
é‡åŒ–æ ¼å¼çš„layer_past:
```
    layer_past=((q_key,key_scale,key_zero_point),
                (q_value,value_scale,value_zero_point))
```
åŸå§‹æ ¼å¼çš„layer_past:
```
    layer_past=(key,value)
```
å¦‚æœéœ€è¦å°†layer_pastä¸­å­˜å¥½çš„keyï¼Œvalueç›´æ¥å–å‡ºä½¿ç”¨ï¼Œå¯ä»¥ä½¿ç”¨åé‡åŒ–æ“ä½œå°†int8æ ¼å¼çš„key/valueè½¬å›floatæ ¼å¼ï¼š
```
    v=dequantize_cache_torch(qv,scale,zero_point)
```

## å¾®è°ƒ

æˆ‘ä»¬æä¾›äº†`finetune.py`è¿™ä¸ªè„šæœ¬ä¾›ç”¨æˆ·å®ç°åœ¨è‡ªå·±çš„æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒçš„åŠŸèƒ½ï¼Œä»¥æ¥å…¥ä¸‹æ¸¸ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†shellè„šæœ¬å‡å°‘ç”¨æˆ·çš„å·¥ä½œé‡ã€‚è¿™ä¸ªè„šæœ¬æ”¯æŒ [DeepSpeed](https://github.com/microsoft/DeepSpeed) å’Œ [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/) ã€‚æˆ‘ä»¬æä¾›çš„shellè„šæœ¬ä½¿ç”¨äº†DeepSpeedï¼Œå› æ­¤å»ºè®®æ‚¨ç¡®ä¿å·²ç»å®‰è£…DeepSpeedã€‚

é¦–å…ˆï¼Œä½ éœ€è¦å‡†å¤‡ä½ çš„è®­ç»ƒæ•°æ®ã€‚ä½ éœ€è¦å°†æ‰€æœ‰æ ·æœ¬æ”¾åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­å¹¶å­˜å…¥jsonæ–‡ä»¶ä¸­ã€‚æ¯ä¸ªæ ·æœ¬å¯¹åº”ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«idå’Œconversationï¼Œå…¶ä¸­åè€…ä¸ºä¸€ä¸ªåˆ—è¡¨ã€‚ç¤ºä¾‹å¦‚ä¸‹æ‰€ç¤ºï¼š
```json
[
  {
    "id": "identity_0",
    "conversations": [
      {
        "from": "user",
        "value": "ä½ å¥½",
      },
      {
        "from": "assistant",
        "value": "æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚"
      }
    ]
  }
]
```

å‡†å¤‡å¥½æ•°æ®åï¼Œä½ å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æä¾›çš„shellè„šæœ¬å®ç°å¾®è°ƒã€‚æ³¨æ„ï¼Œä½ éœ€è¦åœ¨è„šæœ¬ä¸­æŒ‡å®šä½ çš„æ•°æ®çš„è·¯å¾„ã€‚

å¾®è°ƒè„šæœ¬èƒ½å¤Ÿå¸®ä½ å®ç°ï¼š
- å…¨å‚æ•°å¾®è°ƒ
- LoRA
- Q-LoRA

å…¨å‚æ•°å¾®è°ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚ä½ å¯ä»¥è¿è¡Œè¿™ä¸ªè„šæœ¬å¼€å§‹è®­ç»ƒï¼š

```bash
# åˆ†å¸ƒå¼è®­ç»ƒã€‚ç”±äºæ˜¾å­˜é™åˆ¶å°†å¯¼è‡´å•å¡è®­ç»ƒå¤±è´¥ï¼Œæˆ‘ä»¬ä¸æä¾›å•å¡è®­ç»ƒè„šæœ¬ã€‚
sh finetune/finetune_ds.sh
```

å°¤å…¶æ³¨æ„ï¼Œä½ éœ€è¦åœ¨è„šæœ¬ä¸­æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹åç§°æˆ–è·¯å¾„ã€æ•°æ®è·¯å¾„ã€ä»¥åŠæ¨¡å‹è¾“å‡ºçš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚åœ¨è¿™ä¸ªè„šæœ¬ä¸­æˆ‘ä»¬ä½¿ç”¨äº†DeepSpeed ZeRO 3ã€‚å¦‚æœä½ æƒ³ä¿®æ”¹è¿™ä¸ªé…ç½®ï¼Œå¯ä»¥åˆ é™¤æ‰`--deepspeed`è¿™ä¸ªè¾“å…¥æˆ–è€…è‡ªè¡Œæ ¹æ®éœ€æ±‚ä¿®æ”¹DeepSpeedé…ç½®jsonæ–‡ä»¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒï¼Œå› æ­¤ä½ å¯ä»¥è®¾ç½®`--bf16 True`æˆ–è€…`--fp16 True`ã€‚ç»éªŒä¸Šï¼Œå¦‚æœä½ çš„æœºå™¨æ”¯æŒbf16ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨bf16ï¼Œè¿™æ ·å¯ä»¥å’Œæˆ‘ä»¬çš„é¢„è®­ç»ƒå’Œå¯¹é½è®­ç»ƒä¿æŒä¸€è‡´ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬æŠŠé»˜è®¤é…ç½®è®¾ä¸ºå®ƒçš„åŸå› ã€‚

è¿è¡ŒLoRAçš„æ–¹æ³•ç±»ä¼¼å…¨å‚æ•°å¾®è°ƒã€‚ä½†åœ¨å¼€å§‹å‰ï¼Œè¯·ç¡®ä¿å·²ç»å®‰è£…`peft`ä»£ç åº“ã€‚å¦å¤–ï¼Œè®°ä½è¦è®¾ç½®æ­£ç¡®çš„æ¨¡å‹ã€æ•°æ®å’Œè¾“å‡ºè·¯å¾„ã€‚æˆ‘ä»¬å»ºè®®ä½ ä¸ºæ¨¡å‹è·¯å¾„ä½¿ç”¨ç»å¯¹è·¯å¾„ã€‚è¿™æ˜¯å› ä¸ºLoRAä»…å­˜å‚¨adapteréƒ¨åˆ†å‚æ•°ï¼Œè€Œadapteré…ç½®jsonæ–‡ä»¶è®°å½•äº†é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ï¼Œç”¨äºè¯»å–é¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€‚åŒæ ·ï¼Œä½ å¯ä»¥è®¾ç½®bf16æˆ–è€…fp16ã€‚

```bash
# å•å¡è®­ç»ƒ
sh finetune/finetune_lora_single_gpu.sh
# åˆ†å¸ƒå¼è®­ç»ƒ
sh finetune/finetune_lora_ds.sh
```

ä¸å…¨å‚æ•°å¾®è°ƒä¸åŒï¼ŒLoRA ([è®ºæ–‡](https://arxiv.org/abs/2106.09685)) åªæ›´æ–°adapterå±‚çš„å‚æ•°è€Œæ— éœ€æ›´æ–°åŸæœ‰è¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚è¿™ç§æ–¹æ³•å…è®¸ç”¨æˆ·ç”¨æ›´ä½çš„æ˜¾å­˜å¼€é”€æ¥è®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿæ„å‘³ç€æ›´å°çš„è®¡ç®—å¼€é”€ã€‚ç„¶è€Œï¼Œå¦‚æœä½ ä¾ç„¶é‡åˆ°æ˜¾å­˜ä¸è¶³çš„é—®é¢˜ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨Q-LoRA ([è®ºæ–‡](https://arxiv.org/abs/2305.14314))ã€‚è¯¥æ–¹æ³•ä½¿ç”¨4æ¯”ç‰¹é‡åŒ–æ¨¡å‹ä»¥åŠpaged attentionç­‰æŠ€æœ¯å®ç°æ›´å°çš„æ˜¾å­˜å¼€é”€ã€‚è¿è¡ŒQ-LoRAä½ åªéœ€è¿è¡Œå¦‚ä¸‹è„šæœ¬ï¼ˆç›®å‰QLoRAåœ¨å•å¡è®­ç»ƒæ—¶æ··åˆç²¾åº¦æš‚æ—¶è¿˜å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œæˆ‘ä»¬ä¼šå°½å¿«å®Œæˆä¿®å¤å’Œæ›´æ–°ï¼‰ï¼š

```bash
# åˆ†å¸ƒå¼è®­ç»ƒ
sh finetune/finetune_qlora_ds.sh
```

æˆ‘ä»¬å»ºè®®ä½ ä½¿ç”¨æˆ‘ä»¬æä¾›çš„Int4é‡åŒ–æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå³Qwen-7B-Chat-Int4ã€‚ç„¶è€Œï¼Œä¸å…¨å‚æ•°å¾®è°ƒä»¥åŠLoRAä¸åŒï¼ŒQ-LoRAä»…æ”¯æŒfp16ã€‚

ä¸å…¨å‚æ•°å¾®è°ƒä¸åŒï¼ŒLoRAå’ŒQ-LoRAçš„è®­ç»ƒåªéœ€å­˜å‚¨adapteréƒ¨åˆ†çš„å‚æ•°ã€‚å‡å¦‚ä½ éœ€è¦ä½¿ç”¨LoRAè®­ç»ƒåçš„æ¨¡å‹ï¼Œä½ éœ€è¦ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ã€‚å‡è®¾ä½ ä½¿ç”¨Qwen-7Bè®­ç»ƒæ¨¡å‹ï¼Œä½ å¯ä»¥ç”¨å¦‚ä¸‹ä»£ç è¯»å–æ¨¡å‹ï¼š

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()
```

ä¸Šè¿°shellè„šæœ¬ä½¿ç”¨`torchrun`æ¥è¿è¡Œå•GPUå’Œå¤šGPUè®­ç»ƒã€‚åˆ†å¸ƒå¼è®­ç»ƒéœ€è¦æ ¹æ®ä½ çš„éœ€æ±‚å’Œæœºå™¨æŒ‡å®šæ­£ç¡®çš„åˆ†å¸ƒå¼è®­ç»ƒè¶…å‚æ•°ã€‚
<br><br>

## Demo

### Web UI

æˆ‘ä»¬æä¾›äº†Web UIçš„demoä¾›ç”¨æˆ·ä½¿ç”¨ (æ„Ÿè°¢ @wysaid æ”¯æŒ)ã€‚åœ¨å¼€å§‹å‰ï¼Œç¡®ä¿å·²ç»å®‰è£…å¦‚ä¸‹ä»£ç åº“ï¼š

```bash
pip install -r requirements_web_demo.txt
```

éšåè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå¹¶ç‚¹å‡»ç”Ÿæˆé“¾æ¥ï¼š

```bash
python web_demo.py
```

<p align="center">
    <br>
    <img src="assets/web_demo.gif" width="600" />
    <br>
<p>

### äº¤äº’å¼Demo

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€å•çš„äº¤äº’å¼Demoç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹`cli_demo.py`ã€‚å½“å‰æ¨¡å‹å·²ç»æ”¯æŒæµå¼è¾“å‡ºï¼Œç”¨æˆ·å¯é€šè¿‡è¾“å…¥æ–‡å­—çš„æ–¹å¼å’ŒQwen-7B-Chatäº¤äº’ï¼Œæ¨¡å‹å°†æµå¼è¾“å‡ºè¿”å›ç»“æœã€‚è¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š

```bash
python cli_demo.py
```

<p align="center">
    <br>
    <img src="assets/cli_demo.gif" width="600" />
    <br>
<p>
<br>

## API

æˆ‘ä»¬æä¾›äº†OpenAI APIæ ¼å¼çš„æœ¬åœ°APIéƒ¨ç½²æ–¹æ³•ï¼ˆæ„Ÿè°¢@hanpenggitï¼‰ã€‚åœ¨å¼€å§‹ä¹‹å‰å…ˆå®‰è£…å¿…è¦çš„ä»£ç åº“ï¼š

```bash
pip install fastapi uvicorn openai "pydantic>=2.3.0" sse_starlette
```

éšåå³å¯è¿è¡Œä»¥ä¸‹å‘½ä»¤éƒ¨ç½²ä½ çš„æœ¬åœ°APIï¼š

```bash
python openai_api.py
```

ä½ ä¹Ÿå¯ä»¥ä¿®æ”¹å‚æ•°ï¼Œæ¯”å¦‚`-c`æ¥ä¿®æ”¹æ¨¡å‹åç§°æˆ–è·¯å¾„, `--cpu-only`æ”¹ä¸ºCPUéƒ¨ç½²ç­‰ç­‰ã€‚å¦‚æœéƒ¨ç½²å‡ºç°é—®é¢˜ï¼Œæ›´æ–°ä¸Šè¿°ä»£ç åº“å¾€å¾€å¯ä»¥è§£å†³å¤§å¤šæ•°é—®é¢˜ã€‚

ä½¿ç”¨APIåŒæ ·éå¸¸ç®€å•ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
import openai
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "none"

# ä½¿ç”¨æµå¼å›å¤çš„è¯·æ±‚
for chunk in openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ],
    stream=True
    # æµå¼è¾“å‡ºçš„è‡ªå®šä¹‰stopwordsåŠŸèƒ½å°šæœªæ”¯æŒï¼Œæ­£åœ¨å¼€å‘ä¸­
):
    if hasattr(chunk.choices[0].delta, "content"):
        print(chunk.choices[0].delta.content, end="", flush=True)

# ä¸ä½¿ç”¨æµå¼å›å¤çš„è¯·æ±‚
response = openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ],
    stream=False,
    stop=[] # åœ¨æ­¤å¤„æ·»åŠ è‡ªå®šä¹‰çš„stop words ä¾‹å¦‚ReAct promptingæ—¶éœ€è¦å¢åŠ ï¼š stop=["Observation:"]ã€‚
)
print(response.choices[0].message.content)
```

<p align="center">
    <br>
    <img src="assets/openai_api.gif" width="600" />
    <br>
<p>

è¯¥æ¥å£ä¹Ÿæ”¯æŒå‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰ï¼Œä½†æš‚æ—¶ä»…é™ `stream=False` æ—¶èƒ½ç”Ÿæ•ˆã€‚ç”¨æ³•è§[å‡½æ•°è°ƒç”¨ç¤ºä¾‹](examples/function_call_examples.py)ã€‚
<br><br>

## éƒ¨ç½²

åœ¨CPUä¸Šè¿è¡Œéå¸¸ç®€å•ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
```

å¦‚æœä½ é‡åˆ°æ˜¾å­˜ä¸è¶³çš„é—®é¢˜è€Œå¸Œæœ›ä½¿ç”¨å¤šå¼ GPUè¿›è¡Œæ¨ç†ï¼Œå¯ä»¥ä½¿ç”¨æä¾›çš„è„šæœ¬`utils.py`:

```python
from utils import load_model_on_gpus
model = load_model_on_gpus('Qwen/Qwen-7B-Chat', num_gpus=2)
```

ä½ å³å¯ä½¿ç”¨2å¼ GPUè¿›è¡Œæ¨ç†ã€‚
<br><br>

æˆ‘ä»¬åŒæ—¶æä¾›äº†Qwen-LMå’Œtiktokençš„C++å®ç°, æ›´å¤šç»†èŠ‚è¯·æŸ¥çœ‹[qwen.cpp](https://github.com/QwenLM/qwen.cpp).

## å·¥å…·è°ƒç”¨

Qwen-Chaté’ˆå¯¹å·¥å…·ä½¿ç”¨ã€å‡½æ•°è°ƒç”¨èƒ½åŠ›è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç”¨æˆ·å¯ä»¥å¼€å‘åŸºäºQwençš„Agentã€LangChainåº”ç”¨ã€ç”šè‡³Code Interpreterã€‚

æˆ‘ä»¬æä¾›äº†æ–‡æ¡£è¯´æ˜å¦‚ä½•æ ¹æ®ReAct Promptingçš„åŸç†å®ç°å·¥å…·è°ƒç”¨ï¼Œè¯·å‚è§[ReActç¤ºä¾‹](examples/react_prompt.md)ã€‚åŸºäºè¯¥åŸç†ï¼Œæˆ‘ä»¬åœ¨ [openai_api.py](openai_api.py) é‡Œæä¾›äº†å‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰çš„æ”¯æŒã€‚
æˆ‘ä»¬åœ¨å·²å¼€æºçš„ä¸­æ–‡[è¯„æµ‹æ•°æ®é›†](eval/EVALUATION.md)ä¸Šæµ‹è¯•æ¨¡å‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œå¹¶å‘ç°Qwen-Chatèƒ½å¤Ÿå–å¾—ç¨³å®šçš„è¡¨ç°ï¼š

<table>
    <tr>
        <th colspan="4" align="center">ä¸­æ–‡å·¥å…·è°ƒç”¨è¯„æµ‹åŸºå‡†</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selection (Acc.â†‘)</th><th align="center">Tool Input (Rouge-Lâ†‘)</th><th align="center">False Positive Errorâ†“</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">95%</td><td align="center">0.90</td><td align="center">15.0%</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">85%</td><td align="center">0.88</td><td align="center">75.0%</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">98%</td><td align="center">0.91</td><td align="center">7.3%</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">98%</td><td align="center">0.93</td><td align="center">2.4%</td>
    </tr>
</table>

ä¸ºäº†è€ƒå¯ŸQwenä½¿ç”¨Python Code Interpreterå®Œæˆæ•°å­¦è§£é¢˜ã€æ•°æ®å¯è§†åŒ–ã€åŠæ–‡ä»¶å¤„ç†ä¸çˆ¬è™«ç­‰ä»»åŠ¡çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸“é—¨å»ºè®¾å¹¶å¼€æºäº†ä¸€ä¸ªè¯„æµ‹è¿™æ–¹é¢èƒ½åŠ›çš„[è¯„æµ‹åŸºå‡†](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark)ã€‚
æˆ‘ä»¬å‘ç°Qwenåœ¨ç”Ÿæˆä»£ç çš„å¯æ‰§è¡Œç‡ã€ç»“æœæ­£ç¡®æ€§ä¸Šå‡è¡¨ç°è¾ƒå¥½ï¼š

<table>
    <tr>
        <th colspan="4" align="center">ç”Ÿæˆä»£ç çš„å¯æ‰§è¡Œç‡ (%)</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Mathâ†‘</th><th align="center">Visualizationâ†‘</th><th align="center">Generalâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">91.9</td><td align="center">85.9</td><td align="center">82.8</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">89.2</td><td align="center">65.0</td><td align="center">74.1</td>
    </tr>
    <tr>
        <td>LLaMA2-7B-Chat</td>
        <td align="center">41.9</td>
        <td align="center">33.1</td>
        <td align="center">24.1 </td>
    </tr>
    <tr>
        <td>LLaMA2-13B-Chat</td>
        <td align="center">50.0</td>
        <td align="center">40.5</td>
        <td align="center">48.3 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-7B-Instruct</td>
        <td align="center">85.1</td>
        <td align="center">54.0</td>
        <td align="center">70.7 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-13B-Instruct</td>
        <td align="center">93.2</td>
        <td align="center">55.8</td>
        <td align="center">74.1 </td>
    </tr>
    <tr>
        <td>InternLM-7B-Chat-v1.1</td>
        <td align="center">78.4</td>
        <td align="center">44.2</td>
        <td align="center">62.1 </td>
    </tr>
    <tr>
        <td>InternLM-20B-Chat</td>
        <td align="center">70.3</td>
        <td align="center">44.2</td>
        <td align="center">65.5 </td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td>
        <td align="center">82.4</td>
        <td align="center">64.4</td>
        <td align="center">67.2 </td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td>
        <td align="center">89.2</td>
        <td align="center">84.1</td>
        <td align="center">65.5</td>
    </tr>
</table>

<table>
    <tr>
        <th colspan="4" align="center">ä»£ç æ‰§è¡Œç»“æœçš„æ­£ç¡®ç‡ (%)</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Mathâ†‘</th><th align="center">Visualization-Hardâ†‘</th><th align="center">Visualization-Easyâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">82.8</td><td align="center">66.7</td><td align="center">60.8</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">47.3</td><td align="center">33.3</td><td align="center">55.7</td>
    </tr>
    <tr>
        <td>LLaMA2-7B-Chat</td>
        <td align="center">3.9</td>
        <td align="center">14.3</td>
        <td align="center">39.2 </td>
    </tr>
    <tr>
        <td>LLaMA2-13B-Chat</td>
        <td align="center">8.3</td>
        <td align="center">8.3</td>
        <td align="center">40.5 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-7B-Instruct</td>
        <td align="center">14.3</td>
        <td align="center">26.2</td>
        <td align="center">60.8 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-13B-Instruct</td>
        <td align="center">28.2</td>
        <td align="center">27.4</td>
        <td align="center">62.0 </td>
    </tr>
    <tr>
        <td>InternLM-7B-Chat-v1.1</td>
        <td align="center">28.5</td>
        <td align="center">4.8</td>
        <td align="center">40.5 </td>
    </tr>
    <tr>
        <td>InternLM-20B-Chat</td>
        <td align="center">34.6</td>
        <td align="center">21.4</td>
        <td align="center">45.6 </td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td>
        <td align="center">41.9</td>
        <td align="center">40.5</td>
        <td align="center">54.4 </td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td>
        <td align="center">58.4</td>
        <td align="center">53.6</td>
        <td align="center">59.5</td>
    </tr>
</table>

<p align="center">
    <br>
    <img src="assets/code_interpreter_showcase_001.jpg" />
    <br>
<p>

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹å…·å¤‡æ‰®æ¼”HuggingFace Agentçš„èƒ½åŠ›ï¼Œè¯¦è§[ç¤ºä¾‹æ–‡æ¡£](examples/transformers_agent.md)äº†è§£æ›´å¤šä¿¡æ¯ã€‚æ¨¡å‹åœ¨Hugging Faceæä¾›çš„è¯„æµ‹æ•°æ®é›†ä¸Šè¡¨ç°å¦‚ä¸‹ï¼š

<table>
    <tr>
        <th colspan="4" align="center">HuggingFace Agentè¯„æµ‹åŸºå‡† - Runæ¨¡å¼</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selectionâ†‘</th><th align="center">Tool Usedâ†‘</th><th align="center">Codeâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">100</td><td align="center">100</td><td align="center">97.4</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">95.4</td><td align="center">96.3</td><td align="center">87.0</td>
    </tr>
    <tr>
        <td>StarCoder-Base-15B</td><td align="center">86.1</td><td align="center">87.0</td><td align="center">68.9</td>
    </tr>
    <tr>
        <td>StarCoder-15B</td><td align="center">87.0</td><td align="center">88.0</td><td align="center">68.9</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">87.0</td><td align="center">87.0</td><td align="center">71.5</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">93.5</td><td align="center">94.4</td><td align="center">87.0</td>
    </tr>
</table>

<table>
    <tr>
        <th colspan="4" align="center">HuggingFace Agentè¯„æµ‹åŸºå‡† - Chatæ¨¡å¼</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selectionâ†‘</th><th align="center">Tool Usedâ†‘</th><th align="center">Codeâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">98.5</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">97.3</td><td align="center">96.8</td><td align="center">89.6</td>
    </tr>
    <tr>
        <td>StarCoder-Base-15B</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">91.1</td>
    </tr>
    <tr>
        <td>StarCoder-15B</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">89.6</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">94.7</td><td align="center">94.7</td><td align="center">85.1</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">95.5</td>
    </tr>
</table>

<br>

## é•¿æ–‡æœ¬ç†è§£

æˆ‘ä»¬å¼•å…¥äº†NTKæ’å€¼ã€çª—å£æ³¨æ„åŠ›ã€LogNæ³¨æ„åŠ›ç¼©æ”¾ç­‰æŠ€æœ¯æ¥æå‡æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦å¹¶çªç ´è®­ç»ƒåºåˆ—é•¿åº¦çš„é™åˆ¶ã€‚é€šè¿‡arXivæ•°æ®é›†ä¸Šçš„è¯­è¨€æ¨¡å‹å®éªŒï¼Œæˆ‘ä»¬çš„åŸç”Ÿé•¿åº¦ä¸º2Kçš„Qwen-7B/14Båœ¨8Kçš„åºåˆ—é•¿åº¦ä¸‹ä¾ç„¶è¡¨ç°ä¸é”™ï¼Œè€ŒåŸç”Ÿé•¿åº¦æ‰©å±•åˆ°8Kçš„Qwen-7Bèƒ½å¤Ÿåœ¨32Ké•¿åºåˆ—çš„è®¾ç½®ä¸‹å–å¾—ä¸é”™çš„è¡¨ç°ã€‚

<table>
    <tr>
        <th rowspan="2">Model</th><th colspan="6" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th><th align="center">16384</th><th align="center">32768</th>
    </tr>
     <tr>
        <td>Qwen-7B (original)</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">39.35</td><td align="center">469.81</td><td align="center">2645.09</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.59</td><td align="center">3.66</td><td align="center">5.71</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.58</td><td align="center">3.56</td><td align="center">4.62</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.58</td><td align="center">3.49</td><td align="center">4.32</td><td align="center">-</td>
    </tr>
    <tr>
    <tr>
        <td>Qwen-7B</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.31</b></td><td align="center">7.27</td><td align="center">181.49</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.31</b></td><td align="center"><b>3.23</b></td><td align="center">3.33</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.33</b></td><td align="center"><b>3.22</b></td><td align="center"><b>3.17</b></td>
    </tr>
    <tr>
        <td>Qwen-14B</td><td align="center"><b>-</b></td><td align="center"><b>3.46</b></td><td align="center">22.79</td><td align="center">334.65</td><td align="center">3168.35</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>-</b></td><td align="center"><b>3.46</b></td><td align="center"><b>3.29</b></td><td align="center"><b>3.18</b></td><td align="center">3.42</td><td align="center">-</td>
    </tr>
</table>

## Tokenization

> æ³¨ï¼šä½œä¸ºæœ¯è¯­çš„â€œtokenizationâ€åœ¨ä¸­æ–‡ä¸­å°šæ— å…±è¯†çš„æ¦‚å¿µå¯¹åº”ï¼Œæœ¬æ–‡æ¡£é‡‡ç”¨è‹±æ–‡è¡¨è¾¾ä»¥åˆ©è¯´æ˜ã€‚

åŸºäºtiktokençš„tokenizeræœ‰åˆ«äºå…¶ä»–åˆ†è¯å™¨ï¼Œæ¯”å¦‚sentencepiece tokenizerã€‚å°¤å…¶åœ¨å¾®è°ƒé˜¶æ®µï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„ç‰¹æ®Štokençš„ä½¿ç”¨ã€‚å…³äºtokenizerçš„æ›´å¤šä¿¡æ¯ï¼Œä»¥åŠå¾®è°ƒæ—¶æ¶‰åŠçš„ç›¸å…³ä½¿ç”¨ï¼Œè¯·å‚é˜…[æ–‡æ¡£](tokenization_note_zh.md)ã€‚
<br><br>

## å¤ç°

æˆ‘ä»¬æä¾›äº†è¯„æµ‹è„šæœ¬ä»¥ä¾›å¤ç°æˆ‘ä»¬çš„å®éªŒç»“æœã€‚æ³¨æ„ï¼Œç”±äºå†…éƒ¨ä»£ç å’Œå¼€æºä»£ç å­˜åœ¨å°‘è®¸å·®å¼‚ï¼Œè¯„æµ‹ç»“æœå¯èƒ½ä¸æ±‡æŠ¥ç»“æœå­˜åœ¨ç»†å¾®çš„ç»“æœä¸ä¸€è‡´ã€‚è¯·é˜…è¯»[eval/EVALUATION.md](eval/EVALUATION.md)äº†è§£æ›´å¤šä¿¡æ¯ã€‚
<br><br>

## FAQ

å¦‚é‡åˆ°é—®é¢˜ï¼Œæ•¬è¯·æŸ¥é˜…[FAQ](FAQ_zh.md)ä»¥åŠissueåŒºï¼Œå¦‚ä»æ— æ³•è§£å†³å†æäº¤issueã€‚
<br><br>

## ä½¿ç”¨åè®®

ç ”ç©¶äººå‘˜ä¸å¼€å‘è€…å¯ä½¿ç”¨Qwenå’ŒQwen-Chatæˆ–è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€‚æˆ‘ä»¬åŒæ ·å…è®¸å•†ä¸šä½¿ç”¨ï¼Œå…·ä½“ç»†èŠ‚è¯·æŸ¥çœ‹[LICENSE](LICENSE)ã€‚å¦‚éœ€å•†ç”¨ï¼Œè¯·å¡«å†™é—®å·([7B](https://dashscope.console.aliyun.com/openModelApply/qianwen), [14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat))ç”³è¯·ã€‚
<br><br>

## è”ç³»æˆ‘ä»¬

å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œæ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤å’ŒDiscord serverã€‚å½“ç„¶ä¹Ÿå¯ä»¥é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚

