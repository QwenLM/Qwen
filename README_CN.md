<p align="left">
    ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>&nbsp ï½œ &nbsp<a href="README_JA.md">æ—¥æœ¬èª</a> ï½œ &nbsp<a href="README_FR.md">FranÃ§ais</a>
</p>
<br><br>

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg" width="400"/>
<p>
<br>

<p align="center">
    ğŸ¤— <a href="https://huggingface.co/Qwen">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href="https://modelscope.cn/organization/qwen">é­”æ­ç¤¾åŒº</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href="https://arxiv.org/abs/2309.16609">è®ºæ–‡</a> &nbsp&nbsp ï½œ &nbsp&nbspğŸ–¥ï¸ <a href="https://modelscope.cn/studios/qwen/Qwen-14B-Chat-Demo/summary">Demo</a>
<br>
<a href="assets/wechat.png">å¾®ä¿¡</a>&nbsp&nbsp ï½œ &nbsp&nbsp é’‰é’‰ &nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp&nbsp
</p>
<br><br>

|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                        Qwen-Chat (Int8)                         |                                                            Qwen                                                            |
|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|
| 7B  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat">ğŸ¤—</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int4">ğŸ¤—</a>  | <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int8">ğŸ¤—</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B">ğŸ¤—</a>  |
| 14B | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int4">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int8">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B">ğŸ¤—</a> |

æˆ‘ä»¬å¼€æºäº†**Qwen**ï¼ˆé€šä¹‰åƒé—®ï¼‰ç³»åˆ—å·¥ä½œï¼Œå½“å‰å¼€æºæ¨¡å‹çš„å‚æ•°è§„æ¨¡ä¸º70äº¿ï¼ˆ7Bï¼‰å’Œ140äº¿ï¼ˆ14Bï¼‰ã€‚æœ¬æ¬¡å¼€æºåŒ…æ‹¬åŸºç¡€æ¨¡å‹**Qwen**ï¼Œå³**Qwen-7B**å’Œ**Qwen-14B**ï¼Œä»¥åŠå¯¹è¯æ¨¡å‹**Qwen-Chat**ï¼Œå³**Qwen-7B-Chat**å’Œ**Qwen-14B-Chat**ã€‚æ¨¡å‹é“¾æ¥åœ¨è¡¨æ ¼ä¸­ï¼Œè¯·ç‚¹å‡»äº†è§£è¯¦æƒ…ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å…¬å¼€äº†æˆ‘ä»¬çš„<b><a href="https://arxiv.org/abs/2309.16609">æŠ€æœ¯æŠ¥å‘Š</a></b>ï¼Œè¯·ç‚¹å‡»ä¸Šæ–¹è®ºæ–‡é“¾æ¥æŸ¥çœ‹ã€‚

å½“å‰åŸºç¡€æ¨¡å‹å·²ç»ç¨³å®šè®­ç»ƒäº†å¤§è§„æ¨¡é«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„æ•°æ®ï¼Œè¦†ç›–å¤šè¯­è¨€ï¼ˆå½“å‰ä»¥ä¸­æ–‡å’Œè‹±æ–‡ä¸ºä¸»ï¼‰ï¼Œæ€»é‡é«˜è¾¾3ä¸‡äº¿tokenã€‚åœ¨ç›¸å…³åŸºå‡†è¯„æµ‹ä¸­ï¼ŒQwenç³»åˆ—æ¨¡å‹æ‹¿å‡ºéå¸¸æœ‰ç«äº‰åŠ›çš„è¡¨ç°ï¼Œæ˜¾è‘—è¶…å‡ºåŒè§„æ¨¡æ¨¡å‹å¹¶ç´§è¿½ä¸€ç³»åˆ—æœ€å¼ºçš„é—­æºæ¨¡å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åˆ©ç”¨SFTå’ŒRLHFæŠ€æœ¯å®ç°å¯¹é½ï¼Œä»åŸºåº§æ¨¡å‹è®­ç»ƒå¾—åˆ°å¯¹è¯æ¨¡å‹ã€‚Qwen-Chatå…·å¤‡èŠå¤©ã€æ–‡å­—åˆ›ä½œã€æ‘˜è¦ã€ä¿¡æ¯æŠ½å–ã€ç¿»è¯‘ç­‰èƒ½åŠ›ï¼ŒåŒæ—¶è¿˜å…·å¤‡ä¸€å®šçš„ä»£ç ç”Ÿæˆå’Œç®€å•æ•°å­¦æ¨ç†çš„èƒ½åŠ›ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬é’ˆå¯¹LLMå¯¹æ¥å¤–éƒ¨ç³»ç»Ÿç­‰æ–¹é¢é’ˆå¯¹æ€§åœ°åšäº†ä¼˜åŒ–ï¼Œå½“å‰å…·å¤‡è¾ƒå¼ºçš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œä»¥åŠæœ€è¿‘å¤‡å—å…³æ³¨çš„Code Interpreterçš„èƒ½åŠ›å’Œæ‰®æ¼”Agentçš„èƒ½åŠ›ã€‚

åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œä½ å¯ä»¥äº†è§£åˆ°ä»¥ä¸‹å†…å®¹

* å¿«é€Ÿä¸Šæ‰‹Qwen-Chatæ•™ç¨‹ï¼Œç©è½¬å¤§æ¨¡å‹æ¨ç†
* é‡åŒ–æ¨¡å‹ç›¸å…³ç»†èŠ‚ï¼ŒåŒ…æ‹¬GPTQå’ŒKV cacheé‡åŒ–
* æ¨ç†æ€§èƒ½æ•°æ®ï¼ŒåŒ…æ‹¬æ¨ç†é€Ÿåº¦å’Œæ˜¾å­˜å ç”¨
* å¾®è°ƒçš„æ•™ç¨‹ï¼Œå¸®ä½ å®ç°å…¨å‚æ•°å¾®è°ƒã€LoRAä»¥åŠQ-LoRA
* éƒ¨ç½²æ•™ç¨‹ï¼Œä»¥vLLMå’ŒFastChatä¸ºä¾‹
* æ­å»ºDemoçš„æ–¹æ³•ï¼ŒåŒ…æ‹¬WebUIå’ŒCLI Demo
* æ­å»ºAPIçš„æ–¹æ³•ï¼Œæˆ‘ä»¬æä¾›çš„ç¤ºä¾‹ä¸ºOpenAIé£æ ¼çš„API
* æ›´å¤šå…³äºQwenåœ¨å·¥å…·è°ƒç”¨ã€Code Interpreterã€Agentæ–¹é¢çš„å†…å®¹
* é•¿åºåˆ—ç†è§£èƒ½åŠ›åŠè¯„æµ‹
* ä½¿ç”¨åè®®
* ...

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·ä¼˜å…ˆè€ƒè™‘æŸ¥è¯¢[FAQ](FAQ.md)ã€‚å¦‚ä»æœªè§£å†³ï¼Œéšæ—¶æå‡ºissueï¼ˆä½†å»ºè®®ä½¿ç”¨è‹±è¯­æˆ–æä¾›ç¿»è¯‘ï¼Œæœ‰åŠ©äºå¸®åŠ©æ›´å¤šç”¨æˆ·ï¼‰ã€‚å¦‚æœæƒ³å¸®åŠ©æˆ‘ä»¬æå‡ï¼Œæ¬¢è¿æäº¤Pull Requestsï¼

æƒ³å’Œæˆ‘ä»¬ä¸€èµ·è®¨è®ºå’ŒèŠå¤©çš„è¯ï¼Œèµ¶ç´§åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤å’ŒDiscord serverï¼ˆå…¥å£è§æ–‡æ¡£å¼€å¤´éƒ¨åˆ†ï¼‰ï¼
<br><br>

## æ–°é—»

* 2023å¹´10æœˆ17æ—¥ æˆ‘ä»¬æ¨å‡ºäº†Int8é‡åŒ–æ¨¡å‹**Qwen-7B-Chat-Int8**å’Œ**Qwen-14B-Chat-Int8**ã€‚
* 2023å¹´9æœˆ25æ—¥ ğŸ”¥ åœ¨é­”æ­ç¤¾åŒºï¼ˆModelScopeï¼‰å’ŒHugging Faceæ¨å‡º**Qwen-14B**å’Œ**Qwen-14B-Chat**æ¨¡å‹ï¼Œå¹¶å¼€æº [qwen.cpp](https://github.com/QwenLM/qwen.cpp) å’Œ [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent)ã€‚**Qwen-7B**å’Œ**Qwen-7B-Chat**çš„ä»£ç å’Œæ¨¡å‹ä¹ŸåŒæ­¥å¾—åˆ°æ›´æ–°ã€‚**è¯·ä½¿ç”¨æœ€æ–°çš„ä»£ç å’Œæ¨¡å‹ï¼**
    - ç›¸æ¯”åŸç‰ˆQwen-7Bï¼Œæ–°ç‰ˆç”¨äº†æ›´å¤šè®­ç»ƒæ•°æ®ï¼ˆä»2.2Tå¢åŠ åˆ°2.4T tokensï¼‰ï¼Œåºåˆ—é•¿åº¦ä»2048æ‰©å±•è‡³8192ã€‚æ•´ä½“ä¸­æ–‡èƒ½åŠ›ä»¥åŠä»£ç èƒ½åŠ›å‡æœ‰æ‰€æå‡ã€‚
* 2023å¹´9æœˆ12æ—¥ æ”¯æŒQwen-7Bå’ŒQwen-7B-Chatçš„å¾®è°ƒï¼Œå…¶ä¸­åŒ…æ‹¬å…¨å‚æ•°å¾®è°ƒã€LoRAä»¥åŠQ-LoRAã€‚
* 2023å¹´8æœˆ21æ—¥ å‘å¸ƒQwen-7B-Chatçš„Int4é‡åŒ–æ¨¡å‹ï¼ŒQwen-7B-Chat-Int4ã€‚è¯¥æ¨¡å‹æ˜¾å­˜å ç”¨ä½ï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åŠç²¾åº¦æ¨¡å‹æ˜¾è‘—æå‡ï¼Œåœ¨åŸºå‡†è¯„æµ‹ä¸Šæ•ˆæœæŸå¤±è¾ƒå°ã€‚
* 2023å¹´8æœˆ3æ—¥ åœ¨é­”æ­ç¤¾åŒºï¼ˆModelScopeï¼‰å’ŒHugging FaceåŒæ­¥æ¨å‡ºQwen-7Bå’ŒQwen-7B-Chatæ¨¡å‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æŠ€æœ¯å¤‡å¿˜å½•ï¼Œä»‹ç»äº†ç›¸å…³çš„è®­ç»ƒç»†èŠ‚å’Œæ¨¡å‹è¡¨ç°ã€‚
<br>

## è¯„æµ‹è¡¨ç°

Qwen-14BåŠQwen-7B (æœ€æ–°ç‰ˆæœ¬ä½¿ç”¨æ›´å¤§é‡çš„tokenè¿›è¡Œé¢„è®­ç»ƒ)ç›¸æ¯”åŒè§„æ¨¡æ¨¡å‹å‡å®ç°äº†æ•ˆæœçš„æ˜¾è‘—æå‡ã€‚æˆ‘ä»¬è¯„æµ‹çš„æ•°æ®é›†åŒ…æ‹¬MMLUã€C-Evalã€ GSM8Kã€ MATHã€HumanEvalã€MBPPã€BBHç­‰æ•°æ®é›†ï¼Œè€ƒå¯Ÿçš„èƒ½åŠ›åŒ…æ‹¬è‡ªç„¶è¯­è¨€ç†è§£ã€çŸ¥è¯†ã€æ•°å­¦è®¡ç®—å’Œæ¨ç†ã€ä»£ç ç”Ÿæˆã€é€»è¾‘æ¨ç†ç­‰ã€‚å½“ç„¶ï¼Œå³ä¾¿Qwen-14Bç›¸æ¯”GPT-3.5å’ŒGPT-4ä»æœ‰å·®è·ã€‚ 

<p align="left">
    <img src="assets/radar_14b.jpg" width="600"/>
<p>
<br>

| Model                  |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP   |   BBH    |  CMMLU   |
|:-----------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:|
|                        |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot  |  3-shot  |  5-shot  |
| LLaMA2-7B              |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8   |   38.2   |   31.8   |
| LLaMA2-13B             |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3   |   45.6   |   38.4   |
| LLaMA2-34B             |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0   |   44.1   |    -     |
| ChatGLM2-6B            |   47.9   |   51.7   |   32.4   |   6.5    |     -     |    -     |   33.7   |    -     |
| InternLM-7B            |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0   |   37.0   |   51.8   |
| InternLM-20B           |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6   |   52.5   |   59.0   |
| Baichuan2-7B           |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2   |   41.6   |   57.1   |
| Baichuan2-13B          |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2   |   49.0   |   62.0   |
| **Qwen-7B (original)** |   56.7   |   59.6   |   51.6   |   10.4   |   24.4    |   31.2   |   40.6   |   58.8   |
| **Qwen-7B**            |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6   |   45.0   |   62.2   |
| **Qwen-14B**           | **66.3** | **72.1** | **61.3** | **24.8** | **32.3**  | **40.8** | **53.4** | **71.0** |


å¯¹äºä»¥ä¸Šæ‰€æœ‰å¯¹æ¯”æ¨¡å‹ï¼Œæˆ‘ä»¬åˆ—å‡ºäº†å…¶å®˜æ–¹æ±‡æŠ¥ç»“æœä¸[OpenCompass](https://opencompass.org.cn/leaderboard-llm)ç»“æœä¹‹é—´çš„æœ€ä½³åˆ†æ•°ã€‚

æ›´å¤šçš„å®éªŒç»“æœå’Œç»†èŠ‚è¯·æŸ¥çœ‹æˆ‘ä»¬çš„æŠ€æœ¯å¤‡å¿˜å½•ã€‚ç‚¹å‡»[è¿™é‡Œ](https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf)ã€‚
<br><br>

## è¦æ±‚

* python 3.8åŠä»¥ä¸Šç‰ˆæœ¬
* pytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œæ¨è2.0åŠä»¥ä¸Šç‰ˆæœ¬
* å»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·ã€flash-attentionç”¨æˆ·ç­‰éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰
<br>

## å¿«é€Ÿä½¿ç”¨

æˆ‘ä»¬æä¾›ç®€å•çš„ç¤ºä¾‹æ¥è¯´æ˜å¦‚ä½•åˆ©ç”¨ğŸ¤– ModelScopeå’ŒğŸ¤— Transformerså¿«é€Ÿä½¿ç”¨Qwen-7Bå’ŒQwen-7B-Chatã€‚

åœ¨å¼€å§‹å‰ï¼Œè¯·ç¡®ä¿ä½ å·²ç»é…ç½®å¥½ç¯å¢ƒå¹¶å®‰è£…å¥½ç›¸å…³çš„ä»£ç åŒ…ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œç¡®ä¿ä½ æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œç„¶åå®‰è£…ç›¸å…³çš„ä¾èµ–åº“ã€‚

```bash
pip install -r requirements.txt
```

å¦‚æœä½ çš„æ˜¾å¡æ”¯æŒfp16æˆ–bf16ç²¾åº¦ï¼Œæˆ‘ä»¬è¿˜æ¨èå®‰è£…[flash-attention](https://github.com/Dao-AILab/flash-attention)ï¼ˆ**å½“å‰å·²æ”¯æŒflash attention 2**ï¼‰æ¥æé«˜ä½ çš„è¿è¡Œæ•ˆç‡ä»¥åŠé™ä½æ˜¾å­˜å ç”¨ã€‚(**flash-attentionåªæ˜¯å¯é€‰é¡¹ï¼Œä¸å®‰è£…ä¹Ÿå¯æ­£å¸¸è¿è¡Œè¯¥é¡¹ç›®**)

```bash
git clone https://github.com/Dao-AILab/flash-attention
cd flash-attention && pip install .
# ä¸‹æ–¹å®‰è£…å¯é€‰ï¼Œå®‰è£…å¯èƒ½æ¯”è¾ƒç¼“æ…¢ã€‚
# pip install csrc/layer_norm
# pip install csrc/rotary
```

æ¥ä¸‹æ¥ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨Transformersæˆ–è€…ModelScopeæ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚

### ğŸ¤— Transformers

å¦‚å¸Œæœ›ä½¿ç”¨Qwen-chatè¿›è¡Œæ¨ç†ï¼Œæ‰€éœ€è¦å†™çš„åªæ˜¯å¦‚ä¸‹æ‰€ç¤ºçš„æ•°è¡Œä»£ç ã€‚**è¯·ç¡®ä¿ä½ ä½¿ç”¨çš„æ˜¯æœ€æ–°ä»£ç ï¼Œå¹¶æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹åç§°å’Œè·¯å¾„ï¼Œå¦‚`Qwen/Qwen-7B-Chat`å’Œ`Qwen/Qwen-14B-Chat`**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# å¯é€‰çš„æ¨¡å‹åŒ…æ‹¬: "Qwen/Qwen-7B-Chat", "Qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤ä½¿ç”¨è‡ªåŠ¨æ¨¡å¼ï¼Œæ ¹æ®è®¾å¤‡è‡ªåŠ¨é€‰æ‹©ç²¾åº¦
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True).eval()

# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# ç¬¬ä¸€è½®å¯¹è¯
response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚

# ç¬¬äºŒè½®å¯¹è¯
response, history = model.chat(tokenizer, "ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚", history=history)
print(response)
# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚
# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚
# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚
# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚
# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚
# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚

# ç¬¬ä¸‰è½®å¯¹è¯
response, history = model.chat(tokenizer, "ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜", history=history)
print(response)
# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹
```

è¿è¡ŒQwenåŒæ ·éå¸¸ç®€å•ã€‚

<details>
  <summary>è¿è¡ŒQwen</summary>

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# å¯é€‰çš„æ¨¡å‹åŒ…æ‹¬: "Qwen/Qwen-7B", "Qwen/Qwen-14B"
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)

# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤ä½¿ç”¨è‡ªåŠ¨æ¨¡å¼ï¼Œæ ¹æ®è®¾å¤‡è‡ªåŠ¨é€‰æ‹©ç²¾åº¦
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True).eval()

# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)

inputs = tokenizer('è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯', return_tensors='pt')
inputs = inputs.to(model.device)
pred = model.generate(**inputs)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))
# è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯äºšçš„æ–¯äºšè´å·´ï¼ˆAddis Ababaï¼‰...
```

</details>

è‹¥åœ¨ä½¿ç”¨ä¸Šè¿°ä»£ç æ—¶ç”±äºå„ç§åŸå› æ— æ³•ä» HuggingFace æ‹‰å–æ¨¡å‹å’Œä»£ç ï¼Œå¯ä»¥å…ˆä» ModelScope ä¸‹è½½æ¨¡å‹åŠä»£ç è‡³æœ¬åœ°ï¼Œå†ä»æœ¬åœ°åŠ è½½æ¨¡å‹ï¼š

```python
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer

# Downloading model checkpoint to a local dir model_dir
# model_dir = snapshot_download('qwen/Qwen-7B')
# model_dir = snapshot_download('qwen/Qwen-7B-Chat')
# model_dir = snapshot_download('qwen/Qwen-14B')
model_dir = snapshot_download('qwen/Qwen-14B-Chat')

# Loading local checkpoints
# trust_remote_code is still set as True since we still load codes from local dir instead of transformers
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    device_map="auto",
    trust_remote_code=True
).eval()
```

### ğŸ¤– ModelScope

é­”æ­ï¼ˆModelScopeï¼‰æ˜¯å¼€æºçš„æ¨¡å‹å³æœåŠ¡å…±äº«å¹³å°ï¼Œä¸ºæ³›AIå¼€å‘è€…æä¾›çµæ´»ã€æ˜“ç”¨ã€ä½æˆæœ¬çš„ä¸€ç«™å¼æ¨¡å‹æœåŠ¡äº§å“ã€‚ä½¿ç”¨ModelScopeåŒæ ·éå¸¸ç®€å•ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from modelscope import AutoModelForCausalLM, AutoTokenizer
from modelscope import GenerationConfig

# å¯é€‰çš„æ¨¡å‹åŒ…æ‹¬: "qwen/Qwen-7B-Chat", "qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("qwen/Qwen-7B-Chat", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚

response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
response, history = model.chat(tokenizer, "æµ™æ±Ÿçš„çœä¼šåœ¨å“ªé‡Œï¼Ÿ", history=history) 
print(response)
response, history = model.chat(tokenizer, "å®ƒæœ‰ä»€ä¹ˆå¥½ç©çš„æ™¯ç‚¹", history=history)
print(response)
```

### Batchæ¨ç†
åƒé—®æ”¯æŒbatchæ‰¹é‡æ¨ç†ã€‚åœ¨å¼€å¯flash-attentionçš„çŠ¶æ€ä¸‹ï¼Œä½¿ç”¨batchæ¨ç†å¯ä»¥çº¦40%çš„æé€Ÿã€‚ç¤ºä¾‹ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import GenerationConfig
from qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids

tokenizer = AutoTokenizer.from_pretrained(
    './',
    pad_token='<|extra_0|>',
    eos_token='<|endoftext|>',
    padding_side='left',
    trust_remote_code=True
)
model = AutoModelForCausalLM.from_pretrained(
    './',
    pad_token_id=tokenizer.pad_token_id,
    device_map="auto",
    trust_remote_code=True
).eval()
model.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)

all_raw_text = ["æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚", "ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹", "æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°"]
batch_raw_text = []
for q in all_raw_text:
    raw_text, _ = make_context(
        tokenizer,
        q,
        system="You are a helpful assistant.",
        max_window_size=model.generation_config.max_window_size,
        chat_format=model.generation_config.chat_format,
    )
    batch_raw_text.append(raw_text)

batch_input_ids = tokenizer(batch_raw_text, padding='longest')
batch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)
batch_out_ids = model.generate(
    batch_input_ids,
    return_dict_in_generate=False,
    generation_config=model.generation_config
)
padding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]

batch_response = [
    decode_tokens(
        batch_out_ids[i][padding_lens[i]:],
        tokenizer,
        raw_text_len=len(batch_raw_text[i]),
        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),
        chat_format="chatml",
        verbose=False,
        errors='replace'
    ) for i in range(len(all_raw_text))
]
print(batch_response)

response, _ = model.chat(tokenizer, "æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚", history=None)
print(response)

response, _ = model.chat(tokenizer, "ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹", history=None)
print(response)

response, _ = model.chat(tokenizer, "æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°", history=None)
print(response)
```

### CPU

æˆ‘ä»¬æ¨èä½ ä½¿ç”¨ [qwen.cpp](https://github.com/QwenLM/qwen.cpp) æ¥å®ç°CPUéƒ¨ç½²å’Œæ¨ç†ã€‚qwen.cppæ˜¯Qwenå’Œtiktokençš„C++å®ç°ã€‚ä½ å¯ä»¥ç‚¹å‡»é“¾æ¥è¿›å…¥repoäº†è§£è¯¦æƒ…ã€‚

å½“ç„¶ï¼Œç›´æ¥åœ¨CPUä¸Šè¿è¡Œæ¨¡å‹ä¹Ÿæ˜¯å¯ä»¥çš„ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
```

ä½†æ˜¯ï¼Œè¿™æ ·çš„æ¨ç†æ•ˆç‡å¤§æ¦‚ç‡ä¼šéå¸¸ä½ã€‚

### å¤šGPU

å¦‚æœä½ é‡åˆ°æ˜¾å­˜ä¸è¶³çš„é—®é¢˜è€Œå¸Œæœ›ä½¿ç”¨å¤šå¼ GPUè¿›è¡Œæ¨ç†ï¼Œå¯ä»¥ä½¿ç”¨ä¸Šè¿°çš„é»˜è®¤çš„ä½¿ç”¨æ–¹æ³•è¯»å–æ¨¡å‹ã€‚æ­¤å‰æä¾›çš„è„šæœ¬`utils.py`å·²åœæ­¢ç»´æŠ¤ã€‚

å°½ç®¡è¿™ä¸ªæ–¹æ³•å¾ˆç®€å•ï¼Œä½†å®ƒçš„æ•ˆç‡ç›¸å¯¹è¾ƒä½ã€‚æˆ‘ä»¬å»ºè®®ä½¿ç”¨vLLMå’ŒFastChatå¹¶è¯·é˜…è¯»éƒ¨ç½²ç« èŠ‚ã€‚
<br><br>


## é‡åŒ–

### GPTQ

**è¯·æ³¨æ„ï¼šæˆ‘ä»¬æ›´æ–°é‡åŒ–æ–¹æ¡ˆä¸ºåŸºäº [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) çš„é‡åŒ–ï¼Œæä¾›Int4é‡åŒ–æ¨¡å‹ã€‚è¯¥æ–¹æ¡ˆåœ¨æ¨¡å‹è¯„æµ‹æ•ˆæœå‡ ä¹æ— æŸï¼Œä¸”å­˜å‚¨éœ€æ±‚æ›´ä½ï¼Œæ¨ç†é€Ÿåº¦æ›´ä¼˜ã€‚**

ä»¥ä¸‹æˆ‘ä»¬æä¾›ç¤ºä¾‹è¯´æ˜å¦‚ä½•ä½¿ç”¨Int4é‡åŒ–æ¨¡å‹ã€‚åœ¨å¼€å§‹ä½¿ç”¨å‰ï¼Œè¯·å…ˆä¿è¯æ»¡è¶³è¦æ±‚ï¼ˆå¦‚torch 2.0åŠä»¥ä¸Šï¼Œtransformersç‰ˆæœ¬ä¸º4.32.0åŠä»¥ä¸Šï¼Œç­‰ç­‰ï¼‰ï¼Œå¹¶å®‰è£…æ‰€éœ€å®‰è£…åŒ…ï¼š

```bash
pip install auto-gptq optimum
```

å¦‚å®‰è£…`auto-gptq`é‡åˆ°é—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨åˆ°å®˜æ–¹[repo](https://github.com/PanQiWei/AutoGPTQ)æœç´¢åˆé€‚çš„wheelã€‚

éšåå³å¯ä½¿ç”¨å’Œä¸Šè¿°ä¸€è‡´çš„ç”¨æ³•è°ƒç”¨é‡åŒ–æ¨¡å‹ï¼š

```python
# å¯é€‰æ¨¡å‹åŒ…æ‹¬ï¼š"Qwen/Qwen-7B-Chat-Int4", "Qwen/Qwen-14B-Chat-Int4"
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat-Int4",
    device_map="auto",
    trust_remote_code=True
).eval()
response, history = model.chat(tokenizer, "Hi", history=None)
```

æˆ‘ä»¬å¯¹BF16ï¼ŒInt8å’ŒInt4æ¨¡å‹åœ¨åŸºå‡†è¯„æµ‹ä¸Šåšäº†æµ‹è¯•ï¼Œå‘ç°é‡åŒ–æ¨¡å‹æ•ˆæœæŸå¤±è¾ƒå°ï¼Œç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |
|----------------------|:----:|:-----------:|:-----:|:---------:|
| Qwen-7B-Chat (BF16)  | 55.8 |    59.7     | 50.3  |   37.2    |
| Qwen-7B-Chat (Int8)  | 55.4 |    59.4     | 48.3  |   34.8    |
| Qwen-7B-Chat (Int4)  | 55.1 |    59.2     | 49.7  |   29.9    |
| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 60.1  |   43.9    |
| Qwen-14B-Chat (Int8) | 63.6 |    68.6     | 60.0	 |   48.2    |
| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |
<br>


### KV cacheé‡åŒ–

> æ³¨æ„ï¼šç”±äºHugging Faceçš„å†…éƒ¨å®ç°ï¼Œæœ¬åŠŸèƒ½çš„æ”¯æŒæ–‡ä»¶`cache_autogptq_cuda_356.cpp`ä¸`cache_autogptq_cuda_kernel_245.cu`å¯èƒ½æ²¡è¢«ä¸‹è½½ã€‚å¦‚éœ€å¼€å¯ä½¿ç”¨ï¼Œè¯·æ‰‹åŠ¨ä»ç›¸å…³ä½ç½®ä¸‹è½½ï¼Œå¹¶æ”¾ç½®åˆ°ç›¸åº”æ–‡ä»¶ä¸­ã€‚

åœ¨æ¨¡å‹inferæ—¶ï¼Œå¯ä»¥å°†ä¸­é—´ç»“æœkeyä»¥åŠvalueçš„å€¼é‡åŒ–åå‹ç¼©å­˜å‚¨ï¼Œè¿™æ ·ä¾¿å¯ä»¥åœ¨ç›¸åŒçš„å¡ä¸Šå­˜å‚¨æ›´å¤šçš„keyä»¥åŠvalueï¼Œå¢åŠ æ ·æœ¬ååã€‚

æä¾›use_cache_quantizationä»¥åŠuse_cache_kernelä¸¤ä¸ªå‚æ•°å¯¹æ¨¡å‹æ§åˆ¶ï¼Œå½“use_cache_quantizationä»¥åŠuse_cache_kernelå‡å¼€å¯æ—¶ï¼Œå°†å¯åŠ¨kv-cacheé‡åŒ–çš„åŠŸèƒ½ã€‚å…·ä½“ä½¿ç”¨å¦‚ä¸‹ï¼š
```python
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat",
     device_map="auto",
     trust_remote_code=True,
     use_cache_quantization=True,
     use_cache_kernel=True,
     use_flash_attn=False
)
```
æ³¨æ„ï¼šå½“å‰è¯¥åŠŸèƒ½ç›®å‰ä¸æ”¯æŒä¸flash attnåŒæ—¶å¼€å¯ï¼Œå¦‚æœä½ å¼€äº†kv cacheé‡åŒ–çš„åŒæ—¶åˆå¼€äº†flash attnï¼ˆuse_flash_attn=Trueï¼Œ use_cache_quantization=True, use_cache_kernel=Trueï¼‰ï¼Œä¼šé»˜è®¤å°†use_flash_attnå…³é—­ã€‚

æ•ˆæœæ–¹é¢ï¼Œæˆ‘ä»¬éªŒè¯è¿‡Int8 kv-cacheçš„ä½¿ç”¨å¯¹æ¨¡å‹æ•´ä½“çš„ç²¾åº¦æŒ‡æ ‡åŸºæœ¬æ— æŸã€‚æˆ‘ä»¬åšäº†é’ˆå¯¹æ˜¾å­˜å ç”¨çš„æ€§èƒ½æµ‹è¯•ã€‚è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼Œæ¨¡å‹é»˜è®¤ä½¿ç”¨BF16æ ¼å¼ï¼Œé»˜è®¤ç”Ÿæˆçš„seq-length=1024ï¼ˆç”Ÿæˆ1024ä¸ªtokenï¼‰ï¼Œå…¶ä¸­oomè¡¨ç¤ºout of memoryã€‚

å¼€å¯äº†kv-cacheé‡åŒ–ä¹‹åï¼Œæ¨¡å‹åœ¨inferçš„æ—¶å€™å¯ä»¥å¼€å¯æ›´å¤§çš„batch size(bs)

| USE KVCache |  bs=1  |  bs=4  | bs=16  | bs=32  | bs=64  | bs=100 |
|-------------|:------:|:------:|:------:|:------:|:------:|:------:|
| no          | 16.3GB | 24.1GB | 31.7GB | 48.7GB |  oom   |  oom   |
| yes         | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB |


å¼€å¯äº†kv-cacheé‡åŒ–ä¹‹åï¼Œæ¨¡å‹åœ¨inferæ—¶é¢„æµ‹æ›´é•¿çš„seq-lengthï¼ˆslï¼Œç”Ÿæˆçš„tokenæ•°ï¼‰ç»“æœæ—¶ï¼Œå¯ä»¥èŠ‚çº¦æ›´å¤šçš„æ˜¾å­˜ã€‚

| USE KVCache | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |
|-------------|:------:|:-------:|:-------:|:-------:|:-------:|
| no          | 15.2GB | 16.3GB  | 17.6GB  | 19.5GB  | 23.2GB  |
| yes         |  15GB  | 15.5GB  | 15.8GB  | 16.6GB  | 17.6GB  |


æ¨¡å‹å¼€å¯kv cacheé‡åŒ–åå†æ¨¡å‹inferçš„æ—¶å€™ï¼Œä¼šå°†åŸå§‹å­˜è¿›layer_pastçš„floatæ ¼å¼çš„key/valueå˜æˆint8æ ¼å¼çš„qkey/qvalueå’Œç›¸å¯¹åº”çš„é‡åŒ–å‚æ•°ã€‚
å…·ä½“æ“ä½œå¦‚ä¸‹ï¼š
1ã€å°†key/valueè¿›è¡Œé‡åŒ–æ“ä½œ
```
    qv,scale,zero_point=quantize_cache_v(v)
```
2ã€å­˜å…¥layer_pastä¸­:
é‡åŒ–æ ¼å¼çš„layer_past:
```
    layer_past=((q_key,key_scale,key_zero_point),
                (q_value,value_scale,value_zero_point))
```
åŸå§‹æ ¼å¼çš„layer_past:
```
    layer_past=(key,value)
```
å¦‚æœéœ€è¦å°†layer_pastä¸­å­˜å¥½çš„keyï¼Œvalueç›´æ¥å–å‡ºä½¿ç”¨ï¼Œå¯ä»¥ä½¿ç”¨åé‡åŒ–æ“ä½œå°†int8æ ¼å¼çš„key/valueè½¬å›floatæ ¼å¼ï¼š
```
    v=dequantize_cache_torch(qv,scale,zero_point)
```
<br>

### æ¨ç†æ€§èƒ½
è¿™ä¸€éƒ¨åˆ†å°†ä»‹ç»æ¨¡å‹æ¨ç†çš„é€Ÿåº¦å’Œæ˜¾å­˜å ç”¨çš„ç›¸å…³æ•°æ®ã€‚ä¸‹æ–‡çš„æ€§èƒ½æµ‹ç®—ä½¿ç”¨ [æ­¤è„šæœ¬](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py) å®Œæˆã€‚

### æ¨ç†é€Ÿåº¦

æˆ‘ä»¬æµ‹ç®—äº†BF16ã€Int8å’ŒInt4æ¨¡å‹åœ¨ä½¿ç”¨flash attention v2ã€v1æˆ–ä¸ä½¿ç”¨æ—¶ç”Ÿæˆ2048å’Œ8192ä¸ªtokençš„å¹³å‡æ¨ç†é€Ÿåº¦ï¼ˆtokens/sï¼‰ã€‚ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

<table>
    <tr>
      <th rowspan="2">Model Size</th><th rowspan="2">Precision</th><th rowspan="2">FlashAttn</th><th colspan="2" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">2048</th><th align="center">8192</th>
    </tr>
    </tr>
    </tr>
    <tr>
        <th rowspan="9">7B</th><td align="center" rowspan="3">BF16</td><td align="center">v2</td><td align="center">40.93</td><td align="center">36.14</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">40.75</td><td align="center">35.34
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">37.55</td><td align="center">33.56
    </tr>
    <tr>
        <td align="center" rowspan="3">Int8</td><td align="center">v2</td><td align="center">37.47</td><td align="center">32.54</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">37.51</td><td align="center">32.39
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">37.84</td><td align="center">32.65
    </tr>
    <tr>
        <td align="center" rowspan="3">Int4</td><td align="center">v2</td><td align="center">50.09</td><td align="center">38.61</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">45.98</td><td align="center">36.47
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">48.12</td><td align="center">36.70
    </tr>
    <tr>
        <th rowspan="9">14B</th><td align="center" rowspan="3">BF16</td><td align="center">v2</td><td align="center">32.88</td><td align="center">24.87</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">32.76</td><td align="center">28.89
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">29.32</td><td align="center">22.91
    </tr>
    <tr>
        <td align="center" rowspan="3">Int8</td><td align="center">v2</td><td align="center">29.28</td><td align="center">24.22</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">28.31</td><td align="center">23.87
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">31.12</td><td align="center">24.60
    </tr>
    <tr>
        <td align="center" rowspan="3">Int4</td><td align="center">v2</td><td align="center">38.72</td><td align="center">27.33</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">37.81</td><td align="center">26.46
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">37.65</td><td align="center">26.00
    </tr>
</table>

è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼Œä½¿ç”¨PyTorch 2.0.1å’ŒCUDA 11.4ã€‚æ¨ç†é€Ÿåº¦æ˜¯ç¼–ç 2048ä¸ªtokenå’Œç”Ÿæˆ8192ä¸ªtokençš„é€Ÿåº¦å‡å€¼ã€‚

æ³¨æ„ï¼šä»¥ä¸ŠInt4/Int8æ¨¡å‹ç”Ÿæˆé€Ÿåº¦ä½¿ç”¨autogptqåº“ç»™å‡ºï¼Œå½“å‰``AutoModelForCausalLM.from_pretrained``è½½å…¥çš„æ¨¡å‹ç”Ÿæˆé€Ÿåº¦ä¼šæ…¢å¤§çº¦20%ã€‚æˆ‘ä»¬å·²ç»å°†è¯¥é—®é¢˜æ±‡æŠ¥ç»™HuggingFaceå›¢é˜Ÿï¼Œè‹¥æœ‰è§£å†³æ–¹æ¡ˆå°†å³æ—¶æ›´æ–°ã€‚

### æ˜¾å­˜ä½¿ç”¨

æˆ‘ä»¬è¿˜æµ‹ç®—äº†BF16ã€Int8å’ŒInt4æ¨¡å‹ç¼–ç 2048ä¸ªtokenåŠç”Ÿæˆ8192ä¸ªtokençš„å³°å€¼æ˜¾å­˜å ç”¨æƒ…å†µã€‚ç»“æœï¼ˆGBï¼‰å¦‚ä¸‹æ‰€ç¤ºï¼š

<table>
    <tr>
      <th rowspan="2">Model Size</th><th rowspan="2">Precision</th><th colspan="2" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">2048</th><th align="center">8192</th>
    </tr>
    </tr>
    </tr>
    <tr>
        <th rowspan="3">7B</th><td align="center">BF16</td><td align="center">16.99</td><td align="center">22.53</td>
    </tr>
    <tr>
        <td align="center">Int8</td><td align="center">11.20</td><td align="center">16.62
    </tr>
    <tr>
        <td align="center">Int4</td><td align="center">8.21</td><td align="center">13.63</td>
    </tr>
    <tr>
        <th rowspan="3">14B</th><td align="center">BF16</td><td align="center">30.15</td><td align="center">38.94</td>
    </tr>
    <tr>
        <td align="center">Int8</td><td align="center">18.81</td><td align="center">27.54
    </tr>
    <tr>
        <td align="center">Int4</td><td align="center">13.01</td><td align="center">21.79</td>
    </tr>
</table>

<br>

## å¾®è°ƒ

### ä½¿ç”¨æ–¹æ³•
æˆ‘ä»¬æä¾›äº†`finetune.py`è¿™ä¸ªè„šæœ¬ä¾›ç”¨æˆ·å®ç°åœ¨è‡ªå·±çš„æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒçš„åŠŸèƒ½ï¼Œä»¥æ¥å…¥ä¸‹æ¸¸ä»»åŠ¡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†shellè„šæœ¬å‡å°‘ç”¨æˆ·çš„å·¥ä½œé‡ã€‚è¿™ä¸ªè„šæœ¬æ”¯æŒ [DeepSpeed](https://github.com/microsoft/DeepSpeed) å’Œ [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/) ã€‚æˆ‘ä»¬æä¾›çš„shellè„šæœ¬ä½¿ç”¨äº†DeepSpeedï¼Œå› æ­¤å»ºè®®æ‚¨ç¡®ä¿å·²ç»å®‰è£…DeepSpeedã€‚

é¦–å…ˆï¼Œä½ éœ€è¦å‡†å¤‡ä½ çš„è®­ç»ƒæ•°æ®ã€‚ä½ éœ€è¦å°†æ‰€æœ‰æ ·æœ¬æ”¾åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­å¹¶å­˜å…¥jsonæ–‡ä»¶ä¸­ã€‚æ¯ä¸ªæ ·æœ¬å¯¹åº”ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«idå’Œconversationï¼Œå…¶ä¸­åè€…ä¸ºä¸€ä¸ªåˆ—è¡¨ã€‚ç¤ºä¾‹å¦‚ä¸‹æ‰€ç¤ºï¼š
```json
[
  {
    "id": "identity_0",
    "conversations": [
      {
        "from": "user",
        "value": "ä½ å¥½"
      },
      {
        "from": "assistant",
        "value": "æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚"
      }
    ]
  }
]
```

å‡†å¤‡å¥½æ•°æ®åï¼Œä½ å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æä¾›çš„shellè„šæœ¬å®ç°å¾®è°ƒã€‚æ³¨æ„ï¼Œä½ éœ€è¦åœ¨è„šæœ¬ä¸­æŒ‡å®šä½ çš„æ•°æ®çš„è·¯å¾„ã€‚

å¾®è°ƒè„šæœ¬èƒ½å¤Ÿå¸®ä½ å®ç°ï¼š
- å…¨å‚æ•°å¾®è°ƒ
- LoRA
- Q-LoRA

å…¨å‚æ•°å¾®è°ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚ä½ å¯ä»¥è¿è¡Œè¿™ä¸ªè„šæœ¬å¼€å§‹è®­ç»ƒï¼š

```bash
# åˆ†å¸ƒå¼è®­ç»ƒã€‚ç”±äºæ˜¾å­˜é™åˆ¶å°†å¯¼è‡´å•å¡è®­ç»ƒå¤±è´¥ï¼Œæˆ‘ä»¬ä¸æä¾›å•å¡è®­ç»ƒè„šæœ¬ã€‚
sh finetune/finetune_ds.sh
```

å°¤å…¶æ³¨æ„ï¼Œä½ éœ€è¦åœ¨è„šæœ¬ä¸­æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹åç§°æˆ–è·¯å¾„ã€æ•°æ®è·¯å¾„ã€ä»¥åŠæ¨¡å‹è¾“å‡ºçš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚åœ¨è¿™ä¸ªè„šæœ¬ä¸­æˆ‘ä»¬ä½¿ç”¨äº†DeepSpeed ZeRO 3ã€‚å¦‚æœä½ æƒ³ä¿®æ”¹è¿™ä¸ªé…ç½®ï¼Œå¯ä»¥åˆ é™¤æ‰`--deepspeed`è¿™ä¸ªè¾“å…¥æˆ–è€…è‡ªè¡Œæ ¹æ®éœ€æ±‚ä¿®æ”¹DeepSpeedé…ç½®jsonæ–‡ä»¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒï¼Œå› æ­¤ä½ å¯ä»¥è®¾ç½®`--bf16 True`æˆ–è€…`--fp16 True`ã€‚åœ¨ä½¿ç”¨fp16æ—¶ï¼Œè¯·ä½¿ç”¨DeepSpeedæ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒã€‚ç»éªŒä¸Šï¼Œå¦‚æœä½ çš„æœºå™¨æ”¯æŒbf16ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨bf16ï¼Œè¿™æ ·å¯ä»¥å’Œæˆ‘ä»¬çš„é¢„è®­ç»ƒå’Œå¯¹é½è®­ç»ƒä¿æŒä¸€è‡´ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬æŠŠé»˜è®¤é…ç½®è®¾ä¸ºå®ƒçš„åŸå› ã€‚

è¿è¡ŒLoRAçš„æ–¹æ³•ç±»ä¼¼å…¨å‚æ•°å¾®è°ƒã€‚ä½†åœ¨å¼€å§‹å‰ï¼Œè¯·ç¡®ä¿å·²ç»å®‰è£…`peft`ä»£ç åº“ã€‚å¦å¤–ï¼Œè®°ä½è¦è®¾ç½®æ­£ç¡®çš„æ¨¡å‹ã€æ•°æ®å’Œè¾“å‡ºè·¯å¾„ã€‚æˆ‘ä»¬å»ºè®®ä½ ä¸ºæ¨¡å‹è·¯å¾„ä½¿ç”¨ç»å¯¹è·¯å¾„ã€‚è¿™æ˜¯å› ä¸ºLoRAä»…å­˜å‚¨adapteréƒ¨åˆ†å‚æ•°ï¼Œè€Œadapteré…ç½®jsonæ–‡ä»¶è®°å½•äº†é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ï¼Œç”¨äºè¯»å–é¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€‚åŒæ ·ï¼Œä½ å¯ä»¥è®¾ç½®bf16æˆ–è€…fp16ã€‚

```bash
# å•å¡è®­ç»ƒ
sh finetune/finetune_lora_single_gpu.sh
# åˆ†å¸ƒå¼è®­ç»ƒ
sh finetune/finetune_lora_ds.sh
```

ä¸å…¨å‚æ•°å¾®è°ƒä¸åŒï¼ŒLoRA ([è®ºæ–‡](https://arxiv.org/abs/2106.09685)) åªæ›´æ–°adapterå±‚çš„å‚æ•°è€Œæ— éœ€æ›´æ–°åŸæœ‰è¯­è¨€æ¨¡å‹çš„å‚æ•°ã€‚è¿™ç§æ–¹æ³•å…è®¸ç”¨æˆ·ç”¨æ›´ä½çš„æ˜¾å­˜å¼€é”€æ¥è®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿæ„å‘³ç€æ›´å°çš„è®¡ç®—å¼€é”€ã€‚

æ³¨æ„ï¼Œå¦‚æœä½ ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒLoRAå¾®è°ƒï¼Œè€Œéchatæ¨¡å‹ï¼Œæ¨¡å‹çš„embeddingå’Œè¾“å‡ºå±‚çš„å‚æ•°å°†è¢«è®¾ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚è¿™æ˜¯å› ä¸ºé¢„è®­ç»ƒæ¨¡å‹æ²¡æœ‰å­¦ä¹ è¿‡ChatMLæ ¼å¼ä¸­çš„ç‰¹æ®Štokenï¼Œå› æ­¤éœ€è¦å°†è¿™éƒ¨åˆ†å‚æ•°è®¾ä¸ºå¯è®­ç»ƒæ‰èƒ½è®©æ¨¡å‹å­¦ä¼šç†è§£å’Œé¢„æµ‹è¿™äº›tokenã€‚è¿™ä¹Ÿæ„å‘³ç€ï¼Œå‡å¦‚ä½ çš„è®­ç»ƒå¼•å…¥æ–°çš„ç‰¹æ®Štokenï¼Œä½ éœ€è¦é€šè¿‡ä»£ç ä¸­çš„`modules_to_save`å°†è¿™äº›å‚æ•°è®¾ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚æ­¤å¤–ï¼Œè¿™éƒ¨åˆ†è®­ç»ƒå‚æ•°çš„å¼•å…¥ä¼šå½±å“ZeRO 3çš„ä½¿ç”¨ï¼Œå› æ­¤æˆ‘ä»¬é»˜è®¤æ¨èä½¿ç”¨ZeRO 2ã€‚å½“ç„¶ï¼Œå¦‚æœä½ ä¸éœ€è¦å¼•å…¥è¿™éƒ¨åˆ†è®­ç»ƒå‚æ•°ï¼Œä½ å¯ä»¥é€šè¿‡æ›¿æ¢DeepSpeedçš„é…ç½®æ–‡ä»¶æ¥ä½¿ç”¨ZeRO 3ã€‚å¦‚æœä½ æƒ³èŠ‚çœæ˜¾å­˜å ç”¨ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨chatæ¨¡å‹è¿›è¡ŒLoRAå¾®è°ƒï¼Œæ˜¾å­˜å ç”¨å°†å¤§å¹…åº¦é™ä½ã€‚ä¸‹æ–‡çš„æ˜¾å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦çš„è®°å½•å°†è¯¦ç»†ä»‹ç»è¿™éƒ¨åˆ†ç»†èŠ‚ã€‚

å¦‚æœä½ ä¾ç„¶é‡åˆ°æ˜¾å­˜ä¸è¶³çš„é—®é¢˜ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨Q-LoRA ([è®ºæ–‡](https://arxiv.org/abs/2305.14314)) ã€‚è¯¥æ–¹æ³•ä½¿ç”¨4æ¯”ç‰¹é‡åŒ–æ¨¡å‹ä»¥åŠpaged attentionç­‰æŠ€æœ¯å®ç°æ›´å°çš„æ˜¾å­˜å¼€é”€ã€‚

æ³¨æ„ï¼šå¦‚ä½ ä½¿ç”¨å•å¡Q-LoRAï¼Œä½ å¯èƒ½éœ€è¦å®‰è£…`mpi4py`ã€‚ä½ å¯ä»¥é€šè¿‡`pip`æˆ–è€…`conda`æ¥å®‰è£…ã€‚

è¿è¡ŒQ-LoRAä½ åªéœ€è¿è¡Œå¦‚ä¸‹è„šæœ¬ï¼š

```bash
# å•å¡è®­ç»ƒ
sh finetune/finetune_qlora_single_gpu.sh
# åˆ†å¸ƒå¼è®­ç»ƒ
sh finetune/finetune_qlora_ds.sh
```

æˆ‘ä»¬å»ºè®®ä½ ä½¿ç”¨æˆ‘ä»¬æä¾›çš„Int4é‡åŒ–æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå³Qwen-7B-Chat-Int4ã€‚è¯·**ä¸è¦ä½¿ç”¨**éé‡åŒ–æ¨¡å‹ï¼ä¸å…¨å‚æ•°å¾®è°ƒä»¥åŠLoRAä¸åŒï¼ŒQ-LoRAä»…æ”¯æŒfp16ã€‚æ³¨æ„ï¼Œç”±äºæˆ‘ä»¬å‘ç°torch ampæ”¯æŒçš„fp16æ··åˆç²¾åº¦è®­ç»ƒå­˜åœ¨é—®é¢˜ï¼Œå› æ­¤å½“å‰çš„å•å¡è®­ç»ƒQ-LoRAå¿…é¡»ä½¿ç”¨DeepSpeedã€‚æ­¤å¤–ï¼Œä¸Šè¿°LoRAå…³äºç‰¹æ®Štokençš„é—®é¢˜åœ¨Q-LoRAä¾ç„¶å­˜åœ¨ã€‚å¹¶ä¸”ï¼ŒInt4æ¨¡å‹çš„å‚æ•°æ— æ³•è¢«è®¾ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚æ‰€å¹¸çš„æ˜¯ï¼Œæˆ‘ä»¬åªæä¾›äº†Chatæ¨¡å‹çš„Int4æ¨¡å‹ï¼Œå› æ­¤ä½ ä¸ç”¨æ‹…å¿ƒè¿™ä¸ªé—®é¢˜ã€‚ä½†æ˜¯ï¼Œå¦‚æœä½ æ‰§æ„è¦åœ¨Q-LoRAä¸­å¼•å…¥æ–°çš„ç‰¹æ®Štokenï¼Œå¾ˆæŠ±æ­‰ï¼Œæˆ‘ä»¬æ— æ³•ä¿è¯ä½ èƒ½æˆåŠŸè®­ç»ƒã€‚

> æ³¨æ„ï¼šç”±äºHugging Faceçš„å†…éƒ¨å®ç°ï¼Œæ¨¡å‹åœ¨ä¿å­˜æ—¶ï¼Œä¸€äº›éPythonæ–‡ä»¶æœªä¿å­˜ï¼ˆä¾‹å¦‚`*.cpp`ä¸`*.cu`ï¼‰ï¼Œå¦‚éœ€è¦æ”¯æŒç›¸å…³åŠŸèƒ½ï¼Œè¯·æ‰‹åŠ¨å¤åˆ¶æœ‰å…³æ–‡ä»¶ã€‚

ä¸å…¨å‚æ•°å¾®è°ƒä¸åŒï¼ŒLoRAå’ŒQ-LoRAçš„è®­ç»ƒåªéœ€å­˜å‚¨adapteréƒ¨åˆ†çš„å‚æ•°ã€‚å‡å¦‚ä½ éœ€è¦ä½¿ç”¨LoRAè®­ç»ƒåçš„æ¨¡å‹ï¼Œä½ éœ€è¦ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•ã€‚å‡è®¾ä½ ä½¿ç”¨Qwen-7Bè®­ç»ƒæ¨¡å‹ï¼Œä½ å¯ä»¥ç”¨å¦‚ä¸‹ä»£ç è¯»å–æ¨¡å‹ï¼š

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()
```

å¦‚æœä½ è§‰å¾—è¿™æ ·ä¸€æ­¥åˆ°ä½çš„æ–¹å¼è®©ä½ å¾ˆä¸å®‰å¿ƒæˆ–è€…å½±å“ä½ æ¥å…¥ä¸‹æ¸¸åº”ç”¨ï¼Œä½ å¯ä»¥é€‰æ‹©å…ˆåˆå¹¶å¹¶å­˜å‚¨æ¨¡å‹ï¼ˆLoRAæ”¯æŒåˆå¹¶ï¼ŒQ-LoRAä¸æ”¯æŒï¼‰ï¼Œå†ç”¨å¸¸è§„æ–¹å¼è¯»å–ä½ çš„æ–°æ¨¡å‹ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()

merged_model = model.merge_and_unload()
# max_shard_size and safe serialization are not necessary. 
# They respectively work for sharding checkpoint and save the model to safetensors
merged_model.save_pretrained(new_model_directory, max_shard_size="2048MB", safe_serialization=True)
```

`new_model_directory`ç›®å½•å°†åŒ…å«åˆå¹¶åçš„æ¨¡å‹å‚æ•°ä¸ç›¸å…³æ¨¡å‹ä»£ç ã€‚è¯·æ³¨æ„`*.cu`å’Œ`*.cpp`æ–‡ä»¶å¯èƒ½æ²¡è¢«ä¿å­˜ï¼Œè¯·æ‰‹åŠ¨å¤åˆ¶ã€‚å¦å¤–ï¼Œ`merge_and_unload`ä»…ä¿å­˜æ¨¡å‹ï¼Œå¹¶æœªä¿å­˜tokenizerï¼Œå¦‚æœ‰éœ€è¦ï¼Œè¯·å¤åˆ¶ç›¸å…³æ–‡ä»¶æˆ–ä½¿ç”¨ä»¥ä»¥ä¸‹ä»£ç ä¿å­˜
```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(
    path_to_adapter, # path to the output directory
    trust_remote_code=True
)
tokenizer.save_pretrained(new_model_directory)
```


æ³¨æ„ï¼šåˆ†å¸ƒå¼è®­ç»ƒéœ€è¦æ ¹æ®ä½ çš„éœ€æ±‚å’Œæœºå™¨æŒ‡å®šæ­£ç¡®çš„åˆ†å¸ƒå¼è®­ç»ƒè¶…å‚æ•°ã€‚æ­¤å¤–ï¼Œä½ éœ€è¦æ ¹æ®ä½ çš„æ•°æ®ã€æ˜¾å­˜æƒ…å†µå’Œè®­ç»ƒé€Ÿåº¦é¢„æœŸï¼Œä½¿ç”¨`--model_max_length`è®¾å®šä½ çš„æ•°æ®é•¿åº¦ã€‚

### æ˜¾å­˜å ç”¨åŠè®­ç»ƒé€Ÿåº¦
ä¸‹é¢è®°å½•7Bå’Œ14Bæ¨¡å‹åœ¨å•GPUä½¿ç”¨LoRAï¼ˆLoRA (emb)æŒ‡çš„æ˜¯embeddingå’Œè¾“å‡ºå±‚å‚ä¸è®­ç»ƒï¼Œè€ŒLoRAåˆ™ä¸ä¼˜åŒ–è¿™éƒ¨åˆ†å‚æ•°ï¼‰å’ŒQLoRAæ—¶å¤„ç†ä¸åŒé•¿åº¦è¾“å…¥çš„æ˜¾å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦çš„æƒ…å†µã€‚æœ¬æ¬¡è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼Œä½¿ç”¨CUDA 11.8å’ŒPytorch 2.0ï¼Œå¹¶ä½¿ç”¨äº†flash attention 2ã€‚æˆ‘ä»¬ç»Ÿä¸€ä½¿ç”¨batch sizeä¸º1ï¼Œgradient accumulationä¸º8çš„è®­ç»ƒé…ç½®ï¼Œè®°å½•è¾“å…¥é•¿åº¦åˆ†åˆ«ä¸º256ã€512ã€1024ã€2048ã€4096å’Œ8192çš„æ˜¾å­˜å ç”¨ï¼ˆGBï¼‰å’Œè®­ç»ƒé€Ÿåº¦ï¼ˆs/iterï¼‰ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨2å¼ A100æµ‹äº†Qwen-7Bçš„å…¨å‚æ•°å¾®è°ƒã€‚å—é™äºæ˜¾å­˜å¤§å°ï¼Œæˆ‘ä»¬ä»…æµ‹è¯•äº†256ã€512å’Œ1024tokençš„æ€§èƒ½ã€‚å…·ä½“æ•°å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š

<table>
    <tr>
      <th rowspan="2">Model Size</th><th rowspan="2">Method</th><th colspan="6" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">256</th><th align="center">512</th><th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th>
    </tr>
    </tr>
    </tr>
    <tr>
        <th rowspan="4">7B</th><td>LoRA</td><td align="center">20.1G / 1.2s/it</td><td align="center">20.4G / 1.5s/it</td><td align="center">21.5G / 2.8s/it</td><td align="center">23.8G / 5.2s/it</td><td align="center">29.7G / 10.1s/it</td><td align="center">36.6G / 21.3s/it</td>
    </tr>
    <tr>
        <td>LoRA (emb)</td><td align="center">33.7G / 1.4s/it</td><td align="center">34.1G / 1.6s/it</td><td align="center">35.2G / 2.9s/it</td><td align="center">35.1G / 5.3s/it</td><td align="center">39.2G / 10.3s/it</td><td align="center">48.5G / 21.7s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td align="center">11.5G / 3.0s/it</td><td align="center">11.5G / 3.0s/it</td><td align="center">12.3G / 3.5s/it</td><td align="center">13.9G / 7.0s/it</td><td align="center">16.9G / 11.6s/it</td><td align="center">23.5G / 22.3s/it</td>
    </tr>
    <tr>
        <td>Full-parameter</td><td align="center">139.2G / 4.0s/it</td><td align="center">148.0G / 4.0s/it</td><td align="center">162.0G / 4.5s/it</td><td align="center">-</td><td align="center">-</td><td align="center">-</td>
    </tr>
    <tr>
        <th rowspan="3">14B</th><td>LoRA</td><td align="center">34.6G / 1.6s/it</td><td align="center">35.1G / 2.4s/it</td><td align="center">35.3G / 4.4s/it</td><td align="center">37.4G / 8.4s/it</td><td align="center">42.5G / 17.0s/it</td><td align="center">55.2G / 36.0s/it</td>
    </tr>
    <tr>
        <td>LoRA (emb)</td><td align="center">51.2 / 1.7s/it</td><td align="center">51.1G / 2.6s/it</td><td align="center">51.5G / 4.6s/it</td><td align="center">54.1G / 8.6s/it</td><td align="center">56.8G / 17.2s/it</td><td align="center">67.7G / 36.3s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td align="center">18.7G / 5.3s/it</td><td align="center">18.4G / 6.3s/it</td><td align="center">18.9G / 8.2s/it</td><td align="center">19.9G / 11.8s/it</td><td align="center">23.0G / 20.1s/it</td><td align="center">27.9G / 38.3s/it</td>
    </tr>
</table>

<br>

## éƒ¨ç½²

### vLLM
å¦‚å¸Œæœ›éƒ¨ç½²åŠåŠ é€Ÿæ¨ç†ï¼Œæˆ‘ä»¬å»ºè®®ä½ ä½¿ç”¨vLLMå’ŒFastChatã€‚é¦–å…ˆå®‰è£…ç›¸åº”çš„ä»£ç åº“ï¼š
```bash
pip install vllm
pip install "fschat[model_worker,webui]"
```
ä½ ä¹Ÿå¯ä»¥é€šè¿‡`git clone`å’Œ`pip install -e .`çš„æ–¹å¼é€šè¿‡æºç å®‰è£…ã€‚å¦‚æœé‡åˆ°å®‰è£…é—®é¢˜ï¼Œè¯·é˜…è¯»å®ƒä»¬çš„å®˜æ–¹æ–‡æ¡£ã€‚

ä½¿ç”¨vLLMå’ŒFastChatè¿è¡ŒQwenä¹‹å‰ï¼Œé¦–å…ˆå¯åŠ¨ä¸€ä¸ªcontrollerï¼š
```bash
python -m fastchat.serve.controller
```

ç„¶åå¯åŠ¨model workerè¯»å–æ¨¡å‹ã€‚å¦‚ä½¿ç”¨å•å¡æ¨ç†ï¼Œè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š
```bash
python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code
```
ç„¶è€Œï¼Œå¦‚æœä½ å¸Œæœ›ä½¿ç”¨å¤šGPUåŠ é€Ÿæ¨ç†æˆ–è€…å¢å¤§æ˜¾å­˜ï¼Œä½ å¯ä»¥ä½¿ç”¨vLLMæ”¯æŒçš„æ¨¡å‹å¹¶è¡Œæœºåˆ¶ã€‚å‡è®¾ä½ éœ€è¦åœ¨4å¼ GPUä¸Šè¿è¡Œä½ çš„æ¨¡å‹ï¼Œå‘½ä»¤å¦‚ä¸‹æ‰€ç¤ºï¼š
```bash
python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4
```

å¯åŠ¨model workeråï¼Œä½ å¯ä»¥å¯åŠ¨ä¸€ä¸ªweb demoæˆ–è€…OpenAI APIã€‚å¯åŠ¨web demoçš„å‘½ä»¤å¦‚ä¸‹ï¼š
```bash
python -m fastchat.serve.gradio_web_server
```
ä½¿ç”¨OpenAI APIå‰ï¼Œè¯·é˜…è¯»æˆ‘ä»¬çš„APIç« èŠ‚é…ç½®å¥½ç¯å¢ƒï¼Œç„¶åè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š
```bash
python -m fastchat.serve.openai_api_server --host localhost --port 8000
```
<br>

## Demo

### Web UI

æˆ‘ä»¬æä¾›äº†Web UIçš„demoä¾›ç”¨æˆ·ä½¿ç”¨ (æ„Ÿè°¢ @wysaid æ”¯æŒ)ã€‚åœ¨å¼€å§‹å‰ï¼Œç¡®ä¿å·²ç»å®‰è£…å¦‚ä¸‹ä»£ç åº“ï¼š

```bash
pip install -r requirements_web_demo.txt
```

éšåè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå¹¶ç‚¹å‡»ç”Ÿæˆé“¾æ¥ï¼š

```bash
python web_demo.py
```

<p align="center">
    <br>
    <img src="assets/web_demo.gif" width="600" />
    <br>
<p>

### äº¤äº’å¼Demo

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€å•çš„äº¤äº’å¼Demoç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹`cli_demo.py`ã€‚å½“å‰æ¨¡å‹å·²ç»æ”¯æŒæµå¼è¾“å‡ºï¼Œç”¨æˆ·å¯é€šè¿‡è¾“å…¥æ–‡å­—çš„æ–¹å¼å’ŒQwen-7B-Chatäº¤äº’ï¼Œæ¨¡å‹å°†æµå¼è¾“å‡ºè¿”å›ç»“æœã€‚è¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š

```bash
python cli_demo.py
```

<p align="center">
    <br>
    <img src="assets/cli_demo.gif" width="600" />
    <br>
<p>
<br>

## API

æœ€ç®€å•çš„ä½¿ç”¨Qwenæ¨¡å‹APIæœåŠ¡çš„æ–¹æ³•å°±æ˜¯é€šè¿‡DashScopeï¼ˆé˜¿é‡Œäº‘çµç§¯æ¨¡å‹æœåŠ¡ï¼‰ã€‚æˆ‘ä»¬æä¾›äº†ç®€å•ä»‹ç»è¯´æ˜ä½¿ç”¨æ–¹æ³•ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†è‡ªå·±éƒ¨ç½²OpenAIæ ¼å¼çš„APIçš„æ–¹æ³•ã€‚

### DashScope
DashScopeæ˜¯é˜¿é‡Œäº‘æä¾›çš„å¤§è¯­è¨€æ¨¡å‹çš„APIæœåŠ¡ï¼Œç›®å‰æ”¯æŒQwenã€‚ä½†è¯·æ³¨æ„ï¼Œç›®å‰æä¾›æœåŠ¡çš„Qwenæ¨¡å‹ä¸ºå†…éƒ¨æ¨¡å‹ï¼Œæš‚æ— æ›´å¤šå…·ä½“ç»†èŠ‚å¯¹å¤–é€éœ²ã€‚æ¨¡å‹æœåŠ¡åŒ…æ‹¬`qwen-turbo`å’Œ`qwen-plus`ã€‚å‰è€…é€Ÿåº¦æ›´å¿«ï¼Œåè€…æ•ˆæœæ›´ä¼˜ã€‚è¯¦æƒ…è¯·æŸ¥çœ‹[æ–‡æ¡£](https://dashscope.aliyun.com)ã€‚

è¯·é¦–å…ˆå‰å¾€[å®˜ç½‘](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn)å¼€é€šDashScopeï¼Œè·å¾—API Keyï¼ˆAKï¼‰ã€‚å»ºè®®é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®AKï¼š
```bash
export DASHSCOPE_API_KEY="YOUR_DASHSCOPE_API_KEY"
```
éšåå®‰è£…ç›¸å…³ä»£ç åŒ…ï¼Œç‚¹å‡»[æ­¤å¤„](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk)æŸ¥çœ‹å®‰è£…æ–‡æ¡£ã€‚å¦‚ä½¿ç”¨pythonï¼Œåˆ™ç›´æ¥é€šè¿‡pipå®‰è£…ï¼š
```bash
pip install dashscope
```
å¦‚å®‰è£…JAVA SDKï¼Œåˆ™é€šè¿‡å¦‚ä¸‹å‘½ä»¤å®‰è£…ï¼š
```xml
<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->
<dependency>
    <groupId>com.alibaba</groupId>
    <artifactId>dashscope-sdk-java</artifactId>
    <version>the-latest-version</version>
</dependency>
```
æœ€ç®€å•çš„ä½¿ç”¨æ–¹æ³•å°±æ˜¯é€šè¿‡messagesè°ƒç”¨ï¼Œç”¨æ³•ç±»ä¼¼OpenAI APIã€‚ç¤ºä¾‹å¦‚ä¸‹ï¼š
```python
import random
from http import HTTPStatus
from dashscope import Generation


def call_with_messages():
    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},
                {'role': 'user', 'content': 'å¦‚ä½•åšè¥¿çº¢æŸ¿é¸¡è›‹ï¼Ÿ'}]
    gen = Generation()
    response = gen.call(
        Generation.Models.qwen_turbo,
        messages=messages,
        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set
        result_format='message',  # set the result to be "message" format.
    )
    return response


if __name__ == '__main__':
    response = call_with_messages()
    if response.status_code == HTTPStatus.OK:
        print(response)
    else:
        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (
            response.request_id, response.status_code,
            response.code, response.message
        ))
```
æ›´å¤šç”¨æ³•è¯·æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£äº†è§£è¯¦æƒ…ã€‚

### OpenAI API

æˆ‘ä»¬æä¾›äº†OpenAI APIæ ¼å¼çš„æœ¬åœ°APIéƒ¨ç½²æ–¹æ³•ï¼ˆæ„Ÿè°¢@hanpenggitï¼‰ã€‚åœ¨å¼€å§‹ä¹‹å‰å…ˆå®‰è£…å¿…è¦çš„ä»£ç åº“ï¼š

```bash
pip install fastapi uvicorn openai "pydantic>=2.3.0" sse_starlette
```

éšåå³å¯è¿è¡Œä»¥ä¸‹å‘½ä»¤éƒ¨ç½²ä½ çš„æœ¬åœ°APIï¼š

```bash
python openai_api.py
```

ä½ ä¹Ÿå¯ä»¥ä¿®æ”¹å‚æ•°ï¼Œæ¯”å¦‚`-c`æ¥ä¿®æ”¹æ¨¡å‹åç§°æˆ–è·¯å¾„, `--cpu-only`æ”¹ä¸ºCPUéƒ¨ç½²ç­‰ç­‰ã€‚å¦‚æœéƒ¨ç½²å‡ºç°é—®é¢˜ï¼Œæ›´æ–°ä¸Šè¿°ä»£ç åº“å¾€å¾€å¯ä»¥è§£å†³å¤§å¤šæ•°é—®é¢˜ã€‚

ä½¿ç”¨APIåŒæ ·éå¸¸ç®€å•ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
import openai
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "none"

# ä½¿ç”¨æµå¼å›å¤çš„è¯·æ±‚
for chunk in openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ],
    stream=True
    # æµå¼è¾“å‡ºçš„è‡ªå®šä¹‰stopwordsåŠŸèƒ½å°šæœªæ”¯æŒï¼Œæ­£åœ¨å¼€å‘ä¸­
):
    if hasattr(chunk.choices[0].delta, "content"):
        print(chunk.choices[0].delta.content, end="", flush=True)

# ä¸ä½¿ç”¨æµå¼å›å¤çš„è¯·æ±‚
response = openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ],
    stream=False,
    stop=[] # åœ¨æ­¤å¤„æ·»åŠ è‡ªå®šä¹‰çš„stop words ä¾‹å¦‚ReAct promptingæ—¶éœ€è¦å¢åŠ ï¼š stop=["Observation:"]ã€‚
)
print(response.choices[0].message.content)
```

<p align="center">
    <br>
    <img src="assets/openai_api.gif" width="600" />
    <br>
<p>

è¯¥æ¥å£ä¹Ÿæ”¯æŒå‡½æ•°è°ƒç”¨ï¼ˆ**Function Calling**ï¼‰ï¼Œä½†æš‚æ—¶ä»…é™ `stream=False` æ—¶èƒ½ç”Ÿæ•ˆã€‚ç”¨æ³•è§[å‡½æ•°è°ƒç”¨ç¤ºä¾‹](examples/function_call_examples.py)ã€‚
<br><br>


## å·¥å…·è°ƒç”¨

Qwen-Chaté’ˆå¯¹å·¥å…·ä½¿ç”¨ã€å‡½æ•°è°ƒç”¨èƒ½åŠ›è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç”¨æˆ·å¯ä»¥å¼€å‘åŸºäºQwençš„Agentã€LangChainåº”ç”¨ã€ç”šè‡³Code Interpreterã€‚

æˆ‘ä»¬æä¾›äº†æ–‡æ¡£è¯´æ˜å¦‚ä½•æ ¹æ®ReAct Promptingçš„åŸç†å®ç°å·¥å…·è°ƒç”¨ï¼Œè¯·å‚è§[ReActç¤ºä¾‹](examples/react_prompt.md)ã€‚åŸºäºè¯¥åŸç†ï¼Œæˆ‘ä»¬åœ¨ [openai_api.py](openai_api.py) é‡Œæä¾›äº†å‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰çš„æ”¯æŒã€‚
æˆ‘ä»¬åœ¨å·²å¼€æºçš„ä¸­æ–‡[è¯„æµ‹æ•°æ®é›†](eval/EVALUATION.md)ä¸Šæµ‹è¯•æ¨¡å‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œå¹¶å‘ç°Qwen-Chatèƒ½å¤Ÿå–å¾—ç¨³å®šçš„è¡¨ç°ï¼š

<table>
    <tr>
        <th colspan="4" align="center">ä¸­æ–‡å·¥å…·è°ƒç”¨è¯„æµ‹åŸºå‡†</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selection (Acc.â†‘)</th><th align="center">Tool Input (Rouge-Lâ†‘)</th><th align="center">False Positive Errorâ†“</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">95%</td><td align="center">0.90</td><td align="center">15.0%</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">85%</td><td align="center">0.88</td><td align="center">75.0%</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">98%</td><td align="center">0.91</td><td align="center">7.3%</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">98%</td><td align="center">0.93</td><td align="center">2.4%</td>
    </tr>
</table>

ä¸ºäº†è€ƒå¯ŸQwenä½¿ç”¨Python Code Interpreterå®Œæˆæ•°å­¦è§£é¢˜ã€æ•°æ®å¯è§†åŒ–ã€åŠæ–‡ä»¶å¤„ç†ä¸çˆ¬è™«ç­‰ä»»åŠ¡çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬ä¸“é—¨å»ºè®¾å¹¶å¼€æºäº†ä¸€ä¸ªè¯„æµ‹è¿™æ–¹é¢èƒ½åŠ›çš„[è¯„æµ‹åŸºå‡†](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark)ã€‚
æˆ‘ä»¬å‘ç°Qwenåœ¨ç”Ÿæˆä»£ç çš„å¯æ‰§è¡Œç‡ã€ç»“æœæ­£ç¡®æ€§ä¸Šå‡è¡¨ç°è¾ƒå¥½ï¼š

<table>
    <tr>
        <th colspan="4" align="center">ç”Ÿæˆä»£ç çš„å¯æ‰§è¡Œç‡ (%)</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Mathâ†‘</th><th align="center">Visualizationâ†‘</th><th align="center">Generalâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">91.9</td><td align="center">85.9</td><td align="center">82.8</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">89.2</td><td align="center">65.0</td><td align="center">74.1</td>
    </tr>
    <tr>
        <td>LLaMA2-7B-Chat</td>
        <td align="center">41.9</td>
        <td align="center">33.1</td>
        <td align="center">24.1 </td>
    </tr>
    <tr>
        <td>LLaMA2-13B-Chat</td>
        <td align="center">50.0</td>
        <td align="center">40.5</td>
        <td align="center">48.3 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-7B-Instruct</td>
        <td align="center">85.1</td>
        <td align="center">54.0</td>
        <td align="center">70.7 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-13B-Instruct</td>
        <td align="center">93.2</td>
        <td align="center">55.8</td>
        <td align="center">74.1 </td>
    </tr>
    <tr>
        <td>InternLM-7B-Chat-v1.1</td>
        <td align="center">78.4</td>
        <td align="center">44.2</td>
        <td align="center">62.1 </td>
    </tr>
    <tr>
        <td>InternLM-20B-Chat</td>
        <td align="center">70.3</td>
        <td align="center">44.2</td>
        <td align="center">65.5 </td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td>
        <td align="center">82.4</td>
        <td align="center">64.4</td>
        <td align="center">67.2 </td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td>
        <td align="center">89.2</td>
        <td align="center">84.1</td>
        <td align="center">65.5</td>
    </tr>
</table>

<table>
    <tr>
        <th colspan="4" align="center">ä»£ç æ‰§è¡Œç»“æœçš„æ­£ç¡®ç‡ (%)</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Mathâ†‘</th><th align="center">Visualization-Hardâ†‘</th><th align="center">Visualization-Easyâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">82.8</td><td align="center">66.7</td><td align="center">60.8</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">47.3</td><td align="center">33.3</td><td align="center">55.7</td>
    </tr>
    <tr>
        <td>LLaMA2-7B-Chat</td>
        <td align="center">3.9</td>
        <td align="center">14.3</td>
        <td align="center">39.2 </td>
    </tr>
    <tr>
        <td>LLaMA2-13B-Chat</td>
        <td align="center">8.3</td>
        <td align="center">8.3</td>
        <td align="center">40.5 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-7B-Instruct</td>
        <td align="center">14.3</td>
        <td align="center">26.2</td>
        <td align="center">60.8 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-13B-Instruct</td>
        <td align="center">28.2</td>
        <td align="center">27.4</td>
        <td align="center">62.0 </td>
    </tr>
    <tr>
        <td>InternLM-7B-Chat-v1.1</td>
        <td align="center">28.5</td>
        <td align="center">4.8</td>
        <td align="center">40.5 </td>
    </tr>
    <tr>
        <td>InternLM-20B-Chat</td>
        <td align="center">34.6</td>
        <td align="center">21.4</td>
        <td align="center">45.6 </td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td>
        <td align="center">41.9</td>
        <td align="center">40.5</td>
        <td align="center">54.4 </td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td>
        <td align="center">58.4</td>
        <td align="center">53.6</td>
        <td align="center">59.5</td>
    </tr>
</table>

<p align="center">
    <br>
    <img src="assets/code_interpreter_showcase_001.jpg" />
    <br>
<p>

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹å…·å¤‡æ‰®æ¼”HuggingFace Agentçš„èƒ½åŠ›ï¼Œè¯¦è§[ç¤ºä¾‹æ–‡æ¡£](examples/transformers_agent.md)äº†è§£æ›´å¤šä¿¡æ¯ã€‚æ¨¡å‹åœ¨Hugging Faceæä¾›çš„è¯„æµ‹æ•°æ®é›†ä¸Šè¡¨ç°å¦‚ä¸‹ï¼š

<table>
    <tr>
        <th colspan="4" align="center">HuggingFace Agentè¯„æµ‹åŸºå‡† - Runæ¨¡å¼</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selectionâ†‘</th><th align="center">Tool Usedâ†‘</th><th align="center">Codeâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">100</td><td align="center">100</td><td align="center">97.4</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">95.4</td><td align="center">96.3</td><td align="center">87.0</td>
    </tr>
    <tr>
        <td>StarCoder-Base-15B</td><td align="center">86.1</td><td align="center">87.0</td><td align="center">68.9</td>
    </tr>
    <tr>
        <td>StarCoder-15B</td><td align="center">87.0</td><td align="center">88.0</td><td align="center">68.9</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">87.0</td><td align="center">87.0</td><td align="center">71.5</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">93.5</td><td align="center">94.4</td><td align="center">87.0</td>
    </tr>
</table>

<table>
    <tr>
        <th colspan="4" align="center">HuggingFace Agentè¯„æµ‹åŸºå‡† - Chatæ¨¡å¼</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selectionâ†‘</th><th align="center">Tool Usedâ†‘</th><th align="center">Codeâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">98.5</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">97.3</td><td align="center">96.8</td><td align="center">89.6</td>
    </tr>
    <tr>
        <td>StarCoder-Base-15B</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">91.1</td>
    </tr>
    <tr>
        <td>StarCoder-15B</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">89.6</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">94.7</td><td align="center">94.7</td><td align="center">85.1</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">95.5</td>
    </tr>
</table>

<br>

## é•¿æ–‡æœ¬ç†è§£

æˆ‘ä»¬å¼•å…¥äº†NTKæ’å€¼ã€çª—å£æ³¨æ„åŠ›ã€LogNæ³¨æ„åŠ›ç¼©æ”¾ç­‰æŠ€æœ¯æ¥æå‡æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦å¹¶çªç ´è®­ç»ƒåºåˆ—é•¿åº¦çš„é™åˆ¶ã€‚é€šè¿‡arXivæ•°æ®é›†ä¸Šçš„è¯­è¨€æ¨¡å‹å®éªŒï¼Œæˆ‘ä»¬çš„åŸç”Ÿé•¿åº¦ä¸º2Kçš„Qwen-7B/14Båœ¨8Kçš„åºåˆ—é•¿åº¦ä¸‹ä¾ç„¶è¡¨ç°ä¸é”™ï¼Œè€ŒåŸç”Ÿé•¿åº¦æ‰©å±•åˆ°8Kçš„Qwen-7Bèƒ½å¤Ÿåœ¨32Ké•¿åºåˆ—çš„è®¾ç½®ä¸‹å–å¾—ä¸é”™çš„è¡¨ç°ã€‚

<table>
    <tr>
        <th rowspan="2">Model</th><th colspan="6" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th><th align="center">16384</th><th align="center">32768</th>
    </tr>
     <tr>
        <td>Qwen-7B (original)</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">39.35</td><td align="center">469.81</td><td align="center">2645.09</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.59</td><td align="center">3.66</td><td align="center">5.71</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.58</td><td align="center">3.56</td><td align="center">4.62</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.58</td><td align="center">3.49</td><td align="center">4.32</td><td align="center">-</td>
    </tr>
    <tr>
    <tr>
        <td>Qwen-7B</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.31</b></td><td align="center">7.27</td><td align="center">181.49</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.31</b></td><td align="center"><b>3.23</b></td><td align="center">3.33</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.33</b></td><td align="center"><b>3.22</b></td><td align="center"><b>3.17</b></td>
    </tr>
    <tr>
        <td>Qwen-14B</td><td align="center"><b>-</b></td><td align="center"><b>3.46</b></td><td align="center">22.79</td><td align="center">334.65</td><td align="center">3168.35</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>-</b></td><td align="center"><b>3.46</b></td><td align="center"><b>3.29</b></td><td align="center"><b>3.18</b></td><td align="center">3.42</td><td align="center">-</td>
    </tr>
</table>

## Tokenization

> æ³¨ï¼šä½œä¸ºæœ¯è¯­çš„â€œtokenizationâ€åœ¨ä¸­æ–‡ä¸­å°šæ— å…±è¯†çš„æ¦‚å¿µå¯¹åº”ï¼Œæœ¬æ–‡æ¡£é‡‡ç”¨è‹±æ–‡è¡¨è¾¾ä»¥åˆ©è¯´æ˜ã€‚

åŸºäºtiktokençš„tokenizeræœ‰åˆ«äºå…¶ä»–åˆ†è¯å™¨ï¼Œæ¯”å¦‚sentencepiece tokenizerã€‚å°¤å…¶åœ¨å¾®è°ƒé˜¶æ®µï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„ç‰¹æ®Štokençš„ä½¿ç”¨ã€‚å…³äºtokenizerçš„æ›´å¤šä¿¡æ¯ï¼Œä»¥åŠå¾®è°ƒæ—¶æ¶‰åŠçš„ç›¸å…³ä½¿ç”¨ï¼Œè¯·å‚é˜…[æ–‡æ¡£](tokenization_note_zh.md)ã€‚
<br><br>

## å¤ç°

æˆ‘ä»¬æä¾›äº†è¯„æµ‹è„šæœ¬ä»¥ä¾›å¤ç°æˆ‘ä»¬çš„å®éªŒç»“æœã€‚æ³¨æ„ï¼Œç”±äºå†…éƒ¨ä»£ç å’Œå¼€æºä»£ç å­˜åœ¨å°‘è®¸å·®å¼‚ï¼Œè¯„æµ‹ç»“æœå¯èƒ½ä¸æ±‡æŠ¥ç»“æœå­˜åœ¨ç»†å¾®çš„ç»“æœä¸ä¸€è‡´ã€‚è¯·é˜…è¯»[eval/EVALUATION.md](eval/EVALUATION.md)äº†è§£æ›´å¤šä¿¡æ¯ã€‚
<br><br>

## FAQ

å¦‚é‡åˆ°é—®é¢˜ï¼Œæ•¬è¯·æŸ¥é˜…[FAQ](FAQ_zh.md)ä»¥åŠissueåŒºï¼Œå¦‚ä»æ— æ³•è§£å†³å†æäº¤issueã€‚
<br><br>

## å¼•ç”¨
å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ï¼

```
@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
```
<br>

## ä½¿ç”¨åè®®

ç ”ç©¶äººå‘˜ä¸å¼€å‘è€…å¯ä½¿ç”¨Qwenå’ŒQwen-Chatæˆ–è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€‚æˆ‘ä»¬åŒæ ·å…è®¸å•†ä¸šä½¿ç”¨ï¼Œå…·ä½“ç»†èŠ‚è¯·æŸ¥çœ‹[LICENSE](LICENSE)ã€‚å¦‚éœ€å•†ç”¨ï¼Œè¯·å¡«å†™é—®å·([7B](https://dashscope.console.aliyun.com/openModelApply/qianwen), [14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat))ç”³è¯·ã€‚
<br><br>

## è”ç³»æˆ‘ä»¬

å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œæ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤å’ŒDiscord serverã€‚å½“ç„¶ä¹Ÿå¯ä»¥é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚

