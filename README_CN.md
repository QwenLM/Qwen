<p align="left">
        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>&nbsp ï½œ &nbsp<a href="README_JA.md">æ—¥æœ¬èª</a>
</p>
<br><br>

<p align="center">
    <img src="assets/logo.jpg" width="400"/>
<p>
<br>

<p align="center">
        Qwen-7B <a href="https://modelscope.cn/models/qwen/Qwen-7B/summary">ğŸ¤– <a> | <a href="https://huggingface.co/Qwen/Qwen-7B">ğŸ¤—</a>&nbsp ï½œ Qwen-7B-Chat <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary">ğŸ¤– <a> | <a href="https://huggingface.co/Qwen/Qwen-7B-Chat">ğŸ¤—</a>&nbsp | Qwen-7B-Chat-Int4 <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int4">ğŸ¤—</a>
<br>
<a href="assets/wechat.png">WeChat</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://modelscope.cn/studios/qwen/Qwen-7B-Chat-Demo/summary">Demo</a>&nbsp ï½œ &nbsp<a href="https://github.com/QwenLM/Qwen-7B/blob/main/tech_memo.md">Report</a>
</p>
<br><br>

æˆ‘ä»¬åœ¨ğŸ¤– **ModelScope**ä»¥åŠğŸ¤— **Hugging Face**å‡å¼€æºäº†**Qwen-7B**ç³»åˆ—æ¨¡å‹ã€‚è¯·åœ¨æœ¬æ–‡æ¡£é¡¶éƒ¨ç‚¹å‡»ç›¸å…³é“¾æ¥æŸ¥çœ‹ä»“åº“ä¿¡æ¯ã€‚æœ¬ä»“åº“ä¸»è¦åŒ…æ‹¬Qwen-7Bçš„ç®€ä»‹ã€ä½¿ç”¨æŒ‡å—ã€æŠ€æœ¯å¤‡å¿˜ç­‰å†…å®¹ã€‚æƒ³äº†è§£æ›´å¤šå…³äºæ¨¡å‹çš„ä¿¡æ¯ï¼Œè¯·ç‚¹å‡»[é“¾æ¥](tech_memo.md)æŸ¥çœ‹æˆ‘ä»¬çš„æŠ€æœ¯å¤‡å¿˜å½•ã€‚

é€šä¹‰åƒé—®-7Bï¼ˆQwen-7Bï¼‰ æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„é€šä¹‰åƒé—®å¤§æ¨¡å‹ç³»åˆ—çš„70äº¿å‚æ•°è§„æ¨¡çš„æ¨¡å‹ã€‚Qwen-7Bæ˜¯åŸºäºTransformerçš„å¤§è¯­è¨€æ¨¡å‹, åœ¨è¶…å¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå¾—åˆ°ã€‚é¢„è®­ç»ƒæ•°æ®ç±»å‹å¤šæ ·ï¼Œè¦†ç›–å¹¿æ³›ï¼ŒåŒ…æ‹¬å¤§é‡ç½‘ç»œæ–‡æœ¬ã€ä¸“ä¸šä¹¦ç±ã€ä»£ç ç­‰ã€‚åŒæ—¶ï¼Œåœ¨Qwen-7Bçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹é½æœºåˆ¶æ‰“é€ äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„AIåŠ©æ‰‹Qwen-7B-Chatã€‚Qwen-7Bç³»åˆ—æ¨¡å‹çš„ç‰¹ç‚¹åŒ…æ‹¬ï¼š

1. **å¤§è§„æ¨¡é«˜è´¨é‡é¢„è®­ç»ƒæ•°æ®**ï¼šæˆ‘ä»¬ä½¿ç”¨äº†è¶…è¿‡2.2ä¸‡äº¿tokençš„è‡ªå»ºå¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†è¿›è¡Œè¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒã€‚æ•°æ®é›†åŒ…æ‹¬æ–‡æœ¬å’Œä»£ç ç­‰å¤šç§æ•°æ®ç±»å‹ï¼Œè¦†ç›–é€šç”¨é¢†åŸŸå’Œä¸“ä¸šé¢†åŸŸã€‚
2. **ä¼˜ç§€çš„æ¨¡å‹æ€§èƒ½**ï¼šç›¸æ¯”åŒè§„æ¨¡çš„å¼€æºæ¨¡å‹ï¼ŒQwen-7Båœ¨å¤šä¸ªè¯„æµ‹æ•°æ®é›†ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œç”šè‡³è¶…å‡º12-13Bç­‰æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚è¯„æµ‹è¯„ä¼°çš„èƒ½åŠ›èŒƒå›´åŒ…æ‹¬è‡ªç„¶è¯­è¨€ç†è§£ä¸ç”Ÿæˆã€æ•°å­¦è¿ç®—è§£é¢˜ã€ä»£ç ç”Ÿæˆç­‰ã€‚
3. **æ›´å¥½åœ°æ”¯æŒå¤šè¯­è¨€**ï¼šåŸºäºæ›´å¤§è¯è¡¨çš„åˆ†è¯å™¨åœ¨åˆ†è¯ä¸Šæ›´é«˜æ•ˆï¼ŒåŒæ—¶å®ƒå¯¹å…¶ä»–è¯­è¨€è¡¨ç°æ›´åŠ å‹å¥½ã€‚ç”¨æˆ·å¯ä»¥åœ¨Qwen-7Bçš„åŸºç¡€ä¸Šæ›´æ–¹ä¾¿åœ°è®­ç»ƒç‰¹å®šè¯­è¨€çš„7Bè¯­è¨€æ¨¡å‹ã€‚
4. **8Kçš„ä¸Šä¸‹æ–‡é•¿åº¦**ï¼šQwen-7BåŠQwen-7B-Chatå‡èƒ½æ”¯æŒ8Kçš„ä¸Šä¸‹æ–‡é•¿åº¦, å…è®¸ç”¨æˆ·è¾“å…¥æ›´é•¿çš„promptã€‚
5. **æ”¯æŒæ’ä»¶è°ƒç”¨**ï¼šQwen-7B-Chaté’ˆå¯¹æ’ä»¶è°ƒç”¨ç›¸å…³çš„å¯¹é½æ•°æ®åšäº†ç‰¹å®šä¼˜åŒ–ï¼Œå½“å‰æ¨¡å‹èƒ½æœ‰æ•ˆè°ƒç”¨æ’ä»¶ä»¥åŠå‡çº§ä¸ºAgentã€‚

ä»¥ä¸‹ç« èŠ‚çš„ä¿¡æ¯å¯èƒ½å¯¹ä½ æœ‰å¸®åŠ©ï¼Œå»ºè®®é˜…è¯»ã€‚å¦‚æœä½ åœ¨ä½¿ç”¨è¿‡ç¨‹é‡åˆ°é—®é¢˜ï¼Œå»ºè®®å…ˆæŸ¥è¯¢FAQï¼Œå¦‚ä»æ— æ³•è§£å†³å†æäº¤issueã€‚
<br>

## æ–°é—»

* 2023å¹´8æœˆ21æ—¥ å‘å¸ƒQwen-7B-Chatçš„Int4é‡åŒ–æ¨¡å‹ï¼ŒQwen-7B-Chat-Int4ã€‚è¯¥æ¨¡å‹æ˜¾å­˜å ç”¨ä½ï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åŠç²¾åº¦æ¨¡å‹æ˜¾è‘—æå‡ï¼Œåœ¨åŸºå‡†è¯„æµ‹ä¸Šæ•ˆæœæŸå¤±è¾ƒå°ã€‚
* 2023å¹´8æœˆ3æ—¥ åœ¨é­”æ­ç¤¾åŒºï¼ˆModelScopeï¼‰å’ŒHugging FaceåŒæ­¥æ¨å‡ºQwen-7Bå’ŒQwen-7B-Chatæ¨¡å‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æŠ€æœ¯å¤‡å¿˜å½•ï¼Œä»‹ç»äº†ç›¸å…³çš„è®­ç»ƒç»†èŠ‚å’Œæ¨¡å‹è¡¨ç°ã€‚
<br>

## è¯„æµ‹è¡¨ç°

Qwen-7Båœ¨å¤šä¸ªå…¨é¢è¯„ä¼°è‡ªç„¶è¯­è¨€ç†è§£ä¸ç”Ÿæˆã€æ•°å­¦è¿ç®—è§£é¢˜ã€ä»£ç ç”Ÿæˆç­‰èƒ½åŠ›çš„è¯„æµ‹æ•°æ®é›†ä¸Šï¼ŒåŒ…æ‹¬MMLUã€C-Evalã€GSM8Kã€HumanEvalã€WMT22ã€CMMLUç­‰ï¼Œå‡è¶…å‡ºäº†åŒè§„æ¨¡å¤§è¯­è¨€æ¨¡å‹çš„è¡¨ç°ï¼Œç”šè‡³è¶…å‡ºäº†å¦‚12-13Bå‚æ•°ç­‰æ›´å¤§è§„æ¨¡çš„è¯­è¨€æ¨¡å‹ã€‚

| Model             | MMLU           |         C-Eval |          GSM8K |      HumanEval |  WMT22 (en-zh) |         CMMLU |
| :---------------- | :------------: | :------------: | :------------: | :------------: | :------------: |:------------: |
| LLaMA-7B          | 35.1           |              - |           11.0 |           10.5 |            8.7 |             - |
| LLaMA 2-7B        | 45.3           |              - |           14.6 |           12.8 |           17.9 |             - |
| Baichuan-7B       | 42.3           |           42.8 |            9.7 |            9.2 |           26.6 |          44.4 |
| ChatGLM2-6B       | 47.9           |           51.7 |           32.4 |            9.2 |              - |          48.8 |
| InternLM-7B       | 51.0           |           52.8 |           31.2 |           10.4 |           14.8 |             - |
| Baichuan-13B      | 51.6           |           53.6 |           26.6 |           12.8 |           30.0 |          55.8 |
| LLaMA-13B         | 46.9           |           35.5 |           17.8 |           15.8 |           12.0 |             - |
| LLaMA 2-13B       | 54.8           |              - |           28.7 |           18.3 |           24.2 |             - |
| ChatGLM2-12B      | 56.2           |       **61.6** |           40.9 |              - |              - |             - |
| **Qwen-7B**       | **56.7**       |           59.6 |       **51.6** |       **24.4** |       **30.6** |      **58.8** |

<p align="center">
    <img src="assets/performance.png" width="1000"/>
<p>
<br>

æ­¤å¤–ï¼Œæ ¹æ®[OpenCompass](https://opencompass.org.cn/leaderboard-llm)è¿›è¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ç¬¬ä¸‰æ–¹è¯„ä¼°ï¼ŒQwen-7B å’Œ Qwen-7B-Chat æ˜¯å…¶ä¸­è¡¨ç°æœ€ä¼˜çš„7Bå‚æ•°æ¨¡å‹ã€‚è¯¥è¯„ä¼°ç”±å¤§é‡å…¬å¼€åŸºå‡†ç»„æˆï¼Œç”¨äºè¯„ä¼°è¯­è¨€ç†è§£å’Œç”Ÿæˆã€ä»£ç ç”Ÿæˆã€æ•°å­¦ã€æ¨ç†ç­‰ã€‚

æ›´å¤šçš„å®éªŒç»“æœå’Œç»†èŠ‚è¯·æŸ¥çœ‹æˆ‘ä»¬çš„æŠ€æœ¯å¤‡å¿˜å½•ã€‚ç‚¹å‡»[è¿™é‡Œ](tech_memo.md)ã€‚
<br>

## è¦æ±‚

* python 3.8åŠä»¥ä¸Šç‰ˆæœ¬
* pytorch 1.12åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œæ¨è2.0åŠä»¥ä¸Šç‰ˆæœ¬
* å»ºè®®ä½¿ç”¨CUDA 11.4åŠä»¥ä¸Šï¼ˆGPUç”¨æˆ·ã€flash-attentionç”¨æˆ·ç­‰éœ€è€ƒè™‘æ­¤é€‰é¡¹ï¼‰
<br>

## å¿«é€Ÿä½¿ç”¨

æˆ‘ä»¬æä¾›ç®€å•çš„ç¤ºä¾‹æ¥è¯´æ˜å¦‚ä½•åˆ©ç”¨ğŸ¤– ModelScopeå’ŒğŸ¤— Transformerså¿«é€Ÿä½¿ç”¨Qwen-7Bå’ŒQwen-7B-Chatã€‚

åœ¨å¼€å§‹å‰ï¼Œè¯·ç¡®ä¿ä½ å·²ç»é…ç½®å¥½ç¯å¢ƒå¹¶å®‰è£…å¥½ç›¸å…³çš„ä»£ç åŒ…ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œç¡®ä¿ä½ æ»¡è¶³ä¸Šè¿°è¦æ±‚ï¼Œç„¶åå®‰è£…ç›¸å…³çš„ä¾èµ–åº“ã€‚

```bash
pip install -r requirements.txt
```

å¦‚æœä½ çš„æ˜¾å¡æ”¯æŒfp16æˆ–bf16ç²¾åº¦ï¼Œæˆ‘ä»¬è¿˜æ¨èå®‰è£…[flash-attention](https://github.com/Dao-AILab/flash-attention)æ¥æé«˜ä½ çš„è¿è¡Œæ•ˆç‡ä»¥åŠé™ä½æ˜¾å­˜å ç”¨ã€‚(**flash-attentionåªæ˜¯å¯é€‰é¡¹ï¼Œä¸å®‰è£…ä¹Ÿå¯æ­£å¸¸è¿è¡Œè¯¥é¡¹ç›®**)

```bash
git clone -b v1.0.8 https://github.com/Dao-AILab/flash-attention
cd flash-attention && pip install .
# ä¸‹æ–¹å®‰è£…å¯é€‰ï¼Œå®‰è£…å¯èƒ½æ¯”è¾ƒç¼“æ…¢ã€‚
# pip install csrc/layer_norm
# pip install csrc/rotary
```

æ¥ä¸‹æ¥ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨Transformersæˆ–è€…ModelScopeæ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚

#### ğŸ¤— Transformers

å¦‚å¸Œæœ›ä½¿ç”¨Qwen-7B-chatè¿›è¡Œæ¨ç†ï¼Œæ‰€éœ€è¦å†™çš„åªæ˜¯å¦‚ä¸‹æ‰€ç¤ºçš„æ•°è¡Œä»£ç ã€‚**è¯·ç¡®ä¿ä½ ä½¿ç”¨çš„æ˜¯æœ€æ–°ä»£ç ã€‚**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# è¯·æ³¨æ„ï¼šåˆ†è¯å™¨é»˜è®¤è¡Œä¸ºå·²æ›´æ”¹ä¸ºé»˜è®¤å…³é—­ç‰¹æ®Štokenæ”»å‡»é˜²æŠ¤ã€‚
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤ä½¿ç”¨è‡ªåŠ¨æ¨¡å¼ï¼Œæ ¹æ®è®¾å¤‡è‡ªåŠ¨é€‰æ‹©ç²¾åº¦
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True).eval()

# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# ç¬¬ä¸€è½®å¯¹è¯
response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚

# ç¬¬äºŒè½®å¯¹è¯
response, history = model.chat(tokenizer, "ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚", history=history)
print(response)
# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚
# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚
# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚
# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚
# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚
# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚

# ç¬¬ä¸‰è½®å¯¹è¯
response, history = model.chat(tokenizer, "ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜", history=history)
print(response)
# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹
```

è¿è¡ŒQwen-7BåŒæ ·éå¸¸ç®€å•ã€‚

<details>
  <summary>è¿è¡ŒQwen-7B</summary>

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)

# æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, bf16=True).eval()
# æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, fp16=True).eval()
# ä½¿ç”¨CPUè¿›è¡Œæ¨ç†ï¼Œéœ€è¦çº¦32GBå†…å­˜
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="cpu", trust_remote_code=True).eval()
# é»˜è®¤ä½¿ç”¨è‡ªåŠ¨æ¨¡å¼ï¼Œæ ¹æ®è®¾å¤‡è‡ªåŠ¨é€‰æ‹©ç²¾åº¦
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True).eval()

# å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)

inputs = tokenizer('è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯', return_tensors='pt')
inputs = inputs.to(model.device)
pred = model.generate(**inputs)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))
# è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯äºšçš„æ–¯äºšè´å·´ï¼ˆAddis Ababaï¼‰...
```

</details>

#### ğŸ¤– ModelScope

é­”æ­ï¼ˆModelScopeï¼‰æ˜¯å¼€æºçš„æ¨¡å‹å³æœåŠ¡å…±äº«å¹³å°ï¼Œä¸ºæ³›AIå¼€å‘è€…æä¾›çµæ´»ã€æ˜“ç”¨ã€ä½æˆæœ¬çš„ä¸€ç«™å¼æ¨¡å‹æœåŠ¡äº§å“ã€‚ä½¿ç”¨ModelScopeåŒæ ·éå¸¸ç®€å•ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
import os
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks
from modelscope import snapshot_download

model_id = 'QWen/qwen-7b-chat'
revision = 'v1.0.0'

model_dir = snapshot_download(model_id, revision)

pipe = pipeline(
task=Tasks.chat, model=model_dir, device_map='auto')
history = None

text = 'æµ™æ±Ÿçš„çœä¼šåœ¨å“ªé‡Œï¼Ÿ'
results = pipe(text, history=history)
response, history = results['response'], results['history']
print(f'Response: {response}')
text = 'å®ƒæœ‰ä»€ä¹ˆå¥½ç©çš„åœ°æ–¹å‘¢ï¼Ÿ'
results = pipe(text, history=history)
response, history = results['response'], results['history']
print(f'Response: {response}')
```
<br>

## Tokenization

> æ³¨ï¼šä½œä¸ºæœ¯è¯­çš„â€œtokenizationâ€åœ¨ä¸­æ–‡ä¸­å°šæ— å…±è¯†çš„æ¦‚å¿µå¯¹åº”ï¼Œæœ¬æ–‡æ¡£é‡‡ç”¨è‹±æ–‡è¡¨è¾¾ä»¥åˆ©è¯´æ˜ã€‚

åŸºäºtiktokençš„tokenizeræœ‰åˆ«äºå…¶ä»–åˆ†è¯å™¨ï¼Œæ¯”å¦‚sentencepiece tokenizerã€‚å°¤å…¶åœ¨å¾®è°ƒé˜¶æ®µï¼Œéœ€è¦ç‰¹åˆ«æ³¨æ„ç‰¹æ®Štokençš„ä½¿ç”¨ã€‚å…³äºtokenizerçš„æ›´å¤šä¿¡æ¯ï¼Œä»¥åŠå¾®è°ƒæ—¶æ¶‰åŠçš„ç›¸å…³ä½¿ç”¨ï¼Œè¯·å‚é˜…[æ–‡æ¡£](tokenization_note_zh.md)ã€‚
<br>

## é‡åŒ–

### ç”¨æ³•

**è¯·æ³¨æ„ï¼šæˆ‘ä»¬æ›´æ–°é‡åŒ–æ–¹æ¡ˆä¸ºåŸºäº[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)çš„é‡åŒ–ï¼Œæä¾›Qwen-7B-Chatçš„Int4é‡åŒ–æ¨¡å‹[ç‚¹å‡»è¿™é‡Œ](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4)ã€‚ç›¸æ¯”æ­¤å‰æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆåœ¨æ¨¡å‹è¯„æµ‹æ•ˆæœå‡ ä¹æ— æŸï¼Œä¸”å­˜å‚¨éœ€æ±‚æ›´ä½ï¼Œæ¨ç†é€Ÿåº¦æ›´ä¼˜ã€‚**

ä»¥ä¸‹æˆ‘ä»¬æä¾›ç¤ºä¾‹è¯´æ˜å¦‚ä½•ä½¿ç”¨Int4é‡åŒ–æ¨¡å‹ã€‚åœ¨å¼€å§‹ä½¿ç”¨å‰ï¼Œè¯·å…ˆä¿è¯æ»¡è¶³AutoGPTQçš„è¦æ±‚ï¼Œå¹¶ä½¿ç”¨æºä»£ç å®‰è£…ï¼ˆç”±äºæœ€æ–°æ”¯æŒQwençš„ä»£ç æœªå‘å¸ƒåˆ°PyPIï¼‰ï¼š

```bash
git clone https://github.com/PanQiWei/AutoGPTQ.git && cd AutoGPTQ
pip install .
```

éšåä¾¿èƒ½è½»æ¾è¯»å–é‡åŒ–æ¨¡å‹ï¼š

```python
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_quantized("Qwen/Qwen-7B-Chat-Int4", device_map="auto", trust_remote_code=True, use_safetensors=True).eval()
```

æ¨ç†æ–¹æ³•å’ŒåŸºç¡€ç”¨æ³•ç±»ä¼¼ï¼Œä½†æ³¨æ„éœ€è¦ä»å¤–éƒ¨ä¼ å…¥generation configï¼š

```python
from transformers import GenerationConfig
config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat-Int4", trust_remote_code=True)
response, history = model.chat(tokenizer, "Hi", history=None, generation_config=config)
```

### æ•ˆæœè¯„æµ‹

æˆ‘ä»¬å¯¹BF16å’ŒInt4æ¨¡å‹åœ¨åŸºå‡†è¯„æµ‹ä¸Šåšäº†æµ‹è¯•ï¼Œå‘ç°é‡åŒ–æ¨¡å‹æ•ˆæœæŸå¤±è¾ƒå°ï¼Œç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

|  Quantization |   MMLU     |  CEval (val) |  GSM8K |  Humaneval |
| ------------- | :--------: | :----------: | :----: | :--------: |
| BF16          |    53.9    |     54.2     |  41.1  |    24.4    |
| Int4          |    52.6    |     52.9     |  38.1  |    23.8    |

### æ¨ç†é€Ÿåº¦

æˆ‘ä»¬æµ‹ç®—äº†BF16å’ŒInt4æ¨¡å‹ç”Ÿæˆ2048å’Œ8192ä¸ªtokençš„å¹³å‡æ¨ç†é€Ÿåº¦ï¼ˆtokens/sï¼‰ã€‚å¦‚å›¾æ‰€ç¤ºï¼š

|  Quantization | Speed (2048 tokens) | Speed (8192 tokens) |
| ------------- | :------------------:| :------------------:|
|      BF16     | 30.34               | 29.32               |
|      Int4     | 43.56               | 33.92               |

å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è®°å½•åœ¨é•¿åº¦ä¸º1çš„ä¸Šä¸‹æ–‡çš„æ¡ä»¶ä¸‹ç”Ÿæˆ8192ä¸ªtokençš„æ€§èƒ½ã€‚è¯„æµ‹è¿è¡Œäºå•å¼ A100-SXM4-80G GPUï¼Œä½¿ç”¨PyTorch 2.0.1å’ŒCUDA 11.4ã€‚æ¨ç†é€Ÿåº¦æ˜¯ç”Ÿæˆ8192ä¸ªtokençš„é€Ÿåº¦å‡å€¼ã€‚

### æ˜¾å­˜ä½¿ç”¨

æˆ‘ä»¬è¿˜æµ‹ç®—äº†BF16å’ŒInt4æ¨¡å‹ç¼–ç 2048ä¸ªtokenåŠç”Ÿæˆ8192ä¸ªtokençš„å³°å€¼æ˜¾å­˜å ç”¨æƒ…å†µã€‚ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

| Quantization Level | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |
| ------------------ | :---------------------------------: | :-----------------------------------: |
| BF16               |               17.66GB               |                22.58GB                |
| Int4               |               8.21GB                |                13.62GB                |

ä¸Šè¿°æ€§èƒ½æµ‹ç®—ä½¿ç”¨[æ­¤è„šæœ¬](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py)å®Œæˆã€‚
<br>

## Demo

### Web UI

æˆ‘ä»¬æä¾›äº†Web UIçš„demoä¾›ç”¨æˆ·ä½¿ç”¨ (æ„Ÿè°¢ @wysaid æ”¯æŒ)ã€‚åœ¨å¼€å§‹å‰ï¼Œç¡®ä¿å·²ç»å®‰è£…å¦‚ä¸‹ä»£ç åº“ï¼š

```
pip install -r requirements_web_demo.txt
```

éšåè¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå¹¶ç‚¹å‡»ç”Ÿæˆé“¾æ¥ï¼š

```
python web_demo.py
```

<p align="center">
    <br>
    <img src="assets/web_demo.gif" width="600" />
    <br>
<p>

### äº¤äº’å¼Demo

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€å•çš„äº¤äº’å¼Demoç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹`cli_demo.py`ã€‚å½“å‰æ¨¡å‹å·²ç»æ”¯æŒæµå¼è¾“å‡ºï¼Œç”¨æˆ·å¯é€šè¿‡è¾“å…¥æ–‡å­—çš„æ–¹å¼å’ŒQwen-7B-Chatäº¤äº’ï¼Œæ¨¡å‹å°†æµå¼è¾“å‡ºè¿”å›ç»“æœã€‚è¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š

```
python cli_demo.py
```

<p align="center">
    <br>
    <img src="assets/cli_demo.gif" width="600" />
    <br>
<p>

## API

æˆ‘ä»¬æä¾›äº†OpenAI APIæ ¼å¼çš„æœ¬åœ°APIéƒ¨ç½²æ–¹æ³•ï¼ˆæ„Ÿè°¢@hanpenggitï¼‰ã€‚åœ¨å¼€å§‹ä¹‹å‰å…ˆå®‰è£…å¿…è¦çš„ä»£ç åº“ï¼š

```bash
pip install fastapi uvicorn openai pydantic sse_starlette
```

éšåå³å¯è¿è¡Œä»¥ä¸‹å‘½ä»¤éƒ¨ç½²ä½ çš„æœ¬åœ°APIï¼š

```bash
python openai_api.py
```

ä½ ä¹Ÿå¯ä»¥ä¿®æ”¹å‚æ•°ï¼Œæ¯”å¦‚`-c`æ¥ä¿®æ”¹æ¨¡å‹åç§°æˆ–è·¯å¾„, `--cpu-only`æ”¹ä¸ºCPUéƒ¨ç½²ç­‰ç­‰ã€‚å¦‚æœéƒ¨ç½²å‡ºç°é—®é¢˜ï¼Œæ›´æ–°ä¸Šè¿°ä»£ç åº“å¾€å¾€å¯ä»¥è§£å†³å¤§å¤šæ•°é—®é¢˜ã€‚

ä½¿ç”¨APIåŒæ ·éå¸¸ç®€å•ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š

```python
import openai
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "none"

# ä½¿ç”¨æµå¼å›å¤çš„è¯·æ±‚
for chunk in openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ],
    stream=True
    # æµå¼è¾“å‡ºçš„è‡ªå®šä¹‰stopwordsåŠŸèƒ½å°šæœªæ”¯æŒï¼Œæ­£åœ¨å¼€å‘ä¸­
):
    if hasattr(chunk.choices[0].delta, "content"):
        print(chunk.choices[0].delta.content, end="", flush=True)

# ä¸ä½¿ç”¨æµå¼å›å¤çš„è¯·æ±‚
response = openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ],
    stream=False,
    stop=[] # åœ¨æ­¤å¤„æ·»åŠ è‡ªå®šä¹‰çš„stop words ä¾‹å¦‚ReAct promptingæ—¶éœ€è¦å¢åŠ ï¼š stop=["Observation:"]ã€‚
)
print(response.choices[0].message.content)
```

<p align="center">
    <br>
    <img src="assets/openai_api.gif" width="600" />
    <br>
<p>

è¯¥æ¥å£ä¹Ÿæ”¯æŒå‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰ï¼Œä½†æš‚æ—¶ä»…é™ `stream=False` æ—¶èƒ½ç”Ÿæ•ˆã€‚ç”¨æ³•è§[å‡½æ•°è°ƒç”¨ç¤ºä¾‹](examples/function_call_examples.py)ã€‚
<br>

## éƒ¨ç½²

åœ¨CPUä¸Šè¿è¡Œéå¸¸ç®€å•ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
```

å¦‚æœä½ é‡åˆ°æ˜¾å­˜ä¸è¶³çš„é—®é¢˜è€Œå¸Œæœ›ä½¿ç”¨å¤šå¼ GPUè¿›è¡Œæ¨ç†ï¼Œå¯ä»¥ä½¿ç”¨æä¾›çš„è„šæœ¬`utils.py`:

```python
from utils import load_model_on_gpus
model = load_model_on_gpus('Qwen/Qwen-7B-Chat', num_gpus=2)
```

ä½ å³å¯ä½¿ç”¨2å¼ GPUè¿›è¡Œæ¨ç†ã€‚
<br>

## å·¥å…·è°ƒç”¨

Qwen-7B-Chaté’ˆå¯¹åŒ…æ‹¬APIã€æ•°æ®åº“ã€æ¨¡å‹ç­‰å·¥å…·åœ¨å†…çš„è°ƒç”¨è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç”¨æˆ·å¯ä»¥å¼€å‘åŸºäºQwen-7Bçš„LangChainã€Agentç”šè‡³Code Interpreterã€‚åœ¨æˆ‘ä»¬å¼€æºçš„[è¯„æµ‹æ•°æ®é›†](eval/EVALUATION.md)ä¸Šæµ‹è¯•æ¨¡å‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œå¹¶å‘ç°Qwen-7B-Chatèƒ½å¤Ÿå–å¾—ç¨³å®šçš„è¡¨ç°ã€‚

| Model            | Tool Selection (Acc.â†‘) | Tool Input (Rouge-Lâ†‘)  | False Positive Errorâ†“  |
|:-----------------|:----------------------:|:----------------------:|:----------------------:|
| GPT-4            | 95%                    | **0.90**               | 15%                    |
| GPT-3.5          | 85%                    | 0.88                   | 75%                    |
| **Qwen-7B-Chat** | **99%**                | 0.89                   | **9.7%**               |

æˆ‘ä»¬æä¾›äº†æ–‡æ¡£è¯´æ˜å¦‚ä½•æ ¹æ®ReAct Promptingçš„åŸåˆ™å†™ä½œä½ çš„promptã€‚

For how to write and use prompts for ReAct Prompting, please refer to [the ReAct examples](examples/react_prompt.md)ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹æ‰®æ¼”Agentçš„èƒ½åŠ›ã€‚è¯·é˜…è¯»ç›¸å…³æ–‡æ¡£[é“¾æ¥](https://huggingface.co/docs/transformers/transformers_agents)äº†è§£æ›´å¤šä¿¡æ¯ã€‚æ¨¡å‹åœ¨Hugging Faceæä¾›çš„è¯„æµ‹æ•°æ®é›†ä¸Šè¡¨ç°å¦‚ä¸‹ï¼š

| Model            | Tool Selectionâ†‘ | Tool Usedâ†‘  |   Codeâ†‘   |
|:-----------------|:---------------:|:-----------:|:---------:|
| GPT-4            |     **100**     |   **100**   | **97.41** |
| GPT-3.5          |      95.37      |    96.30    |   87.04   |
| StarCoder-15.5B  |      87.04      |    87.96    |   68.89   |
| **Qwen-7B-Chat** |      90.74      |    92.59    |   74.07   |

<br>

## é•¿æ–‡æœ¬ç†è§£

æˆ‘ä»¬å¼•å…¥äº†NTKæ’å€¼ã€çª—å£æ³¨æ„åŠ›ã€LogNæ³¨æ„åŠ›ç¼©æ”¾ç­‰æŠ€æœ¯æ¥æå‡æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦å¹¶çªç ´è®­ç»ƒåºåˆ—é•¿åº¦çš„é™åˆ¶ã€‚æˆ‘ä»¬çš„æ¨¡å‹å·²ç»çªç ´8Kçš„åºåˆ—é•¿åº¦ã€‚é€šè¿‡arXivæ•°æ®é›†ä¸Šçš„è¯­è¨€æ¨¡å‹å®éªŒï¼Œæˆ‘ä»¬å‘ç°Qwen-7Bèƒ½å¤Ÿåœ¨é•¿åºåˆ—çš„è®¾ç½®ä¸‹å–å¾—ä¸é”™çš„è¡¨ç°ã€‚

<table>
    <tr>
        <th rowspan="2">Model</th><th colspan="5" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th><th align="center">16384</th>
    </tr>
    <tr>
        <td>Qwen-7B</td><td align="center"><b>4.23</b></td><td align="center"><b>3.78</b></td><td align="center">39.35</td><td align="center">469.81</td><td align="center">2645.09</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk</td><td align="center"><b>4.23</b></td><td align="center"><b>3.78</b></td><td align="center">3.59</td><td align="center">3.66</td><td align="center">5.71</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn</td><td align="center"><b>4.23</b></td><td align="center"><b>3.78</b></td><td align="center"><b>3.58</b></td><td align="center">3.56</td><td align="center">4.62</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + local_attn</td><td align="center"><b>4.23</b></td><td align="center"><b>3.78</b></td><td align="center"><b>3.58</b></td><td align="center"><b>3.49</b></td><td align="center"><b>4.32</b></td>
    </tr>
</table>
<br>

## å¤ç°

æˆ‘ä»¬æä¾›äº†è¯„æµ‹è„šæœ¬ä»¥ä¾›å¤ç°æˆ‘ä»¬çš„å®éªŒç»“æœã€‚æ³¨æ„ï¼Œç”±äºå†…éƒ¨ä»£ç å’Œå¼€æºä»£ç å­˜åœ¨å°‘è®¸å·®å¼‚ï¼Œè¯„æµ‹ç»“æœå¯èƒ½ä¸æ±‡æŠ¥ç»“æœå­˜åœ¨ç»†å¾®çš„ç»“æœä¸ä¸€è‡´ã€‚è¯·é˜…è¯»[eval/EVALUATION.md](eval/EVALUATION.md)äº†è§£æ›´å¤šä¿¡æ¯ã€‚
<br>

## FAQ

å¦‚é‡åˆ°é—®é¢˜ï¼Œæ•¬è¯·æŸ¥é˜…[FAQ](FAQ_zh.md)ä»¥åŠissueåŒºï¼Œå¦‚ä»æ— æ³•è§£å†³å†æäº¤issueã€‚
<br>

## ä½¿ç”¨åè®®

ç ”ç©¶äººå‘˜ä¸å¼€å‘è€…å¯ä½¿ç”¨Qwen-7Bå’ŒQwen-7B-Chatæˆ–è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€‚æˆ‘ä»¬åŒæ ·å…è®¸å•†ä¸šä½¿ç”¨ï¼Œå…·ä½“ç»†èŠ‚è¯·æŸ¥çœ‹[LICENSE](LICENSE)ã€‚å¦‚éœ€å•†ç”¨ï¼Œè¯·å¡«å†™[é—®å·](https://dashscope.console.aliyun.com/openModelApply/qianwen)ç”³è¯·ã€‚
<br>

## è”ç³»æˆ‘ä»¬

å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œè¯·é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚

