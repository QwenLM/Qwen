<br>
<p align="center">
    <img src="assets/logo.jpg" width="400"/>
<p>
<br>

<p align="center">
        Qwen-7B <a href="https://modelscope.cn/models/qwen/Qwen-7B/summary">ğŸ¤– <a> | <a href="https://huggingface.co/Qwen/Qwen-7B">ğŸ¤—</a>&nbsp ï½œ Qwen-7B-Chat <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary">ğŸ¤– <a>| <a href="https://huggingface.co/Qwen/Qwen-7B-Chat">ğŸ¤—</a>&nbsp ï½œ &nbsp<a href="https://modelscope.cn/studios/qwen/Qwen-7B-Chat-Demo/summary">Demo</a>&nbsp ï½œ &nbsp<a href="https://github.com/QwenLM/Qwen-7B/blob/main/tech_memo.md">Report</a>
</p>
<br>

<p align="center">
        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="READM.md">English</a>
</p>
<br><br>

æˆ‘ä»¬åœ¨ğŸ¤– **ModelScope**ä»¥åŠğŸ¤— **Hugging Face**å‡å¼€æºäº†**Qwen-7B**ç³»åˆ—æ¨¡å‹ã€‚è¯·åœ¨æœ¬æ–‡æ¡£é¡¶éƒ¨ç‚¹å‡»ç›¸å…³é“¾æ¥æŸ¥çœ‹ä»“åº“ä¿¡æ¯ã€‚æœ¬ä»“åº“ä¸»è¦åŒ…æ‹¬Qwen-7Bçš„ç®€ä»‹ã€ä½¿ç”¨æŒ‡å—ã€æŠ€æœ¯å¤‡å¿˜ç­‰å†…å®¹ã€‚æƒ³äº†è§£æ›´å¤šå…³äºæ¨¡å‹çš„ä¿¡æ¯ï¼Œè¯·ç‚¹å‡»[é“¾æ¥](tech_memo.md)æŸ¥çœ‹æˆ‘ä»¬çš„æŠ€æœ¯å¤‡å¿˜å½•ã€‚

é€šä¹‰åƒé—®-7Bï¼ˆQwen-7Bï¼‰ æ˜¯é˜¿é‡Œäº‘ç ”å‘çš„é€šä¹‰åƒé—®å¤§æ¨¡å‹ç³»åˆ—çš„70äº¿å‚æ•°è§„æ¨¡çš„æ¨¡å‹ã€‚Qwen-7Bæ˜¯åŸºäºTransformerçš„å¤§è¯­è¨€æ¨¡å‹, åœ¨è¶…å¤§è§„æ¨¡çš„é¢„è®­ç»ƒæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒå¾—åˆ°ã€‚é¢„è®­ç»ƒæ•°æ®ç±»å‹å¤šæ ·ï¼Œè¦†ç›–å¹¿æ³›ï¼ŒåŒ…æ‹¬å¤§é‡ç½‘ç»œæ–‡æœ¬ã€ä¸“ä¸šä¹¦ç±ã€ä»£ç ç­‰ã€‚åŒæ—¶ï¼Œåœ¨Qwen-7Bçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨å¯¹é½æœºåˆ¶æ‰“é€ äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„AIåŠ©æ‰‹Qwen-7B-Chatã€‚Qwen-7Bç³»åˆ—æ¨¡å‹çš„ç‰¹ç‚¹åŒ…æ‹¬ï¼š

1. **å¤§è§„æ¨¡é«˜è´¨é‡é¢„è®­ç»ƒæ•°æ®**ï¼šæˆ‘ä»¬ä½¿ç”¨äº†è¶…è¿‡2.2ä¸‡äº¿tokençš„è‡ªå»ºå¤§è§„æ¨¡é¢„è®­ç»ƒæ•°æ®é›†è¿›è¡Œè¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒã€‚æ•°æ®é›†åŒ…æ‹¬æ–‡æœ¬å’Œä»£ç ç­‰å¤šç§æ•°æ®ç±»å‹ï¼Œè¦†ç›–é€šç”¨é¢†åŸŸå’Œä¸“ä¸šé¢†åŸŸã€‚
2. **ä¼˜ç§€çš„æ¨¡å‹æ€§èƒ½**ï¼šç›¸æ¯”åŒè§„æ¨¡çš„å¼€æºæ¨¡å‹ï¼ŒQwen-7Båœ¨å¤šä¸ªè¯„æµ‹æ•°æ®é›†ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œç”šè‡³è¶…å‡º12-13Bç­‰æ›´å¤§è§„æ¨¡çš„æ¨¡å‹ã€‚è¯„æµ‹è¯„ä¼°çš„èƒ½åŠ›èŒƒå›´åŒ…æ‹¬è‡ªç„¶è¯­è¨€ç†è§£ä¸ç”Ÿæˆã€æ•°å­¦è¿ç®—è§£é¢˜ã€ä»£ç ç”Ÿæˆç­‰ã€‚
3. **æ›´å¥½åœ°æ”¯æŒå¤šè¯­è¨€**ï¼šåŸºäºæ›´å¤§è¯è¡¨çš„åˆ†è¯å™¨åœ¨åˆ†è¯ä¸Šæ›´é«˜æ•ˆï¼ŒåŒæ—¶å®ƒå¯¹å…¶ä»–è¯­è¨€è¡¨ç°æ›´åŠ å‹å¥½ã€‚ç”¨æˆ·å¯ä»¥åœ¨Qwen-7Bçš„åŸºç¡€ä¸Šæ›´æ–¹ä¾¿åœ°è®­ç»ƒç‰¹å®šè¯­è¨€çš„7Bè¯­è¨€æ¨¡å‹ã€‚
4. **8Kçš„ä¸Šä¸‹æ–‡é•¿åº¦**ï¼šQwen-7BåŠQwen-7B-Chatå‡èƒ½æ”¯æŒ8Kçš„ä¸Šä¸‹æ–‡é•¿åº¦, å…è®¸ç”¨æˆ·è¾“å…¥æ›´é•¿çš„promptã€‚
5. **æ”¯æŒæ’ä»¶è°ƒç”¨**ï¼šQwen-7B-Chaté’ˆå¯¹æ’ä»¶è°ƒç”¨ç›¸å…³çš„å¯¹é½æ•°æ®åšäº†ç‰¹å®šä¼˜åŒ–ï¼Œå½“å‰æ¨¡å‹èƒ½æœ‰æ•ˆè°ƒç”¨æ’ä»¶ä»¥åŠå‡çº§ä¸ºAgentã€‚

## æ–°é—»

* 2023å¹´8æœˆ3æ—¥ åœ¨é­”æ­ç¤¾åŒºï¼ˆModelScopeï¼‰å’ŒHugging FaceåŒæ­¥æ¨å‡ºQwen-7Bå’ŒQwen-7B-Chatæ¨¡å‹ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å‘å¸ƒäº†æŠ€æœ¯å¤‡å¿˜å½•ï¼Œä»‹ç»äº†ç›¸å…³çš„è®­ç»ƒç»†èŠ‚å’Œæ¨¡å‹è¡¨ç°ã€‚

## è¯„æµ‹è¡¨ç°

Qwen-7Båœ¨å¤šä¸ªå…¨é¢è¯„ä¼°è‡ªç„¶è¯­è¨€ç†è§£ä¸ç”Ÿæˆã€æ•°å­¦è¿ç®—è§£é¢˜ã€ä»£ç ç”Ÿæˆç­‰èƒ½åŠ›çš„è¯„æµ‹æ•°æ®é›†ä¸Šï¼ŒåŒ…æ‹¬MMLUã€C-Evalã€GSM8Kã€HumanEvalã€WMT22ç­‰ï¼Œå‡è¶…å‡ºäº†åŒè§„æ¨¡å¤§è¯­è¨€æ¨¡å‹çš„è¡¨ç°ï¼Œç”šè‡³è¶…å‡ºäº†å¦‚12-13Bå‚æ•°ç­‰æ›´å¤§è§„æ¨¡çš„è¯­è¨€æ¨¡å‹ã€‚

| Model        | MMLU     |   C-Eval |    GSM8K | HumanEval | WMT22 (en-zh) |
| :------------- | ---------- | ---------: | ---------: | ----------: | --------------: |
| LLaMA-7B     | 35.1     |        - |     11.0 |      10.5 |           8.7 |
| LLaMA 2-7B   | 45.3     |        - |     14.6 |      12.8 |          17.9 |
| Baichuan-7B  | 42.3     |     42.8 |      9.7 |       9.2 |          26.6 |
| ChatGLM2-6B  | 47.9     |     51.7 |     32.4 |       9.2 |             - |
| InternLM-7B  | 51.0     |     52.8 |     31.2 |      10.4 |          14.8 |
| Baichuan-13B | 51.6     |     53.6 |     26.6 |      12.8 |          30.0 |
| LLaMA-13B    | 46.9     |     35.5 |     17.8 |      15.8 |          12.0 |
| LLaMA 2-13B  | 54.8     |        - |     28.7 |      18.3 |          24.2 |
| ChatGLM2-12B | 56.2     | **61.6** |     40.9 |         - |             - |
| **Qwen-7B**  | **56.7** |     59.6 | **51.6** |  **24.4** |      **30.6** |

<p align="center">
    <img src="assets/performance.png" width="1000"/>
<p>
<br>

æ›´å¤šçš„å®éªŒç»“æœå’Œç»†èŠ‚è¯·æŸ¥çœ‹æˆ‘ä»¬çš„æŠ€æœ¯å¤‡å¿˜å½•ã€‚ç‚¹å‡»[è¿™é‡Œ](techmemo-draft.md)ã€‚

## å¿«é€Ÿä½¿ç”¨

æˆ‘ä»¬æä¾›ç®€å•çš„ç¤ºä¾‹æ¥è¯´æ˜å¦‚ä½•åˆ©ç”¨ğŸ¤– ModelScopeå’ŒğŸ¤— Transformerså¿«é€Ÿä½¿ç”¨Qwen-7Bå’ŒQwen-7B-Chatã€‚

åœ¨å¼€å§‹å‰ï¼Œè¯·ç¡®ä¿ä½ å·²ç»é…ç½®å¥½ç¯å¢ƒå¹¶å®‰è£…å¥½ç›¸å…³çš„ä»£ç åŒ…ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œç¡®ä¿ä½ çš„pytorchç‰ˆæœ¬é«˜äº`1.12`ï¼Œç„¶åå®‰è£…ç›¸å…³çš„ä¾èµ–åº“ã€‚

```bash
pip install transformers==4.31.0 accelerate tiktoken einops
```

æˆ‘ä»¬è¿˜æ¨èå®‰è£…[flash-attention](https://github.com/Dao-AILab/flash-attention)æ¥æé«˜ä½ çš„è¿è¡Œæ•ˆç‡ä»¥åŠé™ä½æ˜¾å­˜å ç”¨ã€‚

```bash
git clone -b v1.0.8 https://github.com/Dao-AILab/flash-attention
cd flash-attention && pip install .
pip install csrc/layer_norm
pip install csrc/rotary
```

æ¥ä¸‹æ¥ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨Transformersæˆ–è€…ModelScopeæ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚

#### ğŸ¤— Transformers

å¦‚å¸Œæœ›ä½¿ç”¨Qwen-7B-chatè¿›è¡Œæ¨ç†ï¼Œæ‰€éœ€è¦å†™çš„åªæ˜¯å¦‚ä¸‹æ‰€ç¤ºçš„æ•°è¡Œä»£ç ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)
## ä½¿ç”¨bf16ç²¾åº¦
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
## ä½¿ç”¨fp16ç²¾åº¦
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# é»˜è®¤ä½¿ç”¨fp32ç²¾åº¦
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True).eval()
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚

# ç¬¬ä¸€è½®å¯¹è¯ 1st dialogue turn
response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚

# ç¬¬äºŒè½®å¯¹è¯ 2nd dialogue turn
response, history = model.chat(tokenizer, "ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚", history=history) 
print(response)
# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚
# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚
# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚
# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚
# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚
# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚

# ç¬¬ä¸‰è½®å¯¹è¯ 3rd dialogue turn
response, history = model.chat(tokenizer, "ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜", history=history)
print(response)
# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹
```

è¿è¡ŒQwen-7BåŒæ ·éå¸¸ç®€å•ã€‚
<details>
  <summary>è¿è¡ŒQwen-7B</summary>
    
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)
## ä½¿ç”¨bf16ç²¾åº¦
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, bf16=True).eval()
## ä½¿ç”¨fp16ç²¾åº¦
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, fp16=True).eval()
# é»˜è®¤ä½¿ç”¨fp32ç²¾åº¦
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True).eval()
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚

inputs = tokenizer('è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯', return_tensors='pt')
inputs = inputs.to('cuda:0')
pred = model.generate(**inputs)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))
# è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯äºšçš„æ–¯äºšè´å·´ï¼ˆAddis Ababaï¼‰...
```
</details>

#### ğŸ¤– ModelScope

é­”æ­ï¼ˆModelScopeï¼‰æ˜¯å¼€æºçš„æ¨¡å‹å³æœåŠ¡å…±äº«å¹³å°ï¼Œä¸ºæ³›AIå¼€å‘è€…æä¾›çµæ´»ã€æ˜“ç”¨ã€ä½æˆæœ¬çš„ä¸€ç«™å¼æ¨¡å‹æœåŠ¡äº§å“ã€‚ä½¿ç”¨ModelScopeåŒæ ·éå¸¸ç®€å•ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
import os
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks
from modelscope import snapshot_download

model_id = 'QWen/qwen-7b-chat'
revision = 'v1.0.0'

model_dir = snapshot_download(model_id, revision)

pipe = pipeline(
task=Tasks.chat, model=model_dir, device_map='auto')
history = None

text = 'æµ™æ±Ÿçš„çœä¼šåœ¨å“ªé‡Œï¼Ÿ'
results = pipe(text, history=history)
response, history = results['response'], results['history']
print(f'Response: {response}')
text = 'å®ƒæœ‰ä»€ä¹ˆå¥½ç©çš„åœ°æ–¹å‘¢ï¼Ÿ'
results = pipe(text, history=history)
response, history = results['response'], results['history']
print(f'Response: {response}')
```

## é‡åŒ–

å¦‚å¸Œæœ›ä½¿ç”¨æ›´ä½ç²¾åº¦çš„é‡åŒ–æ¨¡å‹ï¼Œå¦‚4æ¯”ç‰¹å’Œ8æ¯”ç‰¹çš„æ¨¡å‹ï¼Œæˆ‘ä»¬æä¾›äº†ç®€å•çš„ç¤ºä¾‹æ¥è¯´æ˜å¦‚ä½•å¿«é€Ÿä½¿ç”¨é‡åŒ–æ¨¡å‹ã€‚åœ¨å¼€å§‹å‰ï¼Œç¡®ä¿ä½ å·²ç»å®‰è£…äº†`bitsandbytes`ã€‚

```bash
pip install bitsandbytes
```

ä½ åªéœ€è¦åœ¨`AutoModelForCausalLM.from_pretrained`ä¸­æ·»åŠ ä½ çš„é‡åŒ–é…ç½®ï¼Œå³å¯ä½¿ç”¨é‡åŒ–æ¨¡å‹ã€‚å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from transformers import BitsAndBytesConfig

# quantization configuration for NF4 (4 bits)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_compute_dtype=torch.bfloat16
)

# quantization configuration for Int8 (8 bits)
quantization_config = BitsAndBytesConfig(load_in_8bit=True)

model = AutoModelForCausalLM.from_pretrained(
    args.checkpoint_path,
    device_map="cuda:0",
    quantization_config=quantization_config,
    max_memory=max_memory,
    trust_remote_code=True,
).eval()
```

ä¸Šè¿°æ–¹æ³•å¯ä»¥è®©æˆ‘ä»¬å°†æ¨¡å‹é‡åŒ–æˆ`NF4`å’Œ`Int8`ç²¾åº¦çš„æ¨¡å‹è¿›è¡Œè¯»å–ï¼Œå¸®åŠ©æˆ‘ä»¬èŠ‚çœæ˜¾å­˜å¼€é”€ã€‚æˆ‘ä»¬ä¹Ÿæä¾›äº†ç›¸å…³æ€§èƒ½æ•°æ®ã€‚æˆ‘ä»¬å‘ç°å°½ç®¡æ¨¡å‹åœ¨æ•ˆæœä¸Šå­˜åœ¨æŸå¤±ï¼Œä½†æ¨¡å‹çš„æ˜¾å­˜å¼€é”€å¤§å¹…é™ä½ã€‚

| Precision | MMLU | Memory |
| :---------: | -------: | -----: |
|   BF16   |  56.7 |   16.2G |
|   Int8   |  52.8 |   10.1G |
|    NF4    |  48.9 |    7.4G |

## å·¥å…·è°ƒç”¨

Qwen-7B-Chaté’ˆå¯¹åŒ…æ‹¬APIã€æ•°æ®åº“ã€æ¨¡å‹ç­‰å·¥å…·åœ¨å†…çš„è°ƒç”¨è¿›è¡Œäº†ä¼˜åŒ–ã€‚ç”¨æˆ·å¯ä»¥å¼€å‘åŸºäºQwen-7Bçš„LangChainã€Agentç”šè‡³Code Interpreterã€‚æˆ‘ä»¬åœ¨å†…éƒ¨çš„å³å°†å¼€æºçš„è¯„æµ‹æ•°æ®é›†ä¸Šæµ‹è¯•æ¨¡å‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œå¹¶å‘ç°Qwen-7B-Chatèƒ½å¤Ÿå–å¾—ç¨³å®šçš„è¡¨ç°ã€‚

| Model       | Tool Selection (Acc.â†‘) | Tool Input (Rouge-Lâ†‘) | False Positive Errorâ†“ |
| ------------- | ------------------------- | ------------------------ | ------------------------ |
| GPT-4       | 95%                     | **0.90**               | 15%                    |
| GPT-3.5     | 85%                     | 0.88                   | 75%                    |
| **Qwen-7B** | **99%**                 | 0.89                   | **8.5%**               |

æˆ‘ä»¬æä¾›äº†æ–‡æ¡£è¯´æ˜å¦‚ä½•æ ¹æ®ReAct Promptingçš„åŸåˆ™å†™ä½œä½ çš„promptã€‚

For how to write and use prompts for ReAct Prompting, please refer to [the ReAct examples](examples/react_prompt.md)ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†å®éªŒç»“æœè¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹æ‰®æ¼”Agentçš„èƒ½åŠ›ã€‚è¯·é˜…è¯»ç›¸å…³æ–‡æ¡£[é“¾æ¥](https://huggingface.co/docs/transformers/transformers_agents)äº†è§£æ›´å¤šä¿¡æ¯ã€‚æ¨¡å‹åœ¨Hugging Faceæä¾›çš„è¯„æµ‹æ•°æ®é›†ä¸Šè¡¨ç°å¦‚ä¸‹ï¼š

| Model           | Tool Selectionâ†‘ | Tool Usedâ†‘ | Codeâ†‘    |
| ----------------- | ------------------ | ------------- | ----------- |
| GPT-4           | **100**          | **100**     | **97.41** |
| GPT-3.5         | 95.37            | 96.30       | 87.04     |
| StarCoder-15.5B | 87.04            | 87.96       | 68.89     |
| **Qwen-7B**     | 90.74            | 92.59       | 74.07     |

## é•¿æ–‡æœ¬ç†è§£

æˆ‘ä»¬å¼•å…¥äº†NTKæ’å€¼ã€çª—å£æ³¨æ„åŠ›ã€LogNæ³¨æ„åŠ›ç¼©æ”¾ç­‰æŠ€æœ¯æ¥æå‡æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦å¹¶çªç ´è®­ç»ƒåºåˆ—é•¿åº¦çš„é™åˆ¶ã€‚æˆ‘ä»¬çš„æ¨¡å‹å·²ç»çªç ´8Kçš„åºåˆ—é•¿åº¦ã€‚é€šè¿‡arXivæ•°æ®é›†ä¸Šçš„è¯­è¨€æ¨¡å‹å®éªŒï¼Œæˆ‘ä»¬å‘ç°Qwen-7Bèƒ½å¤Ÿåœ¨é•¿åºåˆ—çš„è®¾ç½®ä¸‹å–å¾—ä¸é”™çš„è¡¨ç°ã€‚

<table>
    <tr>
        <th rowspan="2">Model</th><th colspan="5" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th><th align="center">16384</th>
    </tr>
    <tr>
        <td>Qwen-7B</td><td align="center"><b>4.23</b></td><td align="center"><b>3.78</b></td><td align="center">39.35</td><td align="center">469.81</td><td align="center">2645.09</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk</td><td align="center"><b>4.23</b></td><td align="center"><b>3.78</b></td><td align="center">3.59</td><td align="center">3.66</td><td align="center">5.71</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn</td><td align="center"><b>4.23</b></td><td align="center"><b>3.78</b></td><td align="center"><b>3.58</b></td><td align="center">3.56</td><td align="center">4.62</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + local_attn</td><td align="center"><b>4.23</b></td><td align="center"><b>3.78</b></td><td align="center"><b>3.58</b></td><td align="center"><b>3.49</b></td><td align="center"><b>4.32</b></td>
    </tr>
</table>

## å¤ç°

æˆ‘ä»¬æä¾›äº†è¯„æµ‹è„šæœ¬ä»¥ä¾›å¤ç°æˆ‘ä»¬çš„å®éªŒç»“æœã€‚æ³¨æ„ï¼Œç”±äºå†…éƒ¨ä»£ç å’Œå¼€æºä»£ç å­˜åœ¨å°‘è®¸å·®å¼‚ï¼Œè¯„æµ‹ç»“æœå¯èƒ½ä¸æ±‡æŠ¥ç»“æœå­˜åœ¨ç»†å¾®çš„ç»“æœä¸ä¸€è‡´ã€‚è¯·é˜…è¯»[eval/EVALUATION.md](eval/EVALUATION.md)äº†è§£æ›´å¤šä¿¡æ¯ã€‚

## ä½¿ç”¨åè®®

ç ”ç©¶äººå‘˜ä¸å¼€å‘è€…å¯ä½¿ç”¨Qwen-7Bå’ŒQwen-7B-Chatæˆ–è¿›è¡ŒäºŒæ¬¡å¼€å‘ã€‚æˆ‘ä»¬åŒæ ·å…è®¸å•†ä¸šä½¿ç”¨ï¼Œå…·ä½“ç»†èŠ‚è¯·æŸ¥çœ‹[LICENSE](LICENSE)ã€‚

## è”ç³»æˆ‘ä»¬

å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å›¢é˜Ÿå’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œè¯·é€šè¿‡é‚®ä»¶ï¼ˆqianwen_opensource@alibabacloud.comï¼‰è”ç³»æˆ‘ä»¬ã€‚

