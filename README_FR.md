<p align="left">
    <a href="README_CN.md">‰∏≠Êñá</a>&nbsp ÔΩú &nbsp<a href="README.md">English</a>&nbsp ÔΩú &nbsp<a href="README_JA.md">Êó•Êú¨Ë™û</a>&nbsp ÔΩú &nbspFran√ßais
</p>
<br><br>

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg" width="400"/>
<p>
<br>

<p align="center">
    ü§ó <a href="https://huggingface.co/Qwen">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href="https://modelscope.cn/organization/qwen">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp üìë <a href="https://arxiv.org/abs/2309.16609">Paper</a> &nbsp&nbsp ÔΩú &nbsp&nbspüñ•Ô∏è <a href="https://modelscope.cn/studios/qwen/Qwen-14B-Chat-Demo/summary">Demo</a>
<br>
<a href="assets/wechat.png">WeChat (ÂæÆ‰ø°)</a>&nbsp&nbsp ÔΩú &nbsp&nbsp DingTalk (ÈíâÈíâ) &nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp&nbsp
</p>
<br><br>

|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                        Qwen-Chat (Int8)                         |                                                            Qwen                                                            |
|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|
| 7B  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary">ü§ñ</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat">ü§ó</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary">ü§ñ</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int4">ü§ó</a>  | <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary">ü§ñ</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int8">ü§ó</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B/summary">ü§ñ</a>  <a href="https://huggingface.co/Qwen/Qwen-7B">ü§ó</a>  |
| 14B | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary">ü§ñ</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat">ü§ó</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary">ü§ñ</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int4">ü§ó</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary">ü§ñ</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int8">ü§ó</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B/summary">ü§ñ</a>  <a href="https://huggingface.co/Qwen/Qwen-14B">ü§ó</a> |



Nous ouvrons notre s√©rie **Qwen**, qui comprend d√©sormais **Qwen**, les mod√®les de langue de base, √† savoir **Qwen-7B** et **Qwen-14B**, ainsi que **Qwen-Chat**, les mod√®les de chat, √† savoir **Qwen-7B-Chat** et **Qwen-14B-Chat**. Les liens se trouvent dans le tableau ci-dessus. Cliquez dessus et consultez les fiches des mod√®les. Nous publions √©galement le **[rapport technique](https://arxiv.org/abs/2309.16609)**. Cliquez sur le lien du document et consultez-le !

En bref, nous disposons de mod√®les linguistiques solides, qui ont √©t√© pr√©-entra√Æn√© de mani√®re stable pour 3 000 milliards de tokens de donn√©es multilingues avec une large couverture de domaines, de langues (en particulier le chinois et l'anglais), etc. Ils sont capables d'atteindre des performances comp√©titives sur des ensembles de donn√©es de r√©f√©rence. En outre, nous disposons de mod√®les de chat align√©s sur les pr√©f√©rences humaines bas√©es sur SFT et RLHF (pas encore publi√©s), qui sont capables de chatter, de cr√©er du contenu, d'extraire des informations, de r√©sumer, de traduire, de coder, de r√©soudre des probl√®mes math√©matiques, etc. et d'utiliser des outils, de jouer le r√¥le d'agents ou m√™me code interpreter, etc.

Dans la repo, vous pouvez trouver:

* Comment utiliser Qwen, et profiter de l'inf√©rence simple.
* D√©tails sur les mod√®les de quantization, y compris GPTQ et la quantization de KV cache.
* Statistiques sur les performances de l'inf√©rence, y compris la vitesse et la m√©moire.
* Tutoriels sur le finetuning, y compris le finetuning de param√®tres complets, LoRA, et Q-LoRA.
* Instructions de d√©ploiement, avec l'exemple de vLLM et FastChat.
* Instructions sur la cr√©ation de d√©mos, y compris WebUI, d√©mo CLI, etc.
* Introduction au service API de DashScope, ainsi que les instructions pour construire une API de type OpenAI pour votre mod√®le.
* Informations sur Qwen pour l'utilisation d'outils, d'agents et code interpreter.
* Statistiques de l'√©valuation de la compr√©hension du contexte long.
* Contrat de licence.
* ...

En outre, si vous rencontrez des probl√®mes, consultez d'abord la [FAQ](FAQ.md) pour obtenir de l'aide. Vous vous sentez toujours en difficult√© ? N'h√©sitez pas √† nous envoyer des questions (de pr√©f√©rence en anglais pour que plus de gens puissent vous comprendre) ! Si vous souhaitez nous aider, envoyez-nous des demandes d'extension sans h√©sitation ! Nous sommes toujours enthousiastes √† propos des relations publiques ! 

Vous voulez discuter avec nous ou prendre un caf√© avec nous ? Bienvenue sur notre Discord ou WeChat !
<br><br>

## Nouvelles et mises √† jour

* 2023.10.17 Nous publions le mod√®le quantifi√© Int8 **Qwen-7B-Chat-Int8** et **Qwen-14B-Chat-Int8**.
* 2023.9.25 üî• Nous publions **Qwen-14B** et **Qwen-14B-Chat** sur ModelScope et Hugging Face, ainsi que [qwen.cpp](https://github.com/QwenLM/qwen.cpp) et [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent). Les codes et les poids de **Qwen-7B** et **Qwen-7B-Chat** ont √©galement √©t√© mis √† jour. **S'IL VOUS PLA√éT, TIREZ LA DERNI√àRE VERSION!**
    - Par rapport √† **Qwen-7B** (original), **Qwen-7B** utilise davantage de jetons d'entra√Ænement, passant de 2,2 √† 2,4T de jetons, tandis que la longueur du contexte passe de 2048 √† 8192. La connaissance du chinois et la capacit√© de codage de **Qwen-7B** ont √©t√© encore am√©lior√©es.
* 2023.9.12 Nous prenons d√©sormais en charge le finetuning sur les mod√®les Qwen-7B, y compris le finetuning de tous les param√®tres, LoRA et Q-LoRA.
* 2023.8.21 Nous publions le mod√®le quantifi√© Int4 pour Qwen-7B-Chat, **Qwen-7B-Chat-Int4**, qui n√©cessite de faibles co√ªts de m√©moire mais permet d'am√©liorer la vitesse d'inf√©rence. En outre, il n'y a pas de d√©gradation significative des performances lors de l'√©valuation de r√©f√©rence.
* 2023.8.3 Nous publions **Qwen-7B** et **Qwen-7B-Chat** sur ModelScope et Hugging Face. Nous fournissons √©galement un m√©mo technique pour plus de d√©tails sur le mod√®le, y compris les d√©tails de l'entra√Ænement et les performances du mod√®le.
<br>

## Performance

Qwen-14B et Qwen-7B (il s'agit de la nouvelle version entra√Æn√©e avec davantage de tokens et la longueur du contexte est pass√©e de 2048 √† 8192) surpassent les mod√®les de r√©f√©rence de tailles similaires sur une s√©rie d'ensembles de donn√©es de r√©f√©rence, par exemple MMLU, C-Eval, GSM8K, MATH, HumanEval, MBPP, BBH, etc., qui √©valuent les capacit√©s des mod√®les en mati√®re de compr√©hension du langage naturel, de r√©solution de probl√®mes math√©matiques, de codage, etc. Cependant, m√™me Qwen-14B reste nettement inf√©rieur √† GPT-3.5, sans parler de GPT-4. Voir les r√©sultats ci-dessous.

<p align="left">
    <img src="assets/radar_14b.jpg" width="600"/>
<p>
<br>

| Model              |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP   |   BBH    |  CMMLU   |
|:-------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:|
|                    |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot  |  3-shot  |  5-shot  |
| LLaMA2-7B          |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8   |   38.2   |   31.8   |
| LLaMA2-13B         |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3   |   45.6   |   38.4   |
| LLaMA2-34B         |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0   |   44.1   |    -     |
| ChatGLM2-6B        |   47.9   |   51.7   |   32.4   |   6.5    |     -     |    -     |   33.7   |    -     |
| InternLM-7B        |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0   |   37.0   |   51.8   |
| InternLM-20B       |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6   |   52.5   |   59.0   |
| Baichuan2-7B       |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2   |   41.6   |   57.1   |
| Baichuan2-13B      |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2   |   49.0   |   62.0   |
| Qwen-7B (original) |   56.7   |   59.6   |   51.6   |   10.4   |   24.4    |   31.2   |   40.6   |   58.8   |
| **Qwen-7B**        |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6   |   45.0   |   62.2   |
| **Qwen-14B**       | **66.3** | **72.1** | **61.3** | **24.8** | **32.3**  | **40.8** | **53.4** | **71.0** |

Pour tous les mod√®les compar√©s, nous indiquons les meilleurs scores entre leurs r√©sultats officiels et [OpenCompass] (https://opencompass.org.cn/leaderboard-llm). 

Pour plus de r√©sultats exp√©rimentaux (performances d√©taill√©es des mod√®les sur d'autres ensembles de donn√©es de r√©f√©rence) et de d√©tails, veuillez vous r√©f√©rer √† notre rapport technique en cliquant [ici](https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf).
<br><br>

## Besoins

* python 3.8 et plus
* pytorch 1.12 et plus, 2.0 et plus sont recommand√©s
* transformers 4.32 et plus
* CUDA 11.4 et plus sont recommand√©s (pour les utilisateurs de GPU, les utilisateurs de flash, etc.)
<br>

## D√©marrage Rapide

Ci-dessous, nous fournissons des exemples simples pour montrer comment utiliser Qwen-Chat avec ü§ñ ModelScope et ü§ó Transformers.

Avant d'ex√©cuter le code, assurez-vous d'avoir configur√© l'environnement et install√© les paquets requis. Assurez-vous que vous r√©pondez aux exigences ci-dessus, puis installez les biblioth√®ques d√©pendantes.

```bash
pip install -r requirements.txt
```

Si votre appareil supporte fp16 ou bf16, nous vous recommandons d'installer [flash-attention](https://github.com/Dao-AILab/flash-attention) (**nous supportons flash-attention 2 maintenant.**) pour une meilleure efficacit√© et une moindre utilisation de la m√©moire. (**flash-attention est optionnel et le projet peut fonctionner normalement sans l'installer**)

```bash
git clone https://github.com/Dao-AILab/flash-attention
cd flash-attention && pip install .
# Below are optional. Installing them might be slow.
# pip install csrc/layer_norm
# pip install csrc/rotary
```

Vous pouvez maintenant commencer avec ModelScope ou Transformers.

### ü§ó Transformers

Pour utiliser Qwen-Chat pour l'inf√©rence, il vous suffit de saisir quelques lignes de code, comme indiqu√© ci-dessous. N'oubliez pas de transmettre les noms de mod√®les ou les chemins corrects, tels que "Qwen/Qwen-7B-Chat" et "Qwen/Qwen-14B-Chat". Cependant, **veuillez vous assurer que vous utilisez le code le plus r√©cent**.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# Model names: "Qwen/Qwen-7B-Chat", "Qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
# use auto mode, automatically select precision based on the device.
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat",
    device_map="auto",
    trust_remote_code=True
).eval()

# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# 1st dialogue turn
response, history = model.chat(tokenizer, "‰Ω†Â•Ω", history=None)
print(response)
# ‰Ω†Â•ΩÔºÅÂæàÈ´òÂÖ¥‰∏∫‰Ω†Êèê‰æõÂ∏ÆÂä©„ÄÇ

# 2nd dialogue turn
response, history = model.chat(tokenizer, "ÁªôÊàëËÆ≤‰∏Ä‰∏™Âπ¥ËΩª‰∫∫Â•ãÊñóÂàõ‰∏öÊúÄÁªàÂèñÂæóÊàêÂäüÁöÑÊïÖ‰∫ã„ÄÇ", history=history)
print(response)
# ËøôÊòØ‰∏Ä‰∏™ÂÖ≥‰∫é‰∏Ä‰∏™Âπ¥ËΩª‰∫∫Â•ãÊñóÂàõ‰∏öÊúÄÁªàÂèñÂæóÊàêÂäüÁöÑÊïÖ‰∫ã„ÄÇ
# ÊïÖ‰∫ãÁöÑ‰∏ª‰∫∫ÂÖ¨Âè´ÊùéÊòéÔºå‰ªñÊù•Ëá™‰∏Ä‰∏™ÊôÆÈÄöÁöÑÂÆ∂Â∫≠ÔºåÁà∂ÊØçÈÉΩÊòØÊôÆÈÄöÁöÑÂ∑•‰∫∫„ÄÇ‰ªéÂ∞èÔºåÊùéÊòéÂ∞±Á´ã‰∏ã‰∫Ü‰∏Ä‰∏™ÁõÆÊ†áÔºöË¶ÅÊàê‰∏∫‰∏ÄÂêçÊàêÂäüÁöÑ‰ºÅ‰∏öÂÆ∂„ÄÇ
# ‰∏∫‰∫ÜÂÆûÁé∞Ëøô‰∏™ÁõÆÊ†áÔºåÊùéÊòéÂã§Â•ãÂ≠¶‰π†ÔºåËÄÉ‰∏ä‰∫ÜÂ§ßÂ≠¶„ÄÇÂú®Â§ßÂ≠¶ÊúüÈó¥Ôºå‰ªñÁßØÊûÅÂèÇÂä†ÂêÑÁßçÂàõ‰∏öÊØîËµõÔºåËé∑Âæó‰∫Ü‰∏çÂ∞ëÂ•ñÈ°π„ÄÇ‰ªñËøòÂà©Áî®ËØæ‰ΩôÊó∂Èó¥ÂéªÂÆû‰π†ÔºåÁßØÁ¥Ø‰∫ÜÂÆùË¥µÁöÑÁªèÈ™å„ÄÇ
# ÊØï‰∏öÂêéÔºåÊùéÊòéÂÜ≥ÂÆöÂºÄÂßãËá™Â∑±ÁöÑÂàõ‰∏ö‰πãË∑Ø„ÄÇ‰ªñÂºÄÂßãÂØªÊâæÊäïËµÑÊú∫‰ºöÔºå‰ΩÜÂ§öÊ¨°ÈÉΩË¢´ÊãíÁªù‰∫Ü„ÄÇÁÑ∂ËÄåÔºå‰ªñÂπ∂Ê≤°ÊúâÊîæÂºÉ„ÄÇ‰ªñÁªßÁª≠Âä™ÂäõÔºå‰∏çÊñ≠ÊîπËøõËá™Â∑±ÁöÑÂàõ‰∏öËÆ°ÂàíÔºåÂπ∂ÂØªÊâæÊñ∞ÁöÑÊäïËµÑÊú∫‰ºö„ÄÇ
# ÊúÄÁªàÔºåÊùéÊòéÊàêÂäüÂú∞Ëé∑Âæó‰∫Ü‰∏ÄÁ¨îÊäïËµÑÔºåÂºÄÂßã‰∫ÜËá™Â∑±ÁöÑÂàõ‰∏ö‰πãË∑Ø„ÄÇ‰ªñÊàêÁ´ã‰∫Ü‰∏ÄÂÆ∂ÁßëÊäÄÂÖ¨Âè∏Ôºå‰∏ìÊ≥®‰∫éÂºÄÂèëÊñ∞ÂûãËΩØ‰ª∂„ÄÇÂú®‰ªñÁöÑÈ¢ÜÂØº‰∏ãÔºåÂÖ¨Âè∏ËøÖÈÄüÂèëÂ±ïËµ∑Êù•ÔºåÊàê‰∏∫‰∫Ü‰∏ÄÂÆ∂ÊàêÂäüÁöÑÁßëÊäÄ‰ºÅ‰∏ö„ÄÇ
# ÊùéÊòéÁöÑÊàêÂäüÂπ∂‰∏çÊòØÂÅ∂ÁÑ∂ÁöÑ„ÄÇ‰ªñÂã§Â•ã„ÄÅÂùöÈüß„ÄÅÂãá‰∫éÂÜíÈô©Ôºå‰∏çÊñ≠Â≠¶‰π†ÂíåÊîπËøõËá™Â∑±„ÄÇ‰ªñÁöÑÊàêÂäü‰πüËØÅÊòé‰∫ÜÔºåÂè™Ë¶ÅÂä™ÂäõÂ•ãÊñóÔºå‰ªª‰Ωï‰∫∫ÈÉΩÊúâÂèØËÉΩÂèñÂæóÊàêÂäü„ÄÇ

# 3rd dialogue turn
response, history = model.chat(tokenizer, "ÁªôËøô‰∏™ÊïÖ‰∫ãËµ∑‰∏Ä‰∏™Ê†áÈ¢ò", history=history)
print(response)
# „ÄäÂ•ãÊñóÂàõ‰∏öÔºö‰∏Ä‰∏™Âπ¥ËΩª‰∫∫ÁöÑÊàêÂäü‰πãË∑Ø„Äã
```

L'ex√©cution du mod√®le pr√©-entra√Æn√© de Qwen est √©galement simple.

<details>
  <summary>Running Qwen</summary>

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# Model names: "Qwen/Qwen-7B", "Qwen/Qwen-14B" 
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)
# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="cpu", trust_remote_code=True).eval()
# use auto mode, automatically select precision based on the device.
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B",
    device_map="auto",
    trust_remote_code=True
).eval()

# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)

inputs = tokenizer('ËíôÂè§ÂõΩÁöÑÈ¶ñÈÉΩÊòØ‰πåÂÖ∞Â∑¥ÊâòÔºàUlaanbaatarÔºâ\nÂÜ∞Â≤õÁöÑÈ¶ñÈÉΩÊòØÈõ∑ÂÖãÈõÖÊú™ÂÖãÔºàReykjavikÔºâ\nÂüÉÂ°û‰øÑÊØî‰∫öÁöÑÈ¶ñÈÉΩÊòØ', return_tensors='pt')
inputs = inputs.to(model.device)
pred = model.generate(**inputs)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))
# ËíôÂè§ÂõΩÁöÑÈ¶ñÈÉΩÊòØ‰πåÂÖ∞Â∑¥ÊâòÔºàUlaanbaatarÔºâ\nÂÜ∞Â≤õÁöÑÈ¶ñÈÉΩÊòØÈõ∑ÂÖãÈõÖÊú™ÂÖãÔºàReykjavikÔºâ\nÂüÉÂ°û‰øÑÊØî‰∫öÁöÑÈ¶ñÈÉΩÊòØ‰∫öÁöÑÊñØ‰∫öË¥ùÂ∑¥ÔºàAddis AbabaÔºâ...
```

</details>

En cas de probl√®me de r√©seau lors de la tentative de t√©l√©chargement des poids et des codes du mod√®le √† partir de HuggingFace, une autre approche consiste √† r√©cup√©rer le point de contr√¥le √† partir de ModelScope, puis √† le charger √† partir du r√©pertoire local, comme indiqu√© ci-dessous:

```python
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer

# Downloading model checkpoint to a local dir model_dir
# model_dir = snapshot_download('qwen/Qwen-7B')
# model_dir = snapshot_download('qwen/Qwen-7B-Chat')
# model_dir = snapshot_download('qwen/Qwen-14B')
model_dir = snapshot_download('qwen/Qwen-14B-Chat')

# Loading local checkpoints
# trust_remote_code is still set as True since we still load codes from local dir instead of transformers
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    device_map="auto",
    trust_remote_code=True
).eval()
```

### ü§ñ ModelScope

ModelScope est une plateforme opensource pour Model-as-a-Service (MaaS), qui fournit un service de mod√®le flexible et rentable aux d√©veloppeurs d'IA. De m√™me, vous pouvez ex√©cuter les mod√®les avec ModelScope comme indiqu√© ci-dessous:

```python
from modelscope import AutoModelForCausalLM, AutoTokenizer
from modelscope import GenerationConfig

# Model names: "qwen/Qwen-7B-Chat", "qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("qwen/Qwen-7B-Chat", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True) # ÂèØÊåáÂÆö‰∏çÂêåÁöÑÁîüÊàêÈïøÂ∫¶„ÄÅtop_pÁ≠âÁõ∏ÂÖ≥Ë∂ÖÂèÇ

response, history = model.chat(tokenizer, "‰Ω†Â•Ω", history=None)
print(response)
response, history = model.chat(tokenizer, "ÊµôÊ±üÁöÑÁúÅ‰ºöÂú®Âì™ÈáåÔºü", history=history) 
print(response)
response, history = model.chat(tokenizer, "ÂÆÉÊúâ‰ªÄ‰πàÂ•ΩÁé©ÁöÑÊôØÁÇπ", history=history)
print(response)
```

### Inf√©rence par lots
Qwen prend en charge l'inf√©rence par lots. Lorsque flash attention est activ√©e, l'utilisation de l'inf√©rence par lots peut entra√Æner une acc√©l√©ration de 40 %. Le code d'exemple est pr√©sent√© ci-dessous:
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import GenerationConfig
from qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids

tokenizer = AutoTokenizer.from_pretrained(
    './',
    pad_token='<|extra_0|>',
    eos_token='<|endoftext|>',
    padding_side='left',
    trust_remote_code=True
)
model = AutoModelForCausalLM.from_pretrained(
    './',
    pad_token_id=tokenizer.pad_token_id,
    device_map="auto",
    trust_remote_code=True
).eval()
model.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)

all_raw_text = ["ÊàëÊÉ≥Âê¨‰Ω†ËØ¥Áà±Êàë„ÄÇ", "‰ªäÂ§©ÊàëÊÉ≥ÂêÉÁÇπÂï•ÔºåÁîúÁîúÁöÑÔºåÊé®Ëçê‰∏ã", "ÊàëÈ©¨‰∏äËøüÂà∞‰∫ÜÔºåÊÄé‰πàÂÅöÊâçËÉΩ‰∏çËøüÂà∞"]
batch_raw_text = []
for q in all_raw_text:
    raw_text, _ = make_context(
        tokenizer,
        q,
        system="You are a helpful assistant.",
        max_window_size=model.generation_config.max_window_size,
        chat_format=model.generation_config.chat_format,
    )
    batch_raw_text.append(raw_text)

batch_input_ids = tokenizer(batch_raw_text, padding='longest')
batch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)
batch_out_ids = model.generate(
    batch_input_ids,
    return_dict_in_generate=False,
    generation_config=model.generation_config
)
padding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]

batch_response = [
    decode_tokens(
        batch_out_ids[i][padding_lens[i]:],
        tokenizer,
        raw_text_len=len(batch_raw_text[i]),
        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),
        chat_format="chatml",
        verbose=False,
        errors='replace'
    ) for i in range(len(all_raw_text))
]
print(batch_response)

response, _ = model.chat(tokenizer, "ÊàëÊÉ≥Âê¨‰Ω†ËØ¥Áà±Êàë„ÄÇ", history=None)
print(response)

response, _ = model.chat(tokenizer, "‰ªäÂ§©ÊàëÊÉ≥ÂêÉÁÇπÂï•ÔºåÁîúÁîúÁöÑÔºåÊé®Ëçê‰∏ã", history=None)
print(response)

response, _ = model.chat(tokenizer, "ÊàëÈ©¨‰∏äËøüÂà∞‰∫ÜÔºåÊÄé‰πàÂÅöÊâçËÉΩ‰∏çËøüÂà∞", history=None)
print(response)
```

### CPU

Pour d√©ployer nos mod√®les sur CPU, nous vous conseillons vivement d'utiliser [qwen.cpp](https://github.com/QwenLM/qwen.cpp), qui est une impl√©mentation purement C++ de Qwen et de tiktoken. Consultez le repo pour plus de d√©tails!

Il est simple d'ex√©cuter directement le mod√®le sur le CPU, ce qui n√©cessite la sp√©cification de votre appareil:

```python
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
```

Cependant, il est probable que vous souffriez d'une efficacit√© d'inf√©rence extr√™mement faible.

### Plusieurs GPU

Si vous souffrez d'un manque de m√©moire GPU et que vous souhaitez ex√©cuter le mod√®le sur plus d'un GPU, vous pouvez utiliser directement la m√©thode de chargement par d√©faut, qui est maintenant support√©e par Transformers. La m√©thode pr√©c√©dente bas√©e sur `utils.py` est obsol√®te.

Cependant, bien que cette m√©thode soit simple, l'efficacit√© du parall√©lisme natif du pipeline est faible. Nous vous conseillons d'utiliser vLLM avec FastChat et de lire la section relative au d√©ploiement.
<br><br>

## Quantization

### GPTQ

Nous proposons une solution bas√©e sur [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), et publions les mod√®les quantifi√©s Int4, qui permettent d'obtenir des effets de mod√®le presque sans perte mais des performances am√©lior√©es en termes de co√ªts de m√©moire et de vitesse d'inf√©rence.

Nous d√©montrons ici comment utiliser les mod√®les quantifi√©s que nous fournissons pour l'inf√©rence. Avant de commencer, assurez-vous que vous r√©pondez aux exigences d'auto-gptq (par exemple, torch 2.0 et plus, transformers 4.32.0 et plus, etc.) et installez les paquets requis:

```bash
pip install auto-gptq optimum
```

Si vous rencontrez des probl√®mes pour installer `auto-gptq`, nous vous conseillons de consulter le [repo](https://github.com/PanQiWei/AutoGPTQ) officiel pour trouver une roue.

Vous pouvez ensuite charger facilement le mod√®le quantifi√© et lancer l'inf√©rence comme d'habitude:

```python
# Model names: "Qwen/Qwen-7B-Chat-Int4", "Qwen/Qwen-14B-Chat-Int4"
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat-Int4",
    device_map="auto",
    trust_remote_code=True
).eval()
response, history = model.chat(tokenizer, "Hi", history=None)
```

Nous illustrons les performances des mod√®les BF16, Int8 et Int4 sur le benchmark, et nous constatons que le mod√®le quantifi√© ne souffre pas d'une d√©gradation significative des performances. Les r√©sultats sont pr√©sent√©s ci-dessous:

| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |
|----------------------|:----:|:-----------:|:-----:|:---------:|
| Qwen-7B-Chat (BF16)  | 55.8 |    59.7     | 50.3  |   37.2    |
| Qwen-7B-Chat (Int8)  | 55.4 |    59.4     | 48.3  |   34.8    |
| Qwen-7B-Chat (Int4)  | 55.1 |    59.2     | 49.7  |   29.9    |
| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 60.1  |   43.9    |
| Qwen-14B-Chat (Int8) | 63.6 |    68.6     | 60.0	 |   48.2    |
| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |

### Quantization du cache KV

Attention Le cache KV peut √™tre quantifi√© et compress√© pour le stockage, afin d'obtenir un d√©bit d'√©chantillonnage plus √©lev√©. Les param√®tres `use_cache_quantization` et `use_cache_kernel` sont fournis pour contr√¥ler le comportement de quantification du cache KV
Lorsque `use_cache_quantization=True` et `use_cache_kernel=True`, la quantization de kv-cache est activ√©e.
La m√©thode d'utilisation sp√©cifique est la suivante:

```python
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat",
     device_map="auto",
     trust_remote_code=True,
     use_cache_quantization=True,
     use_cache_kernel=True,
     use_flash_attn=False
)
```
Attention : Actuellement, la quantization du cache kv et le flash attn ne peuvent pas √™tre activ√©s en m√™me temps.
Si vous activez la quantification du cache kv et use_flash_attn en m√™me temps (`use_flash_attn=True`, `use_cache_quantization=True`, `use_cache_kernel=True`), use_flash_attn est d√©sactiv√© par d√©faut (`use_flash_attn=false`).

Nous avons v√©rifi√© que l'utilisation du mod√®le int8-kvcache quantifi√© ne souffre pas d'une d√©gradation significative des performances dans l'√©valuation en aval. En outre, nous √©valuons ses performances en nous concentrant sur l'empreinte m√©moire. 
Le profilage s'ex√©cute sur un seul GPU A100-SXM4-80G avec PyTorch 2.0.1 et CUDA 11.4. 
Nous utilisons des mod√®les BF16, et g√©n√©rons 1024 tokens (seq-length=1024) par d√©faut, et oom indique qu'il n'y a plus de m√©moire.

Lorsque la quantization de kv-cache est activ√©e, nous pouvons utiliser une taille de lot (bs) plus importante.

| USE KVCache |  bs=1  |  bs=4  | bs=16  | bs=32  | bs=64  | bs=100 |
|-------------|:------:|:------:|:------:|:------:|:------:|:------:|
| no          | 16.3GB | 24.1GB | 31.7GB | 48.7GB |  oom   |  oom   |
| yes         | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB |

Lorsque la quantification de kv-cache est activ√©e, le mod√®le peut √©conomiser plus de m√©moire lorsqu'il g√©n√®re des s√©quences plus longues (sl, nombre de jetons g√©n√©r√©s) lors de l'inf√©rence.

| USE KVCache | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |
|-------------|:------:|:-------:|:-------:|:-------:|:-------:|
| no          | 15.2GB | 16.3GB  | 17.6GB  | 19.5GB  | 23.2GB  |
| yes         |  15GB  | 15.5GB  | 15.8GB  | 16.6GB  | 17.6GB  |

Le mod√®le qui active la quantification du kv-cache convertit le format du layer-past de float √† int8, tandis que le layer-past quantifi√© stocke √©galement les param√®tres de quantification de la valeur actuelle.
Les √©tapes sp√©cifiques sont les suivantes :

1. Quantifier cl√©/valeur
```
    qv,scale,zero_point=quantize_cache_v(v)
```
2. Stocker dans layer_past

Following is the format of quantized layer_past:
```
    layer_past=((q_key,key_scale,key_zero_point),
                (q_value,value_scale,value_zero_point))
```
Format de base de layer_past:
```
    layer_past=(key,value)
```
Si vous souhaitez utiliser l'attention KV qui est quantifi√©e, vous pouvez utiliser l'op√©ration de d√©quantification pour convertir la cl√©/valeur int8 en format float comme suit 
vous pouvez utiliser l'op√©ration de d√©quantification pour reconvertir la cl√©/valeur int8 au format float comme suit :
```
    v=dequantize_cache_torch(qv,scale,zero_point)
```
<br>


## Performance de l'inf√©rence

Cette section fournit les statistiques de vitesse et de m√©moire des mod√®les dans diff√©rentes pr√©cisions. Le profilage de la vitesse et de la m√©moire est effectu√© √† l'aide de [ce script](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py).

### Vitesse

Nous avons mesur√© la vitesse moyenne d'inf√©rence (jetons/s) pour la g√©n√©ration de 2048 et 8192 jetons avec les mod√®les dans la pr√©cision de BF16, Int8, et Int4 sous la condition d'utiliser l'attention flash v1, v2, ou de ne pas l'utiliser.

<table>
    <tr>
      <th rowspan="2">Model Size</th><th rowspan="2">Precision</th><th rowspan="2">FlashAttn</th><th colspan="2" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">2048</th><th align="center">8192</th>
    </tr>
    </tr>
    </tr>
    <tr>
        <th rowspan="9">7B</th><td align="center" rowspan="3">BF16</td><td align="center">v2</td><td align="center">40.93</td><td align="center">36.14</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">40.75</td><td align="center">35.34
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">37.55</td><td align="center">33.56
    </tr>
    <tr>
        <td align="center" rowspan="3">Int8</td><td align="center">v2</td><td align="center">37.47</td><td align="center">32.54</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">37.51</td><td align="center">32.39
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">37.84</td><td align="center">32.65
    </tr>
    <tr>
        <td align="center" rowspan="3">Int4</td><td align="center">v2</td><td align="center">50.09</td><td align="center">38.61</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">45.98</td><td align="center">36.47
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">48.12</td><td align="center">36.70
    </tr>
    <tr>
        <th rowspan="9">14B</th><td align="center" rowspan="3">BF16</td><td align="center">v2</td><td align="center">32.88</td><td align="center">24.87</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">32.76</td><td align="center">28.89
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">29.32</td><td align="center">22.91
    </tr>
    <tr>
        <td align="center" rowspan="3">Int8</td><td align="center">v2</td><td align="center">29.28</td><td align="center">24.22</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">28.31</td><td align="center">23.87
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">31.12</td><td align="center">24.60
    </tr>
    <tr>
        <td align="center" rowspan="3">Int4</td><td align="center">v2</td><td align="center">38.72</td><td align="center">27.33</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">37.81</td><td align="center">26.46
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">37.65</td><td align="center">26.00
    </tr>
</table>


En d√©tail, le profilage consiste √† encoder 2048 jetons et √† g√©n√©rer 8192 nouveaux jetons. Le profilage s'ex√©cute sur un seul GPU A100-SXM4-80G avec PyTorch 2.0.1 et CUDA 11.8. La vitesse d'inf√©rence est calcul√©e en moyenne sur les jetons encod√©s et g√©n√©r√©s.

Note : La vitesse de g√©n√©ration des mod√®les Int4/Int8 mentionn√©s ci-dessus est fournie par la biblioth√®que autogptq. La vitesse actuelle du mod√®le charg√© √† l'aide de "AutoModelForCausalLM.from_pretrained" sera environ 20% plus lente. Nous avons signal√© ce probl√®me √† l'√©quipe HuggingFace et nous le mettrons √† jour rapidement si une solution est disponible.

### Utilisation de la m√©moire du GPU

Nous avons √©galement √©tabli le profil de l'utilisation maximale de la m√©moire du GPU pour l'encodage de 2048 jetons en tant que contexte (et la g√©n√©ration d'un seul jeton) et la g√©n√©ration de 8192 jetons (avec un seul jeton en tant que contexte) sous BF16, Int8 ou Int4 niveau de quantization, respectivement. Les r√©sultats (GB) sont pr√©sent√©s ci-dessous.

<table>
    <tr>
      <th rowspan="2">Model Size</th><th rowspan="2">Precision</th><th colspan="2" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">2048</th><th align="center">8192</th>
    </tr>
    </tr>
    </tr>
    <tr>
        <th rowspan="3">7B</th><td align="center">BF16</td><td align="center">16.99</td><td align="center">22.53</td>
    </tr>
    <tr>
        <td align="center">Int8</td><td align="center">11.20</td><td align="center">16.62
    </tr>
    <tr>
        <td align="center">Int4</td><td align="center">8.21</td><td align="center">13.63</td>
    </tr>
    <tr>
        <th rowspan="3">14B</th><td align="center">BF16</td><td align="center">30.15</td><td align="center">38.94</td>
    </tr>
    <tr>
        <td align="center">Int8</td><td align="center">18.81</td><td align="center">27.54
    </tr>
    <tr>
        <td align="center">Int4</td><td align="center">13.01</td><td align="center">21.79</td>
    </tr>
</table>


<br>


## Finetuning

### Utilisation
Nous fournissons maintenant le script d'entra√Ænement officiel, `finetune.py`, pour que les utilisateurs puissent ajuster le mod√®le pr√©-entra√Æn√© pour les applications en aval de mani√®re simple. De plus, nous fournissons des scripts shell pour lancer le finetune sans soucis. Ce script prend en charge l'entra√Ænement avec [DeepSpeed](https://github.com/microsoft/DeepSpeed) et [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/). Les scripts que nous fournissons utilisent DeepSpeed (Note : il peut y avoir des conflits avec la derni√®re version de pydantic) et Peft. Vous pouvez les installer en proc√©dant comme suit:
```bash
pip install peft deepspeed
```

Pour pr√©parer vos donn√©es d'entra√Ænement, vous devez rassembler tous les √©chantillons dans une liste et l'enregistrer dans un fichier json. Chaque √©chantillon est un dictionnaire compos√© d'un identifiant et d'une liste de conversation. Voici un exemple simple de liste avec 1 √©chantillon:
```json
[
  {
    "id": "identity_0",
    "conversations": [
      {
        "from": "user",
        "value": "‰Ω†Â•Ω"
      },
      {
        "from": "assistant",
        "value": "ÊàëÊòØ‰∏Ä‰∏™ËØ≠Ë®ÄÊ®°ÂûãÔºåÊàëÂè´ÈÄö‰πâÂçÉÈóÆ„ÄÇ"
      }
    ]
  }
]
```

Apr√®s la pr√©paration des donn√©es, vous pouvez utiliser les scripts shell fournis pour lancer le finetuning. N'oubliez pas de sp√©cifier le chemin d'acc√®s au fichier de donn√©es, `$DATA`.

Les scripts de finetuning vous permettent d'effectuer les op√©rations suivantes
- Finetuning de tous les param√®tres
- LoRA
- Q-LoRA

Le finetuning de tous les param√®tres n√©cessite la mise √† jour de tous les param√®tres au cours de l'ensemble du processus de formation. Pour lancer votre formation, ex√©cutez le script suivant:

```bash
# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.
sh finetune/finetune_ds.sh
```

N'oubliez pas de sp√©cifier le nom ou le chemin d'acc√®s au mod√®le, le chemin d'acc√®s aux donn√©es, ainsi que le r√©pertoire de sortie dans les scripts shell. Une autre chose √† noter est que nous utilisons DeepSpeed ZeRO 3 dans ce script. Si vous voulez faire des changements, il suffit de supprimer l'argument `--deepspeed` ou de faire des changements dans le fichier json de configuration de DeepSpeed en fonction de vos besoins. De plus, ce script supporte l'entra√Ænement en pr√©cision mixte, et donc vous pouvez utiliser `--bf16 True` ou `--fp16 True`. N'oubliez pas d'utiliser DeepSpeed lorsque vous utilisez fp16 en raison de l'entra√Ænement de pr√©cision mixte. Empiriquement, nous vous conseillons d'utiliser bf16 pour rendre votre apprentissage coh√©rent avec notre pr√©-entra√Ænement et notre alignement si votre machine supporte bf16, et nous l'utilisons donc par d√©faut.

Pour ex√©cuter LoRA, utilisez un autre script √† ex√©cuter comme indiqu√© ci-dessous. Avant de commencer, assurez-vous que vous avez install√© `peft`. Vous devez sp√©cifier les chemins d'acc√®s √† votre mod√®le, √† vos donn√©es et √† vos r√©sultats. Nous vous conseillons d'utiliser des chemins absolus pour votre mod√®le pr√©-entra√Æn√©. En effet, LoRA ne sauvegarde que l'adaptateur et le chemin absolu dans le fichier json de configuration de l'adaptateur est utilis√© pour trouver le mod√®le pr√©-entra√Æn√© √† charger. De plus, ce script supporte √† la fois bf16 et fp16.

```bash
# Single GPU training
sh finetune/finetune_lora_single_gpu.sh
# Distributed training
sh finetune/finetune_lora_ds.sh
```

Par rapport au finetuning de tous les param√®tres, LoRA ([paper](https://arxiv.org/abs/2106.09685)) ne met √† jour que les param√®tres des couches d'adaptateurs, tout en gelant les couches originales du grand mod√®le de langage. Cela permet de r√©duire consid√©rablement les co√ªts de m√©moire et donc les co√ªts de calcul.

Notez que si vous utilisez LoRA pour affiner le mod√®le de langue, par exemple Qwen-7B, au lieu des mod√®les de chat, par exemple Qwen-7B-Chat, le script change automatiquement les embedding et la couche de sortie en tant que param√®tres entra√Ænables. En effet, le mod√®le de langue n'a aucune connaissance des jetons sp√©ciaux apport√©s par le format ChatML. Ces couches doivent donc √™tre mises √† jour pour que le mod√®le comprenne et pr√©dise les jetons. En d'autres termes, si votre entra√Ænement apporte des tokens sp√©ciaux dans LoRA, vous devez d√©finir les couches comme des param√®tres entra√Ænables en d√©finissant `modules_to_save` √† l'int√©rieur du code. De plus, si ces param√®tres sont entra√Ænables, il n'est pas possible d'utiliser ZeRO 3, et c'est pourquoi nous utilisons ZeRO 2 par d√©faut dans le script. Si vous n'avez pas de nouveaux param√®tres entra√Ænables, vous pouvez passer √† ZeRO 3 en modifiant le fichier de configuration de DeepSpeed. En outre, nous constatons qu'il existe un √©cart important entre l'empreinte m√©moire de LoRA avec et sans ces param√®tres d'entra√Ænement. Par cons√©quent, si vous avez des probl√®mes de m√©moire, nous vous conseillons d'affiner les mod√®les de chat de LoRA. Consultez le profil ci-dessous pour plus d'informations.

Si vous souffrez toujours d'un manque de m√©moire, vous pouvez envisager Q-LoRA ([paper](https://arxiv.org/abs/2305.14314)), qui utilise le mod√®le de langage quantifi√© et d'autres techniques telles que l'attention pagin√©e pour r√©duire encore les co√ªts de m√©moire.

Note : pour ex√©cuter l'entra√Ænement Q-LoRA sur un seul GPU, vous pouvez avoir besoin d'installer `mpi4py` via `pip` ou `conda`.

Pour lancer Q-LoRA, ex√©cutez directement le script suivant:

```bash
# Single GPU training
sh finetune/finetune_qlora_single_gpu.sh
# Distributed training
sh finetune/finetune_qlora_ds.sh
```

Pour Q-LoRA, nous vous conseillons de charger le mod√®le quantifi√© que nous fournissons, par exemple Qwen-7B-Chat-Int4. Vous **NE DEVRIEZ PAS** utiliser les mod√®les bf16. Contrairement au finetuning de tous les param√®tres et √† la LoRA, seul le mod√®le fp16 est pris en charge pour la Q-LoRA. Pour l'entra√Ænement sur un seul GPU, nous devons utiliser DeepSpeed pour l'entra√Ænement en pr√©cision mixte en raison de notre observation des erreurs caus√©es par torch amp. En outre, pour Q-LoRA, les probl√®mes avec les jetons sp√©ciaux dans LoRA existent toujours. Cependant, comme nous ne fournissons que les mod√®les Int4 pour les mod√®les de chat, ce qui signifie que le mod√®le de langage a appris les tokens sp√©ciaux du format ChatML, vous n'avez pas √† vous soucier des couches. Notez que les couches du mod√®le Int4 ne doivent pas √™tre entra√Ænables, et donc si vous introduisez des tokens sp√©ciaux dans votre entra√Ænement, Q-LoRA risque de ne pas fonctionner.

Contrairement au finetuning des param√®tres complets, l'entra√Ænement de LoRA et de Q-LoRA n'enregistre que les param√®tres de l'adaptateur. Supposons que votre entra√Ænement commence √† partir de Qwen-7B, vous pouvez charger le mod√®le finalis√© pour l'inf√©rence comme indiqu√© ci-dessous:

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()
```

Si vous souhaitez fusionner les adaptateurs et enregistrer le mod√®le affin√© en tant que mod√®le autonome (vous ne pouvez le faire qu'avec LoRA, et vous **NE POUVEZ PAS** fusionner les param√®tres de Q-LoRA), vous pouvez ex√©cuter les codes suivants:

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()

merged_model = model.merge_and_unload()
# max_shard_size and safe serialization are not necessary. 
# They respectively work for sharding checkpoint and save the model to safetensors
merged_model.save_pretrained(new_model_directory, max_shard_size="2048MB", safe_serialization=True)
```

Note : Pour l'entra√Ænement multi-GPU, vous devez sp√©cifier les hyperparam√®tres appropri√©s pour l'entra√Ænement distribu√© en fonction de votre machine. De plus, nous vous conseillons de sp√©cifier votre longueur maximale de s√©quence avec l'argument `--model_max_length`, en fonction de votre consid√©ration des donn√©es, de l'empreinte m√©moire, et de la vitesse d'apprentissage.

### Profilage de la m√©moire et de la vitesse
Nous profilons la m√©moire du GPU et la vitesse d'apprentissage de LoRA (LoRA (emb) se r√©f√®re √† l'apprentissage de l'embedding et la couche de sortie, tandis que LoRA n'a pas de couche d'int√©gration et de sortie pouvant √™tre entra√Æn√©e) et de Q-LoRA dans la configuration de l'apprentissage sur un seul GPU. Dans ce test, nous exp√©rimentons sur un seul GPU A100-SXM4-80G, et nous utilisons CUDA 11.8 et Pytorch 2.0. Flash attention 2 est appliqu√©. Nous utilisons uniform√©ment une taille de lot de 1 et une accumulation de gradient de 8. Nous profilons la m√©moire (GB) et la vitesse (s/iter) des entr√©es de diff√©rentes longueurs, √† savoir 256, 512, 1024, 2048, 4096, et 8192. Nous pr√©sentons √©galement les statistiques du finetuning de tous les param√®tres avec Qwen-7B sur 2 GPU A100. Nous ne pr√©sentons que les statistiques de 256, 512 et 1024 jetons en raison de la limitation de la m√©moire du GPU. Les statistiques sont list√©es ci-dessous :

<table>
    <tr>
      <th rowspan="2">Model Size</th><th rowspan="2">Method</th><th colspan="6" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">256</th><th align="center">512</th><th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th>
    </tr>
    </tr>
    </tr>
    <tr>
        <th rowspan="4">7B</th><td>LoRA</td><td align="center">20.1G / 1.2s/it</td><td align="center">20.4G / 1.5s/it</td><td align="center">21.5G / 2.8s/it</td><td align="center">23.8G / 5.2s/it</td><td align="center">29.7G / 10.1s/it</td><td align="center">36.6G / 21.3s/it</td>
    </tr>
    <tr>
        <td>LoRA (emb)</td><td align="center">33.7G / 1.4s/it</td><td align="center">34.1G / 1.6s/it</td><td align="center">35.2G / 2.9s/it</td><td align="center">35.1G / 5.3s/it</td><td align="center">39.2G / 10.3s/it</td><td align="center">48.5G / 21.7s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td align="center">11.5G / 3.0s/it</td><td align="center">11.5G / 3.0s/it</td><td align="center">12.3G / 3.5s/it</td><td align="center">13.9G / 7.0s/it</td><td align="center">16.9G / 11.6s/it</td><td align="center">23.5G / 22.3s/it</td>
    </tr>
    <tr>
        <td>Full-parameter</td><td align="center">139.2G / 4.0s/it</td><td align="center">148.0G / 4.0s/it</td><td align="center">162.0G / 4.5s/it</td><td align="center">-</td><td align="center">-</td><td align="center">-</td>
    </tr>
    <tr>
        <th rowspan="3">14B</th><td>LoRA</td><td align="center">34.6G / 1.6s/it</td><td align="center">35.1G / 2.4s/it</td><td align="center">35.3G / 4.4s/it</td><td align="center">37.4G / 8.4s/it</td><td align="center">42.5G / 17.0s/it</td><td align="center">55.2G / 36.0s/it</td>
    </tr>
    <tr>
        <td>LoRA (emb)</td><td align="center">51.2 / 1.7s/it</td><td align="center">51.1G / 2.6s/it</td><td align="center">51.5G / 4.6s/it</td><td align="center">54.1G / 8.6s/it</td><td align="center">56.8G / 17.2s/it</td><td align="center">67.7G / 36.3s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td align="center">18.7G / 5.3s/it</td><td align="center">18.4G / 6.3s/it</td><td align="center">18.9G / 8.2s/it</td><td align="center">19.9G / 11.8s/it</td><td align="center">23.0G / 20.1s/it</td><td align="center">27.9G / 38.3s/it</td>
    </tr>
</table>
<br>

## D√©ploiement

### vLLM 
Pour le d√©ploiement et l'inf√©rence rapide, nous sugg√©rons d'utiliser vLLM avec FastChat. Installez d'abord les paquets:
```bash
pip install vllm
pip install "fschat[model_worker,webui]"
```
Ou vous pouvez les installer √† partir des sources par `git clone` et `pip install -e .`. Nous vous conseillons de lire leurs documents si vous rencontrez des probl√®mes lors de l'installation.

Pour faire fonctionner Qwen avec vLLM et FastChat, vous devez d'abord lancer un contr√¥leur par:
```bash
python -m fastchat.serve.controller
```

Ensuite, vous pouvez lancer le travailleur de mod√®le, ce qui signifie charger votre mod√®le pour l'inf√©rence. Pour l'inf√©rence sur un seul GPU, vous pouvez directement lancer:
```bash
python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code
```
Cependant, si vous souhaitez ex√©cuter le mod√®le sur plusieurs GPU pour une inf√©rence plus rapide ou une m√©moire plus importante, vous pouvez utiliser le parall√©lisme tensoriel pris en charge par vLLM. Supposons que vous ex√©cutiez le mod√®le sur 4 GPU, la commande est pr√©sent√©e ci-dessous:
```bash
python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4
```

Apr√®s avoir lanc√© votre model worker, vous pouvez lancer une d√©mo web ou une API OpenAI comme vous le souhaitez. Pour la d√©mo web, ex√©cutez la commande suivante:
```bash
python -m fastchat.serve.gradio_web_server
```
Pour l'API OpenAI, consultez d'abord la documentation de notre API OpenAI pour l'installation. Ex√©cutez ensuite la commande:
```bash
python -m fastchat.serve.openai_api_server --host localhost --port 8000
```
<br>

## D√©mo

### Interface Web

Nous fournissons du code pour que les utilisateurs puissent construire une d√©mo d'interface web (merci √† @wysaid). Avant de commencer, assurez-vous d'installer les paquets suivants:

```
pip install -r requirements_web_demo.txt
```

Ex√©cutez ensuite la commande ci-dessous et cliquez sur le lien g√©n√©r√©:

```bash
python web_demo.py
```

<p align="center">
    <br>
    <img src="assets/web_demo.gif" width="600" />
    <br>
<p>

### D√©mo CLI

Nous fournissons un exemple de d√©monstration CLI dans `cli_demo.py`, qui prend en charge la sortie en continu pour la g√©n√©ration. Les utilisateurs peuvent interagir avec Qwen-7B-Chat en saisissant des invites, et le mod√®le renvoie les sorties du mod√®le en mode streaming. Ex√©cutez la commande ci-dessous:

```bash
python cli_demo.py
```

<p align="center">
    <br>
    <img src="assets/cli_demo.gif" width="600" />
    <br>
<p>
<br>

## API

Le moyen le plus simple d'utiliser Qwen via les API est le service API DashScope via Alibaba Cloud. Nous pr√©sentons une introduction √† l'utilisation. De plus, nous fournissons un script pour vous permettre de d√©ployer une API de type OpenAI sur vos propres serveurs.

### DashScope
DashScope est le service API de grands mod√®les linguistiques fourni par Alibaba Cloud, qui prend d√©sormais en charge Qwen. Notez que les mod√®les derri√®re DashScope sont des versions internes temporairement sans d√©tails fournis. Les services comprennent `qwen-turbo` et `qwen-plus`, le premier fonctionnant plus rapidement et le second atteignant de meilleures performances. Pour plus d'informations, consultez la documentation [ici] (https://dashscope.aliyun.com).

Veuillez vous rendre sur le site officiel [lien](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn) pour cr√©er un compte DashScope et obtenir la cl√© API (AK). Nous recommandons de d√©finir l'AK √† l'aide d'une variable d'environnement:
```bash
export DASHSCOPE_API_KEY="YOUR_DASHSCOPE_API_KEY"
```
Installez ensuite les paquets et cliquez sur [ici](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk) pour obtenir la documentation. Si vous utilisez Python, vous pouvez installer DashScope avec pip:
```bash
pip install dashscope
```
Si vous utilisez JAVA SDK, vous pouvez l'installer de cette mani√®re:
```xml
<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->
<dependency>
    <groupId>com.alibaba</groupId>
    <artifactId>dashscope-sdk-java</artifactId>
    <version>the-latest-version</version>
</dependency>
```
La mani√®re la plus simple d'utiliser DashScope est l'utilisation de messages, qui est similaire √† l'API OpenAI. L'exemple est pr√©sent√© ci-dessous:
```python
import random
from http import HTTPStatus
from dashscope import Generation


def call_with_messages():
    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},
                {'role': 'user', 'content': 'Â¶Ç‰ΩïÂÅöË•øÁ∫¢ÊüøÈ∏°ËõãÔºü'}]
    gen = Generation()
    response = gen.call(
        Generation.Models.qwen_turbo,
        messages=messages,
        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set
        result_format='message',  # set the result to be "message" format.
    )
    return response


if __name__ == '__main__':
    response = call_with_messages()
    if response.status_code == HTTPStatus.OK:
        print(response)
    else:
        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (
            response.request_id, response.status_code,
            response.code, response.message
        ))
```
Pour d'autres utilisations, veuillez consulter le site web officiel pour plus de d√©tails.

### API OpenAI

Nous fournissons des m√©thodes pour d√©ployer une API locale bas√©e sur l'API OpenAI (merci √† @hanpenggit). Avant de commencer, installez les paquets n√©cessaires:

```bash
pip install fastapi uvicorn openai "pydantic>=2.3.0" sse_starlette
```

Ex√©cutez ensuite la commande pour d√©ployer votre API:

```bash
python openai_api.py
```

Vous pouvez modifier vos arguments, par exemple, `-c` pour le nom ou le chemin du poids, `--cpu-only` pour le d√©ploiement CPU, etc. Si vous rencontrez des probl√®mes lors du lancement du d√©ploiement de l'API, la mise √† jour des paquets vers la derni√®re version peut probablement les r√©soudre.

L'utilisation de l'API est simple. Voir l'exemple ci-dessous:

```python
import openai
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "none"

# create a request activating streaming response
for chunk in openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "‰Ω†Â•Ω"}
    ],
    stream=True 
    # Specifying stop words in streaming output format is not yet supported and is under development.
):
    if hasattr(chunk.choices[0].delta, "content"):
        print(chunk.choices[0].delta.content, end="", flush=True)

# create a request not activating streaming response
response = openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "‰Ω†Â•Ω"}
    ],
    stream=False,
    stop=[] # You can add custom stop words here, e.g., stop=["Observation:"] for ReAct prompting.
)
print(response.choices[0].message.content)
```

<p align="center">
    <br>
    <img src="assets/openai_api.gif" width="600" />
    <br>
<p>

**Function calling** est aussi support√© (mais seulement quand `stream=False` pour le moment). Voir [l'exemple d'utilisation](examples/function_call_examples.py) ici.
<br><br>


## Utilisation des outils

Qwen-Chat a √©t√© optimis√© pour l'utilisation d'outils et les capacit√©s d'appel de fonctions. Les utilisateurs peuvent d√©velopper des agents, des applications LangChain, et m√™me augmenter Qwen avec un Code Interpreter.

Nous fournissons une documentation sur la mani√®re d'impl√©menter les appels d'outils bas√©s sur le principe de ReAct Prompting, veuillez vous r√©f√©rer √† [l'exemple ReAct](examples/react_prompt.md). Sur la base de ce principe, nous fournissons un support pour function calling dans [openai_api.py](openai_api.py).

Nous avons test√© les capacit√©s d'appel d'outil du mod√®le sur notre benchmark d'√©valuation chinois √† source ouverte et nous avons constat√© que Qwen-Chat obtient syst√©matiquement de bons r√©sultats:

<table>
    <tr>
        <th colspan="4" align="center">Chinese Tool-Use Benchmark</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selection (Acc.‚Üë)</th><th align="center">Tool Input (Rouge-L‚Üë)</th><th align="center">False Positive Error‚Üì</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">95%</td><td align="center">0.90</td><td align="center">15.0%</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">85%</td><td align="center">0.88</td><td align="center">75.0%</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">98%</td><td align="center">0.91</td><td align="center">7.3%</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">98%</td><td align="center">0.93</td><td align="center">2.4%</td>
    </tr>
</table>

Pour √©valuer la capacit√© de Qwen √† utiliser l'interpr√©teur de code Python pour des t√¢ches telles que la r√©solution de probl√®mes math√©matiques, la visualisation de donn√©es et d'autres t√¢ches g√©n√©rales telles que la manipulation de fichiers et l'exploration du Web, nous avons cr√©√© et mis en libre acc√®s un test de r√©f√©rence sp√©cialement con√ßu pour √©valuer ces capacit√©s. Vous pouvez trouver le benchmark sur ce [lien](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark).

Nous avons observ√© que Qwen est performant en termes d'ex√©cutabilit√© du code et de pr√©cision des r√©sultats lors de la g√©n√©ration du code:

<table>
    <tr>
        <th colspan="4" align="center">Executable Rate of Generated Code (%)</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Math‚Üë</th><th align="center">Visualization‚Üë</th><th align="center">General‚Üë</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">91.9</td><td align="center">85.9</td><td align="center">82.8</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">89.2</td><td align="center">65.0</td><td align="center">74.1</td>
    </tr>
    <tr>
        <td>LLaMA2-7B-Chat</td>
        <td align="center">41.9</td>
        <td align="center">33.1</td>
        <td align="center">24.1 </td>
    </tr>
    <tr>
        <td>LLaMA2-13B-Chat</td>
        <td align="center">50.0</td>
        <td align="center">40.5</td>
        <td align="center">48.3 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-7B-Instruct</td>
        <td align="center">85.1</td>
        <td align="center">54.0</td>
        <td align="center">70.7 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-13B-Instruct</td>
        <td align="center">93.2</td>
        <td align="center">55.8</td>
        <td align="center">74.1 </td>
    </tr>
    <tr>
        <td>InternLM-7B-Chat-v1.1</td>
        <td align="center">78.4</td>
        <td align="center">44.2</td>
        <td align="center">62.1 </td>
    </tr>
    <tr>
        <td>InternLM-20B-Chat</td>
        <td align="center">70.3</td>
        <td align="center">44.2</td>
        <td align="center">65.5 </td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td>
        <td align="center">82.4</td>
        <td align="center">64.4</td>
        <td align="center">67.2 </td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td>
        <td align="center">89.2</td>
        <td align="center">84.1</td>
        <td align="center">65.5</td>
    </tr>
</table>

<table>
    <tr>
        <th colspan="4" align="center">Accuracy of Code Execution Results (%)</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Math‚Üë</th><th align="center">Visualization-Hard‚Üë</th><th align="center">Visualization-Easy‚Üë</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">82.8</td><td align="center">66.7</td><td align="center">60.8</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">47.3</td><td align="center">33.3</td><td align="center">55.7</td>
    </tr>
    <tr>
        <td>LLaMA2-7B-Chat</td>
        <td align="center">3.9</td>
        <td align="center">14.3</td>
        <td align="center">39.2 </td>
    </tr>
    <tr>
        <td>LLaMA2-13B-Chat</td>
        <td align="center">8.3</td>
        <td align="center">8.3</td>
        <td align="center">40.5 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-7B-Instruct</td>
        <td align="center">14.3</td>
        <td align="center">26.2</td>
        <td align="center">60.8 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-13B-Instruct</td>
        <td align="center">28.2</td>
        <td align="center">27.4</td>
        <td align="center">62.0 </td>
    </tr>
    <tr>
        <td>InternLM-7B-Chat-v1.1</td>
        <td align="center">28.5</td>
        <td align="center">4.8</td>
        <td align="center">40.5 </td>
    </tr>
    <tr>
        <td>InternLM-20B-Chat</td>
        <td align="center">34.6</td>
        <td align="center">21.4</td>
        <td align="center">45.6 </td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td>
        <td align="center">41.9</td>
        <td align="center">40.5</td>
        <td align="center">54.4 </td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td>
        <td align="center">58.4</td>
        <td align="center">53.6</td>
        <td align="center">59.5</td>
    </tr>
</table>

<p align="center">
    <br>
    <img src="assets/code_interpreter_showcase_001.jpg" />
    <br>
<p>

En outre, nous fournissons √©galement des r√©sultats exp√©rimentaux d√©montrant que notre mod√®le est capable d'agir en tant qu'agent Hugging Face. Pour plus d'informations, veuillez vous r√©f√©rer √† la [documentation de l'exemple](examples/transformers_agent.md). Les performances du mod√®le sur l'ensemble des donn√©es d'√©valuation fournies par Hugging Face sont les suivantes:

<table>
    <tr>
        <th colspan="4" align="center">HuggingFace Agent Benchmark- Run Mode</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selection‚Üë</th><th align="center">Tool Used‚Üë</th><th align="center">Code‚Üë</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">100</td><td align="center">100</td><td align="center">97.4</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">95.4</td><td align="center">96.3</td><td align="center">87.0</td>
    </tr>
    <tr>
        <td>StarCoder-Base-15B</td><td align="center">86.1</td><td align="center">87.0</td><td align="center">68.9</td>
    </tr>
    <tr>
        <td>StarCoder-15B</td><td align="center">87.0</td><td align="center">88.0</td><td align="center">68.9</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">87.0</td><td align="center">87.0</td><td align="center">71.5</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">93.5</td><td align="center">94.4</td><td align="center">87.0</td>
    </tr>
</table>

<table>
    <tr>
        <th colspan="4" align="center">HuggingFace Agent Benchmark - Chat Mode</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selection‚Üë</th><th align="center">Tool Used‚Üë</th><th align="center">Code‚Üë</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">98.5</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">97.3</td><td align="center">96.8</td><td align="center">89.6</td>
    </tr>
    <tr>
        <td>StarCoder-Base-15B</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">91.1</td>
    </tr>
    <tr>
        <td>StarCoder-15B</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">89.6</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">94.7</td><td align="center">94.7</td><td align="center">85.1</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">95.5</td>
    </tr>
</table>

<br>

## Compr√©hension du contexte long

Pour √©tendre la longueur du contexte et briser le goulot d'√©tranglement de la longueur de la s√©quence d'entra√Ænement, nous introduisons plusieurs techniques, y compris l'interpolation consciente de NTK, l'attention de fen√™tre, et l'√©chelle d'attention LogN, pour √©tendre la longueur du contexte de Qwen-7B/14B de 2k √† plus de 8k tokens, et Qwen-7B de 8k √† 32k tokens. Nous menons des exp√©riences de mod√©lisation du langage sur l'ensemble de donn√©es arXiv avec l'√©valuation PPL et nous constatons que Qwen peut atteindre des performances exceptionnelles dans le sc√©nario d'un contexte long. Les r√©sultats sont pr√©sent√©s ci-dessous :

<table>
    <tr>
        <th rowspan="2">Model</th><th colspan="6" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th><th align="center">16384</th><th align="center">32768</th>
    </tr>
     <tr>
        <td>Qwen-7B (original)</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">39.35</td><td align="center">469.81</td><td align="center">2645.09</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.59</td><td align="center">3.66</td><td align="center">5.71</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.58</td><td align="center">3.56</td><td align="center">4.62</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.58</td><td align="center">3.49</td><td align="center">4.32</td><td align="center">-</td>
    </tr>
    <tr>
    <tr>
        <td>Qwen-7B</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.31</b></td><td align="center">7.27</td><td align="center">181.49</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.33</b></td><td align="center"><b>3.22</b></td><td align="center"><b>3.17</b></td>
    </tr>
    <tr>
        <td>Qwen-14B</td><td align="center"><b>-</b></td><td align="center"><b>3.46</b></td><td align="center">22.79</td><td align="center">334.65</td><td align="center">3168.35</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>-</b></td><td align="center"><b>3.46</b></td><td align="center"><b>3.29</b></td><td align="center"><b>3.18</b></td><td align="center">3.42</td><td align="center">-</td>
    </tr>
</table>


## Tokenizer

Notre tokenizer bas√© sur tiktoken est diff√©rent des autres tokenizers, par exemple le tokenizer sentencepiece. Vous devez faire attention aux tokens sp√©ciaux, en particulier lors de la mise au point. Pour des informations plus d√©taill√©es sur le tokenizer et son utilisation dans le cadre du finetuning, veuillez vous r√©f√©rer √† la [documentation](tokenization_note.md).
<br><br>

## Reproduction

Pour reproduire les performances du mod√®le sur des ensembles de donn√©es de r√©f√©rence, nous fournissons des scripts permettant de reproduire les r√©sultats. Consultez [eval/EVALUATION.md](eval/EVALUATION.md) pour plus d'informations. Notez que la reproduction peut entra√Æner de l√©g√®res diff√©rences par rapport √† nos r√©sultats.
<br><br>

## FAQ

Si vous rencontrez des probl√®mes, veuillez vous r√©f√©rer √† la [FAQ](FAQ.md) et aux probl√®mes pour trouver une solution avant de lancer un nouveau probl√®me.
<br><br>

## Citation
Si vous trouvez notre travail utile, n'h√©sitez pas √† nous citer.

```
@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
```
<br>

## Accord de Licence

Les chercheurs et les d√©veloppeurs sont libres d'utiliser les codes et les poids des mod√®les de Qwen et de Qwen-Chat. Nous autorisons √©galement leur utilisation commerciale. Consultez notre licence √† [LICENSE](LICENSE) pour plus de d√©tails. Si vous avez des exigences en mati√®re d'utilisation commerciale, veuillez remplir le formulaire ([7B](https://dashscope.console.aliyun.com/openModelApply/qianwen), [14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat)) pour en faire la demande.
<br><br>

## Contactez-nous

Si vous souhaitez laisser un message √† notre √©quipe de recherche ou √† notre √©quipe produit, rejoignez nos groupes Discord ou WeChat! N'h√©sitez pas non plus √† envoyer un courriel √† qianwen_opensource@alibabacloud.com.

