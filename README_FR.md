<p align="left">
    <a href="README_CN.md">ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>&nbsp ï½œ &nbsp<a href="README_JA.md">æ—¥æœ¬èª</a>&nbsp ï½œ &nbspFranÃ§ais
</p>
<br><br>

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg" width="400"/>
<p>
<br>

<p align="center">
    ğŸ¤— <a href="https://huggingface.co/Qwen">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href="https://modelscope.cn/organization/qwen">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href="https://arxiv.org/abs/2309.16609">Paper</a> &nbsp&nbsp ï½œ &nbsp&nbspğŸ–¥ï¸ <a href="https://modelscope.cn/studios/qwen/Qwen-14B-Chat-Demo/summary">Demo</a>
<br>
<a href="assets/wechat.png">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp ï½œ &nbsp&nbsp DingTalk (é’‰é’‰) &nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp&nbsp
</p>
<br><br>

|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                        Qwen-Chat (Int8)                         |                                                            Qwen                                                            |
|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|
| 7B  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat">ğŸ¤—</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int4">ğŸ¤—</a>  | <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int8">ğŸ¤—</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B">ğŸ¤—</a>  |
| 14B | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int4">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int8">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B">ğŸ¤—</a> |



Nous ouvrons notre sÃ©rie **Qwen**, qui comprend dÃ©sormais **Qwen**, les modÃ¨les de langue de base, Ã  savoir **Qwen-7B** et **Qwen-14B**, ainsi que **Qwen-Chat**, les modÃ¨les de chat, Ã  savoir **Qwen-7B-Chat** et **Qwen-14B-Chat**. Les liens se trouvent dans le tableau ci-dessus. Cliquez dessus et consultez les fiches des modÃ¨les. Nous publions Ã©galement le **[rapport technique](https://arxiv.org/abs/2309.16609)**. Cliquez sur le lien du document et consultez-le !

En bref, nous disposons de modÃ¨les linguistiques solides, qui ont Ã©tÃ© prÃ©-entraÃ®nÃ© de maniÃ¨re stable pour 3 000 milliards de tokens de donnÃ©es multilingues avec une large couverture de domaines, de langues (en particulier le chinois et l'anglais), etc. Ils sont capables d'atteindre des performances compÃ©titives sur des ensembles de donnÃ©es de rÃ©fÃ©rence. En outre, nous disposons de modÃ¨les de chat alignÃ©s sur les prÃ©fÃ©rences humaines basÃ©es sur SFT et RLHF (pas encore publiÃ©s), qui sont capables de chatter, de crÃ©er du contenu, d'extraire des informations, de rÃ©sumer, de traduire, de coder, de rÃ©soudre des problÃ¨mes mathÃ©matiques, etc. et d'utiliser des outils, de jouer le rÃ´le d'agents ou mÃªme code interpreter, etc.

Dans la repo, vous pouvez trouver:

* Comment utiliser Qwen, et profiter de l'infÃ©rence simple.
* DÃ©tails sur les modÃ¨les de quantization, y compris GPTQ et la quantization de KV cache.
* Statistiques sur les performances de l'infÃ©rence, y compris la vitesse et la mÃ©moire.
* Tutoriels sur le finetuning, y compris le finetuning de paramÃ¨tres complets, LoRA, et Q-LoRA.
* Instructions de dÃ©ploiement, avec l'exemple de vLLM et FastChat.
* Instructions sur la crÃ©ation de dÃ©mos, y compris WebUI, dÃ©mo CLI, etc.
* Introduction au service API de DashScope, ainsi que les instructions pour construire une API de type OpenAI pour votre modÃ¨le.
* Informations sur Qwen pour l'utilisation d'outils, d'agents et code interpreter.
* Statistiques de l'Ã©valuation de la comprÃ©hension du contexte long.
* Contrat de licence.
* ...

En outre, si vous rencontrez des problÃ¨mes, consultez d'abord la [FAQ](FAQ.md) pour obtenir de l'aide. Vous vous sentez toujours en difficultÃ© ? N'hÃ©sitez pas Ã  nous envoyer des questions (de prÃ©fÃ©rence en anglais pour que plus de gens puissent vous comprendre) ! Si vous souhaitez nous aider, envoyez-nous des demandes d'extension sans hÃ©sitation ! Nous sommes toujours enthousiastes Ã  propos des relations publiques ! 

Vous voulez discuter avec nous ou prendre un cafÃ© avec nous ? Bienvenue sur notre Discord ou WeChat !
<br><br>

## Nouvelles et mises Ã  jour

* 2023.10.17 Nous publions le modÃ¨le quantifiÃ© Int8 **Qwen-7B-Chat-Int8** et **Qwen-14B-Chat-Int8**.
* 2023.9.25 ğŸ”¥ Nous publions **Qwen-14B** et **Qwen-14B-Chat** sur ModelScope et Hugging Face, ainsi que [qwen.cpp](https://github.com/QwenLM/qwen.cpp) et [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent). Les codes et les poids de **Qwen-7B** et **Qwen-7B-Chat** ont Ã©galement Ã©tÃ© mis Ã  jour. **S'IL VOUS PLAÃT, TIREZ LA DERNIÃˆRE VERSION!**
    - Par rapport Ã  **Qwen-7B** (original), **Qwen-7B** utilise davantage de jetons d'entraÃ®nement, passant de 2,2 Ã  2,4T de jetons, tandis que la longueur du contexte passe de 2048 Ã  8192. La connaissance du chinois et la capacitÃ© de codage de **Qwen-7B** ont Ã©tÃ© encore amÃ©liorÃ©es.
* 2023.9.12 Nous prenons dÃ©sormais en charge le finetuning sur les modÃ¨les Qwen-7B, y compris le finetuning de tous les paramÃ¨tres, LoRA et Q-LoRA.
* 2023.8.21 Nous publions le modÃ¨le quantifiÃ© Int4 pour Qwen-7B-Chat, **Qwen-7B-Chat-Int4**, qui nÃ©cessite de faibles coÃ»ts de mÃ©moire mais permet d'amÃ©liorer la vitesse d'infÃ©rence. En outre, il n'y a pas de dÃ©gradation significative des performances lors de l'Ã©valuation de rÃ©fÃ©rence.
* 2023.8.3 Nous publions **Qwen-7B** et **Qwen-7B-Chat** sur ModelScope et Hugging Face. Nous fournissons Ã©galement un mÃ©mo technique pour plus de dÃ©tails sur le modÃ¨le, y compris les dÃ©tails de l'entraÃ®nement et les performances du modÃ¨le.
<br>

## Performance

Qwen-14B et Qwen-7B (il s'agit de la nouvelle version entraÃ®nÃ©e avec davantage de tokens et la longueur du contexte est passÃ©e de 2048 Ã  8192) surpassent les modÃ¨les de rÃ©fÃ©rence de tailles similaires sur une sÃ©rie d'ensembles de donnÃ©es de rÃ©fÃ©rence, par exemple MMLU, C-Eval, GSM8K, MATH, HumanEval, MBPP, BBH, etc., qui Ã©valuent les capacitÃ©s des modÃ¨les en matiÃ¨re de comprÃ©hension du langage naturel, de rÃ©solution de problÃ¨mes mathÃ©matiques, de codage, etc. Cependant, mÃªme Qwen-14B reste nettement infÃ©rieur Ã  GPT-3.5, sans parler de GPT-4. Voir les rÃ©sultats ci-dessous.

<p align="left">
    <img src="assets/radar_14b.jpg" width="600"/>
<p>
<br>

| Model              |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP   |   BBH    |  CMMLU   |
|:-------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:|
|                    |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot  |  3-shot  |  5-shot  |
| LLaMA2-7B          |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8   |   38.2   |   31.8   |
| LLaMA2-13B         |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3   |   45.6   |   38.4   |
| LLaMA2-34B         |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0   |   44.1   |    -     |
| ChatGLM2-6B        |   47.9   |   51.7   |   32.4   |   6.5    |     -     |    -     |   33.7   |    -     |
| InternLM-7B        |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0   |   37.0   |   51.8   |
| InternLM-20B       |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6   |   52.5   |   59.0   |
| Baichuan2-7B       |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2   |   41.6   |   57.1   |
| Baichuan2-13B      |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2   |   49.0   |   62.0   |
| Qwen-7B (original) |   56.7   |   59.6   |   51.6   |   10.4   |   24.4    |   31.2   |   40.6   |   58.8   |
| **Qwen-7B**        |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6   |   45.0   |   62.2   |
| **Qwen-14B**       | **66.3** | **72.1** | **61.3** | **24.8** | **32.3**  | **40.8** | **53.4** | **71.0** |

Pour tous les modÃ¨les comparÃ©s, nous indiquons les meilleurs scores entre leurs rÃ©sultats officiels et [OpenCompass] (https://opencompass.org.cn/leaderboard-llm). 

Pour plus de rÃ©sultats expÃ©rimentaux (performances dÃ©taillÃ©es des modÃ¨les sur d'autres ensembles de donnÃ©es de rÃ©fÃ©rence) et de dÃ©tails, veuillez vous rÃ©fÃ©rer Ã  notre rapport technique en cliquant [ici](https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf).
<br><br>

## Besoins

* python 3.8 et plus
* pytorch 1.12 et plus, 2.0 et plus sont recommandÃ©s
* transformers 4.32 et plus
* CUDA 11.4 et plus sont recommandÃ©s (pour les utilisateurs de GPU, les utilisateurs de flash, etc.)
<br>

## DÃ©marrage Rapide

Ci-dessous, nous fournissons des exemples simples pour montrer comment utiliser Qwen-Chat avec ğŸ¤– ModelScope et ğŸ¤— Transformers.

Avant d'exÃ©cuter le code, assurez-vous d'avoir configurÃ© l'environnement et installÃ© les paquets requis. Assurez-vous que vous rÃ©pondez aux exigences ci-dessus, puis installez les bibliothÃ¨ques dÃ©pendantes.

```bash
pip install -r requirements.txt
```

Si votre appareil supporte fp16 ou bf16, nous vous recommandons d'installer [flash-attention](https://github.com/Dao-AILab/flash-attention) (**nous supportons flash-attention 2 maintenant.**) pour une meilleure efficacitÃ© et une moindre utilisation de la mÃ©moire. (**flash-attention est optionnel et le projet peut fonctionner normalement sans l'installer**)

```bash
git clone https://github.com/Dao-AILab/flash-attention
cd flash-attention && pip install .
# Below are optional. Installing them might be slow.
# pip install csrc/layer_norm
# pip install csrc/rotary
```

Vous pouvez maintenant commencer avec ModelScope ou Transformers.

### ğŸ¤— Transformers

Pour utiliser Qwen-Chat pour l'infÃ©rence, il vous suffit de saisir quelques lignes de code, comme indiquÃ© ci-dessous. N'oubliez pas de transmettre les noms de modÃ¨les ou les chemins corrects, tels que "Qwen/Qwen-7B-Chat" et "Qwen/Qwen-14B-Chat". Cependant, **veuillez vous assurer que vous utilisez le code le plus rÃ©cent**.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# Model names: "Qwen/Qwen-7B-Chat", "Qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
# use auto mode, automatically select precision based on the device.
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat",
    device_map="auto",
    trust_remote_code=True
).eval()

# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# 1st dialogue turn
response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚

# 2nd dialogue turn
response, history = model.chat(tokenizer, "ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚", history=history)
print(response)
# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚
# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚
# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚
# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚
# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚
# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚

# 3rd dialogue turn
response, history = model.chat(tokenizer, "ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜", history=history)
print(response)
# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹
```

L'exÃ©cution du modÃ¨le prÃ©-entraÃ®nÃ© de Qwen est Ã©galement simple.

<details>
  <summary>Running Qwen</summary>

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# Model names: "Qwen/Qwen-7B", "Qwen/Qwen-14B" 
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)
# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="cpu", trust_remote_code=True).eval()
# use auto mode, automatically select precision based on the device.
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B",
    device_map="auto",
    trust_remote_code=True
).eval()

# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)

inputs = tokenizer('è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯', return_tensors='pt')
inputs = inputs.to(model.device)
pred = model.generate(**inputs)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))
# è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯äºšçš„æ–¯äºšè´å·´ï¼ˆAddis Ababaï¼‰...
```

</details>

En cas de problÃ¨me de rÃ©seau lors de la tentative de tÃ©lÃ©chargement des poids et des codes du modÃ¨le Ã  partir de HuggingFace, une autre approche consiste Ã  rÃ©cupÃ©rer le point de contrÃ´le Ã  partir de ModelScope, puis Ã  le charger Ã  partir du rÃ©pertoire local, comme indiquÃ© ci-dessous:

```python
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer

# Downloading model checkpoint to a local dir model_dir
# model_dir = snapshot_download('qwen/Qwen-7B')
# model_dir = snapshot_download('qwen/Qwen-7B-Chat')
# model_dir = snapshot_download('qwen/Qwen-14B')
model_dir = snapshot_download('qwen/Qwen-14B-Chat')

# Loading local checkpoints
# trust_remote_code is still set as True since we still load codes from local dir instead of transformers
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    device_map="auto",
    trust_remote_code=True
).eval()
```

### ğŸ¤– ModelScope

ModelScope est une plateforme opensource pour Model-as-a-Service (MaaS), qui fournit un service de modÃ¨le flexible et rentable aux dÃ©veloppeurs d'IA. De mÃªme, vous pouvez exÃ©cuter les modÃ¨les avec ModelScope comme indiquÃ© ci-dessous:

```python
from modelscope import AutoModelForCausalLM, AutoTokenizer
from modelscope import GenerationConfig

# Model names: "qwen/Qwen-7B-Chat", "qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("qwen/Qwen-7B-Chat", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚

response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
response, history = model.chat(tokenizer, "æµ™æ±Ÿçš„çœä¼šåœ¨å“ªé‡Œï¼Ÿ", history=history) 
print(response)
response, history = model.chat(tokenizer, "å®ƒæœ‰ä»€ä¹ˆå¥½ç©çš„æ™¯ç‚¹", history=history)
print(response)
```

### InfÃ©rence par lots
Qwen prend en charge l'infÃ©rence par lots. Lorsque flash attention est activÃ©e, l'utilisation de l'infÃ©rence par lots peut entraÃ®ner une accÃ©lÃ©ration de 40 %. Le code d'exemple est prÃ©sentÃ© ci-dessous:
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import GenerationConfig
from qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids

tokenizer = AutoTokenizer.from_pretrained(
    './',
    pad_token='<|extra_0|>',
    eos_token='<|endoftext|>',
    padding_side='left',
    trust_remote_code=True
)
model = AutoModelForCausalLM.from_pretrained(
    './',
    pad_token_id=tokenizer.pad_token_id,
    device_map="auto",
    trust_remote_code=True
).eval()
model.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)

all_raw_text = ["æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚", "ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹", "æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°"]
batch_raw_text = []
for q in all_raw_text:
    raw_text, _ = make_context(
        tokenizer,
        q,
        system="You are a helpful assistant.",
        max_window_size=model.generation_config.max_window_size,
        chat_format=model.generation_config.chat_format,
    )
    batch_raw_text.append(raw_text)

batch_input_ids = tokenizer(batch_raw_text, padding='longest')
batch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)
batch_out_ids = model.generate(
    batch_input_ids,
    return_dict_in_generate=False,
    generation_config=model.generation_config
)
padding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]

batch_response = [
    decode_tokens(
        batch_out_ids[i][padding_lens[i]:],
        tokenizer,
        raw_text_len=len(batch_raw_text[i]),
        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),
        chat_format="chatml",
        verbose=False,
        errors='replace'
    ) for i in range(len(all_raw_text))
]
print(batch_response)

response, _ = model.chat(tokenizer, "æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚", history=None)
print(response)

response, _ = model.chat(tokenizer, "ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹", history=None)
print(response)

response, _ = model.chat(tokenizer, "æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°", history=None)
print(response)
```

### CPU

Pour dÃ©ployer nos modÃ¨les sur CPU, nous vous conseillons vivement d'utiliser [qwen.cpp](https://github.com/QwenLM/qwen.cpp), qui est une implÃ©mentation purement C++ de Qwen et de tiktoken. Consultez le repo pour plus de dÃ©tails!

Il est simple d'exÃ©cuter directement le modÃ¨le sur le CPU, ce qui nÃ©cessite la spÃ©cification de votre appareil:

```python
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
```

Cependant, il est probable que vous souffriez d'une efficacitÃ© d'infÃ©rence extrÃªmement faible.

### Plusieurs GPU

Si vous souffrez d'un manque de mÃ©moire GPU et que vous souhaitez exÃ©cuter le modÃ¨le sur plus d'un GPU, vous pouvez utiliser directement la mÃ©thode de chargement par dÃ©faut, qui est maintenant supportÃ©e par Transformers. La mÃ©thode prÃ©cÃ©dente basÃ©e sur `utils.py` est obsolÃ¨te.

Cependant, bien que cette mÃ©thode soit simple, l'efficacitÃ© du parallÃ©lisme natif du pipeline est faible. Nous vous conseillons d'utiliser vLLM avec FastChat et de lire la section relative au dÃ©ploiement.
<br><br>

## Quantization

### GPTQ

Nous proposons une solution basÃ©e sur [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), et publions les modÃ¨les quantifiÃ©s Int4, qui permettent d'obtenir des effets de modÃ¨le presque sans perte mais des performances amÃ©liorÃ©es en termes de coÃ»ts de mÃ©moire et de vitesse d'infÃ©rence.

Nous dÃ©montrons ici comment utiliser les modÃ¨les quantifiÃ©s que nous fournissons pour l'infÃ©rence. Avant de commencer, assurez-vous que vous rÃ©pondez aux exigences d'auto-gptq (par exemple, torch 2.0 et plus, transformers 4.32.0 et plus, etc.) et installez les paquets requis:

```bash
pip install auto-gptq optimum
```

Si vous rencontrez des problÃ¨mes pour installer `auto-gptq`, nous vous conseillons de consulter le [repo](https://github.com/PanQiWei/AutoGPTQ) officiel pour trouver une roue.

Vous pouvez ensuite charger facilement le modÃ¨le quantifiÃ© et lancer l'infÃ©rence comme d'habitude:

```python
# Model names: "Qwen/Qwen-7B-Chat-Int4", "Qwen/Qwen-14B-Chat-Int4"
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat-Int4",
    device_map="auto",
    trust_remote_code=True
).eval()
response, history = model.chat(tokenizer, "Hi", history=None)
```

Nous illustrons les performances des modÃ¨les BF16, Int8 et Int4 sur le benchmark, et nous constatons que le modÃ¨le quantifiÃ© ne souffre pas d'une dÃ©gradation significative des performances. Les rÃ©sultats sont prÃ©sentÃ©s ci-dessous:

| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |
|----------------------|:----:|:-----------:|:-----:|:---------:|
| Qwen-7B-Chat (BF16)  | 55.8 |    59.7     | 50.3  |   37.2    |
| Qwen-7B-Chat (Int8)  | 55.4 |    59.4     | 48.3  |   34.8    |
| Qwen-7B-Chat (Int4)  | 55.1 |    59.2     | 49.7  |   29.9    |
| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 60.1  |   43.9    |
| Qwen-14B-Chat (Int8) | 63.6 |    68.6     | 60.0	 |   48.2    |
| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |

### Quantization du cache KV

Attention Le cache KV peut Ãªtre quantifiÃ© et compressÃ© pour le stockage, afin d'obtenir un dÃ©bit d'Ã©chantillonnage plus Ã©levÃ©. Les paramÃ¨tres `use_cache_quantization` et `use_cache_kernel` sont fournis pour contrÃ´ler le comportement de quantification du cache KV
Lorsque `use_cache_quantization=True` et `use_cache_kernel=True`, la quantization de kv-cache est activÃ©e.
La mÃ©thode d'utilisation spÃ©cifique est la suivante:

```python
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat",
     device_map="auto",
     trust_remote_code=True,
     use_cache_quantization=True,
     use_cache_kernel=True,
     use_flash_attn=False
)
```
Attention : Actuellement, la quantization du cache kv et le flash attn ne peuvent pas Ãªtre activÃ©s en mÃªme temps.
Si vous activez la quantification du cache kv et use_flash_attn en mÃªme temps (`use_flash_attn=True`, `use_cache_quantization=True`, `use_cache_kernel=True`), use_flash_attn est dÃ©sactivÃ© par dÃ©faut (`use_flash_attn=false`).

Nous avons vÃ©rifiÃ© que l'utilisation du modÃ¨le int8-kvcache quantifiÃ© ne souffre pas d'une dÃ©gradation significative des performances dans l'Ã©valuation en aval. En outre, nous Ã©valuons ses performances en nous concentrant sur l'empreinte mÃ©moire. 
Le profilage s'exÃ©cute sur un seul GPU A100-SXM4-80G avec PyTorch 2.0.1 et CUDA 11.4. 
Nous utilisons des modÃ¨les BF16, et gÃ©nÃ©rons 1024 tokens (seq-length=1024) par dÃ©faut, et oom indique qu'il n'y a plus de mÃ©moire.

Lorsque la quantization de kv-cache est activÃ©e, nous pouvons utiliser une taille de lot (bs) plus importante.

| USE KVCache |  bs=1  |  bs=4  | bs=16  | bs=32  | bs=64  | bs=100 |
|-------------|:------:|:------:|:------:|:------:|:------:|:------:|
| no          | 16.3GB | 24.1GB | 31.7GB | 48.7GB |  oom   |  oom   |
| yes         | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB |

Lorsque la quantification de kv-cache est activÃ©e, le modÃ¨le peut Ã©conomiser plus de mÃ©moire lorsqu'il gÃ©nÃ¨re des sÃ©quences plus longues (sl, nombre de jetons gÃ©nÃ©rÃ©s) lors de l'infÃ©rence.

| USE KVCache | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |
|-------------|:------:|:-------:|:-------:|:-------:|:-------:|
| no          | 15.2GB | 16.3GB  | 17.6GB  | 19.5GB  | 23.2GB  |
| yes         |  15GB  | 15.5GB  | 15.8GB  | 16.6GB  | 17.6GB  |

Le modÃ¨le qui active la quantification du kv-cache convertit le format du layer-past de float Ã  int8, tandis que le layer-past quantifiÃ© stocke Ã©galement les paramÃ¨tres de quantification de la valeur actuelle.
Les Ã©tapes spÃ©cifiques sont les suivantes :

1. Quantifier clÃ©/valeur
```
    qv,scale,zero_point=quantize_cache_v(v)
```
2. Stocker dans layer_past

Following is the format of quantized layer_past:
```
    layer_past=((q_key,key_scale,key_zero_point),
                (q_value,value_scale,value_zero_point))
```
Format de base de layer_past:
```
    layer_past=(key,value)
```
Si vous souhaitez utiliser l'attention KV qui est quantifiÃ©e, vous pouvez utiliser l'opÃ©ration de dÃ©quantification pour convertir la clÃ©/valeur int8 en format float comme suit 
vous pouvez utiliser l'opÃ©ration de dÃ©quantification pour reconvertir la clÃ©/valeur int8 au format float comme suit :
```
    v=dequantize_cache_torch(qv,scale,zero_point)
```
<br>


## Performance de l'infÃ©rence

Cette section fournit les statistiques de vitesse et de mÃ©moire des modÃ¨les dans diffÃ©rentes prÃ©cisions. Le profilage de la vitesse et de la mÃ©moire est effectuÃ© Ã  l'aide de [ce script](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py).

### Vitesse

Nous avons mesurÃ© la vitesse moyenne d'infÃ©rence (jetons/s) pour la gÃ©nÃ©ration de 2048 et 8192 jetons avec les modÃ¨les dans la prÃ©cision de BF16, Int8, et Int4 sous la condition d'utiliser l'attention flash v1, v2, ou de ne pas l'utiliser.

<table>
    <tr>
      <th rowspan="2">Model Size</th><th rowspan="2">Precision</th><th rowspan="2">FlashAttn</th><th colspan="2" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">2048</th><th align="center">8192</th>
    </tr>
    </tr>
    </tr>
    <tr>
        <th rowspan="9">7B</th><td align="center" rowspan="3">BF16</td><td align="center">v2</td><td align="center">40.93</td><td align="center">36.14</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">40.75</td><td align="center">35.34
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">37.55</td><td align="center">33.56
    </tr>
    <tr>
        <td align="center" rowspan="3">Int8</td><td align="center">v2</td><td align="center">37.47</td><td align="center">32.54</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">37.51</td><td align="center">32.39
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">37.84</td><td align="center">32.65
    </tr>
    <tr>
        <td align="center" rowspan="3">Int4</td><td align="center">v2</td><td align="center">50.09</td><td align="center">38.61</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">45.98</td><td align="center">36.47
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">48.12</td><td align="center">36.70
    </tr>
    <tr>
        <th rowspan="9">14B</th><td align="center" rowspan="3">BF16</td><td align="center">v2</td><td align="center">32.88</td><td align="center">24.87</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">32.76</td><td align="center">28.89
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">29.32</td><td align="center">22.91
    </tr>
    <tr>
        <td align="center" rowspan="3">Int8</td><td align="center">v2</td><td align="center">29.28</td><td align="center">24.22</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">28.31</td><td align="center">23.87
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">31.12</td><td align="center">24.60
    </tr>
    <tr>
        <td align="center" rowspan="3">Int4</td><td align="center">v2</td><td align="center">38.72</td><td align="center">27.33</td>
    </tr>
    <tr>
        <td align="center">v1</td><td align="center">37.81</td><td align="center">26.46
    </tr>
    <tr>
        <td align="center">Disabled</td><td align="center">37.65</td><td align="center">26.00
    </tr>
</table>


En dÃ©tail, le profilage consiste Ã  encoder 2048 jetons et Ã  gÃ©nÃ©rer 8192 nouveaux jetons. Le profilage s'exÃ©cute sur un seul GPU A100-SXM4-80G avec PyTorch 2.0.1 et CUDA 11.8. La vitesse d'infÃ©rence est calculÃ©e en moyenne sur les jetons encodÃ©s et gÃ©nÃ©rÃ©s.

Note : La vitesse de gÃ©nÃ©ration des modÃ¨les Int4/Int8 mentionnÃ©s ci-dessus est fournie par la bibliothÃ¨que autogptq. La vitesse actuelle du modÃ¨le chargÃ© Ã  l'aide de "AutoModelForCausalLM.from_pretrained" sera environ 20% plus lente. Nous avons signalÃ© ce problÃ¨me Ã  l'Ã©quipe HuggingFace et nous le mettrons Ã  jour rapidement si une solution est disponible.

### Utilisation de la mÃ©moire du GPU

Nous avons Ã©galement Ã©tabli le profil de l'utilisation maximale de la mÃ©moire du GPU pour l'encodage de 2048 jetons en tant que contexte (et la gÃ©nÃ©ration d'un seul jeton) et la gÃ©nÃ©ration de 8192 jetons (avec un seul jeton en tant que contexte) sous BF16, Int8 ou Int4 niveau de quantization, respectivement. Les rÃ©sultats (GB) sont prÃ©sentÃ©s ci-dessous.

<table>
    <tr>
      <th rowspan="2">Model Size</th><th rowspan="2">Precision</th><th colspan="2" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">2048</th><th align="center">8192</th>
    </tr>
    </tr>
    </tr>
    <tr>
        <th rowspan="3">7B</th><td align="center">BF16</td><td align="center">16.99</td><td align="center">22.53</td>
    </tr>
    <tr>
        <td align="center">Int8</td><td align="center">11.20</td><td align="center">16.62
    </tr>
    <tr>
        <td align="center">Int4</td><td align="center">8.21</td><td align="center">13.63</td>
    </tr>
    <tr>
        <th rowspan="3">14B</th><td align="center">BF16</td><td align="center">30.15</td><td align="center">38.94</td>
    </tr>
    <tr>
        <td align="center">Int8</td><td align="center">18.81</td><td align="center">27.54
    </tr>
    <tr>
        <td align="center">Int4</td><td align="center">13.01</td><td align="center">21.79</td>
    </tr>
</table>


<br>


## Finetuning

### Utilisation
Nous fournissons maintenant le script d'entraÃ®nement officiel, `finetune.py`, pour que les utilisateurs puissent ajuster le modÃ¨le prÃ©-entraÃ®nÃ© pour les applications en aval de maniÃ¨re simple. De plus, nous fournissons des scripts shell pour lancer le finetune sans soucis. Ce script prend en charge l'entraÃ®nement avec [DeepSpeed](https://github.com/microsoft/DeepSpeed) et [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/). Les scripts que nous fournissons utilisent DeepSpeed (Note : il peut y avoir des conflits avec la derniÃ¨re version de pydantic) et Peft. Vous pouvez les installer en procÃ©dant comme suit:
```bash
pip install peft deepspeed
```

Pour prÃ©parer vos donnÃ©es d'entraÃ®nement, vous devez rassembler tous les Ã©chantillons dans une liste et l'enregistrer dans un fichier json. Chaque Ã©chantillon est un dictionnaire composÃ© d'un identifiant et d'une liste de conversation. Voici un exemple simple de liste avec 1 Ã©chantillon:
```json
[
  {
    "id": "identity_0",
    "conversations": [
      {
        "from": "user",
        "value": "ä½ å¥½"
      },
      {
        "from": "assistant",
        "value": "æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚"
      }
    ]
  }
]
```

AprÃ¨s la prÃ©paration des donnÃ©es, vous pouvez utiliser les scripts shell fournis pour lancer le finetuning. N'oubliez pas de spÃ©cifier le chemin d'accÃ¨s au fichier de donnÃ©es, `$DATA`.

Les scripts de finetuning vous permettent d'effectuer les opÃ©rations suivantes
- Finetuning de tous les paramÃ¨tres
- LoRA
- Q-LoRA

Le finetuning de tous les paramÃ¨tres nÃ©cessite la mise Ã  jour de tous les paramÃ¨tres au cours de l'ensemble du processus de formation. Pour lancer votre formation, exÃ©cutez le script suivant:

```bash
# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.
sh finetune/finetune_ds.sh
```

N'oubliez pas de spÃ©cifier le nom ou le chemin d'accÃ¨s au modÃ¨le, le chemin d'accÃ¨s aux donnÃ©es, ainsi que le rÃ©pertoire de sortie dans les scripts shell. Une autre chose Ã  noter est que nous utilisons DeepSpeed ZeRO 3 dans ce script. Si vous voulez faire des changements, il suffit de supprimer l'argument `--deepspeed` ou de faire des changements dans le fichier json de configuration de DeepSpeed en fonction de vos besoins. De plus, ce script supporte l'entraÃ®nement en prÃ©cision mixte, et donc vous pouvez utiliser `--bf16 True` ou `--fp16 True`. N'oubliez pas d'utiliser DeepSpeed lorsque vous utilisez fp16 en raison de l'entraÃ®nement de prÃ©cision mixte. Empiriquement, nous vous conseillons d'utiliser bf16 pour rendre votre apprentissage cohÃ©rent avec notre prÃ©-entraÃ®nement et notre alignement si votre machine supporte bf16, et nous l'utilisons donc par dÃ©faut.

Pour exÃ©cuter LoRA, utilisez un autre script Ã  exÃ©cuter comme indiquÃ© ci-dessous. Avant de commencer, assurez-vous que vous avez installÃ© `peft`. Vous devez spÃ©cifier les chemins d'accÃ¨s Ã  votre modÃ¨le, Ã  vos donnÃ©es et Ã  vos rÃ©sultats. Nous vous conseillons d'utiliser des chemins absolus pour votre modÃ¨le prÃ©-entraÃ®nÃ©. En effet, LoRA ne sauvegarde que l'adaptateur et le chemin absolu dans le fichier json de configuration de l'adaptateur est utilisÃ© pour trouver le modÃ¨le prÃ©-entraÃ®nÃ© Ã  charger. De plus, ce script supporte Ã  la fois bf16 et fp16.

```bash
# Single GPU training
sh finetune/finetune_lora_single_gpu.sh
# Distributed training
sh finetune/finetune_lora_ds.sh
```

Par rapport au finetuning de tous les paramÃ¨tres, LoRA ([paper](https://arxiv.org/abs/2106.09685)) ne met Ã  jour que les paramÃ¨tres des couches d'adaptateurs, tout en gelant les couches originales du grand modÃ¨le de langage. Cela permet de rÃ©duire considÃ©rablement les coÃ»ts de mÃ©moire et donc les coÃ»ts de calcul.

Notez que si vous utilisez LoRA pour affiner le modÃ¨le de langue, par exemple Qwen-7B, au lieu des modÃ¨les de chat, par exemple Qwen-7B-Chat, le script change automatiquement les embedding et la couche de sortie en tant que paramÃ¨tres entraÃ®nables. En effet, le modÃ¨le de langue n'a aucune connaissance des jetons spÃ©ciaux apportÃ©s par le format ChatML. Ces couches doivent donc Ãªtre mises Ã  jour pour que le modÃ¨le comprenne et prÃ©dise les jetons. En d'autres termes, si votre entraÃ®nement apporte des tokens spÃ©ciaux dans LoRA, vous devez dÃ©finir les couches comme des paramÃ¨tres entraÃ®nables en dÃ©finissant `modules_to_save` Ã  l'intÃ©rieur du code. De plus, si ces paramÃ¨tres sont entraÃ®nables, il n'est pas possible d'utiliser ZeRO 3, et c'est pourquoi nous utilisons ZeRO 2 par dÃ©faut dans le script. Si vous n'avez pas de nouveaux paramÃ¨tres entraÃ®nables, vous pouvez passer Ã  ZeRO 3 en modifiant le fichier de configuration de DeepSpeed. En outre, nous constatons qu'il existe un Ã©cart important entre l'empreinte mÃ©moire de LoRA avec et sans ces paramÃ¨tres d'entraÃ®nement. Par consÃ©quent, si vous avez des problÃ¨mes de mÃ©moire, nous vous conseillons d'affiner les modÃ¨les de chat de LoRA. Consultez le profil ci-dessous pour plus d'informations.

Si vous souffrez toujours d'un manque de mÃ©moire, vous pouvez envisager Q-LoRA ([paper](https://arxiv.org/abs/2305.14314)), qui utilise le modÃ¨le de langage quantifiÃ© et d'autres techniques telles que l'attention paginÃ©e pour rÃ©duire encore les coÃ»ts de mÃ©moire.

Note : pour exÃ©cuter l'entraÃ®nement Q-LoRA sur un seul GPU, vous pouvez avoir besoin d'installer `mpi4py` via `pip` ou `conda`.

Pour lancer Q-LoRA, exÃ©cutez directement le script suivant:

```bash
# Single GPU training
sh finetune/finetune_qlora_single_gpu.sh
# Distributed training
sh finetune/finetune_qlora_ds.sh
```

Pour Q-LoRA, nous vous conseillons de charger le modÃ¨le quantifiÃ© que nous fournissons, par exemple Qwen-7B-Chat-Int4. Vous **NE DEVRIEZ PAS** utiliser les modÃ¨les bf16. Contrairement au finetuning de tous les paramÃ¨tres et Ã  la LoRA, seul le modÃ¨le fp16 est pris en charge pour la Q-LoRA. Pour l'entraÃ®nement sur un seul GPU, nous devons utiliser DeepSpeed pour l'entraÃ®nement en prÃ©cision mixte en raison de notre observation des erreurs causÃ©es par torch amp. En outre, pour Q-LoRA, les problÃ¨mes avec les jetons spÃ©ciaux dans LoRA existent toujours. Cependant, comme nous ne fournissons que les modÃ¨les Int4 pour les modÃ¨les de chat, ce qui signifie que le modÃ¨le de langage a appris les tokens spÃ©ciaux du format ChatML, vous n'avez pas Ã  vous soucier des couches. Notez que les couches du modÃ¨le Int4 ne doivent pas Ãªtre entraÃ®nables, et donc si vous introduisez des tokens spÃ©ciaux dans votre entraÃ®nement, Q-LoRA risque de ne pas fonctionner.

Contrairement au finetuning des paramÃ¨tres complets, l'entraÃ®nement de LoRA et de Q-LoRA n'enregistre que les paramÃ¨tres de l'adaptateur. Supposons que votre entraÃ®nement commence Ã  partir de Qwen-7B, vous pouvez charger le modÃ¨le finalisÃ© pour l'infÃ©rence comme indiquÃ© ci-dessous:

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()
```

Si vous souhaitez fusionner les adaptateurs et enregistrer le modÃ¨le affinÃ© en tant que modÃ¨le autonome (vous ne pouvez le faire qu'avec LoRA, et vous **NE POUVEZ PAS** fusionner les paramÃ¨tres de Q-LoRA), vous pouvez exÃ©cuter les codes suivants:

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()

merged_model = model.merge_and_unload()
# max_shard_size and safe serialization are not necessary. 
# They respectively work for sharding checkpoint and save the model to safetensors
merged_model.save_pretrained(new_model_directory, max_shard_size="2048MB", safe_serialization=True)
```

Note : Pour l'entraÃ®nement multi-GPU, vous devez spÃ©cifier les hyperparamÃ¨tres appropriÃ©s pour l'entraÃ®nement distribuÃ© en fonction de votre machine. De plus, nous vous conseillons de spÃ©cifier votre longueur maximale de sÃ©quence avec l'argument `--model_max_length`, en fonction de votre considÃ©ration des donnÃ©es, de l'empreinte mÃ©moire, et de la vitesse d'apprentissage.

### Profilage de la mÃ©moire et de la vitesse
Nous profilons la mÃ©moire du GPU et la vitesse d'apprentissage de LoRA (LoRA (emb) se rÃ©fÃ¨re Ã  l'apprentissage de l'embedding et la couche de sortie, tandis que LoRA n'a pas de couche d'intÃ©gration et de sortie pouvant Ãªtre entraÃ®nÃ©e) et de Q-LoRA dans la configuration de l'apprentissage sur un seul GPU. Dans ce test, nous expÃ©rimentons sur un seul GPU A100-SXM4-80G, et nous utilisons CUDA 11.8 et Pytorch 2.0. Flash attention 2 est appliquÃ©. Nous utilisons uniformÃ©ment une taille de lot de 1 et une accumulation de gradient de 8. Nous profilons la mÃ©moire (GB) et la vitesse (s/iter) des entrÃ©es de diffÃ©rentes longueurs, Ã  savoir 256, 512, 1024, 2048, 4096, et 8192. Nous prÃ©sentons Ã©galement les statistiques du finetuning de tous les paramÃ¨tres avec Qwen-7B sur 2 GPU A100. Nous ne prÃ©sentons que les statistiques de 256, 512 et 1024 jetons en raison de la limitation de la mÃ©moire du GPU. Les statistiques sont listÃ©es ci-dessous :

<table>
    <tr>
      <th rowspan="2">Model Size</th><th rowspan="2">Method</th><th colspan="6" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">256</th><th align="center">512</th><th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th>
    </tr>
    </tr>
    </tr>
    <tr>
        <th rowspan="4">7B</th><td>LoRA</td><td align="center">20.1G / 1.2s/it</td><td align="center">20.4G / 1.5s/it</td><td align="center">21.5G / 2.8s/it</td><td align="center">23.8G / 5.2s/it</td><td align="center">29.7G / 10.1s/it</td><td align="center">36.6G / 21.3s/it</td>
    </tr>
    <tr>
        <td>LoRA (emb)</td><td align="center">33.7G / 1.4s/it</td><td align="center">34.1G / 1.6s/it</td><td align="center">35.2G / 2.9s/it</td><td align="center">35.1G / 5.3s/it</td><td align="center">39.2G / 10.3s/it</td><td align="center">48.5G / 21.7s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td align="center">11.5G / 3.0s/it</td><td align="center">11.5G / 3.0s/it</td><td align="center">12.3G / 3.5s/it</td><td align="center">13.9G / 7.0s/it</td><td align="center">16.9G / 11.6s/it</td><td align="center">23.5G / 22.3s/it</td>
    </tr>
    <tr>
        <td>Full-parameter</td><td align="center">139.2G / 4.0s/it</td><td align="center">148.0G / 4.0s/it</td><td align="center">162.0G / 4.5s/it</td><td align="center">-</td><td align="center">-</td><td align="center">-</td>
    </tr>
    <tr>
        <th rowspan="3">14B</th><td>LoRA</td><td align="center">34.6G / 1.6s/it</td><td align="center">35.1G / 2.4s/it</td><td align="center">35.3G / 4.4s/it</td><td align="center">37.4G / 8.4s/it</td><td align="center">42.5G / 17.0s/it</td><td align="center">55.2G / 36.0s/it</td>
    </tr>
    <tr>
        <td>LoRA (emb)</td><td align="center">51.2 / 1.7s/it</td><td align="center">51.1G / 2.6s/it</td><td align="center">51.5G / 4.6s/it</td><td align="center">54.1G / 8.6s/it</td><td align="center">56.8G / 17.2s/it</td><td align="center">67.7G / 36.3s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td align="center">18.7G / 5.3s/it</td><td align="center">18.4G / 6.3s/it</td><td align="center">18.9G / 8.2s/it</td><td align="center">19.9G / 11.8s/it</td><td align="center">23.0G / 20.1s/it</td><td align="center">27.9G / 38.3s/it</td>
    </tr>
</table>
<br>

## DÃ©ploiement

### vLLM 
Pour le dÃ©ploiement et l'infÃ©rence rapide, nous suggÃ©rons d'utiliser vLLM avec FastChat. Installez d'abord les paquets:
```bash
pip install vllm
pip install "fschat[model_worker,webui]"
```
Ou vous pouvez les installer Ã  partir des sources par `git clone` et `pip install -e .`. Nous vous conseillons de lire leurs documents si vous rencontrez des problÃ¨mes lors de l'installation.

Pour faire fonctionner Qwen avec vLLM et FastChat, vous devez d'abord lancer un contrÃ´leur par:
```bash
python -m fastchat.serve.controller
```

Ensuite, vous pouvez lancer le travailleur de modÃ¨le, ce qui signifie charger votre modÃ¨le pour l'infÃ©rence. Pour l'infÃ©rence sur un seul GPU, vous pouvez directement lancer:
```bash
python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code
```
Cependant, si vous souhaitez exÃ©cuter le modÃ¨le sur plusieurs GPU pour une infÃ©rence plus rapide ou une mÃ©moire plus importante, vous pouvez utiliser le parallÃ©lisme tensoriel pris en charge par vLLM. Supposons que vous exÃ©cutiez le modÃ¨le sur 4 GPU, la commande est prÃ©sentÃ©e ci-dessous:
```bash
python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4
```

AprÃ¨s avoir lancÃ© votre model worker, vous pouvez lancer une dÃ©mo web ou une API OpenAI comme vous le souhaitez. Pour la dÃ©mo web, exÃ©cutez la commande suivante:
```bash
python -m fastchat.serve.gradio_web_server
```
Pour l'API OpenAI, consultez d'abord la documentation de notre API OpenAI pour l'installation. ExÃ©cutez ensuite la commande:
```bash
python -m fastchat.serve.openai_api_server --host localhost --port 8000
```
<br>

## DÃ©mo

### Interface Web

Nous fournissons du code pour que les utilisateurs puissent construire une dÃ©mo d'interface web (merci Ã  @wysaid). Avant de commencer, assurez-vous d'installer les paquets suivants:

```
pip install -r requirements_web_demo.txt
```

ExÃ©cutez ensuite la commande ci-dessous et cliquez sur le lien gÃ©nÃ©rÃ©:

```bash
python web_demo.py
```

<p align="center">
    <br>
    <img src="assets/web_demo.gif" width="600" />
    <br>
<p>

### DÃ©mo CLI

Nous fournissons un exemple de dÃ©monstration CLI dans `cli_demo.py`, qui prend en charge la sortie en continu pour la gÃ©nÃ©ration. Les utilisateurs peuvent interagir avec Qwen-7B-Chat en saisissant des invites, et le modÃ¨le renvoie les sorties du modÃ¨le en mode streaming. ExÃ©cutez la commande ci-dessous:

```bash
python cli_demo.py
```

<p align="center">
    <br>
    <img src="assets/cli_demo.gif" width="600" />
    <br>
<p>
<br>

## API

Le moyen le plus simple d'utiliser Qwen via les API est le service API DashScope via Alibaba Cloud. Nous prÃ©sentons une introduction Ã  l'utilisation. De plus, nous fournissons un script pour vous permettre de dÃ©ployer une API de type OpenAI sur vos propres serveurs.

### DashScope
DashScope est le service API de grands modÃ¨les linguistiques fourni par Alibaba Cloud, qui prend dÃ©sormais en charge Qwen. Notez que les modÃ¨les derriÃ¨re DashScope sont des versions internes temporairement sans dÃ©tails fournis. Les services comprennent `qwen-turbo` et `qwen-plus`, le premier fonctionnant plus rapidement et le second atteignant de meilleures performances. Pour plus d'informations, consultez la documentation [ici] (https://dashscope.aliyun.com).

Veuillez vous rendre sur le site officiel [lien](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn) pour crÃ©er un compte DashScope et obtenir la clÃ© API (AK). Nous recommandons de dÃ©finir l'AK Ã  l'aide d'une variable d'environnement:
```bash
export DASHSCOPE_API_KEY="YOUR_DASHSCOPE_API_KEY"
```
Installez ensuite les paquets et cliquez sur [ici](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk) pour obtenir la documentation. Si vous utilisez Python, vous pouvez installer DashScope avec pip:
```bash
pip install dashscope
```
Si vous utilisez JAVA SDK, vous pouvez l'installer de cette maniÃ¨re:
```xml
<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->
<dependency>
    <groupId>com.alibaba</groupId>
    <artifactId>dashscope-sdk-java</artifactId>
    <version>the-latest-version</version>
</dependency>
```
La maniÃ¨re la plus simple d'utiliser DashScope est l'utilisation de messages, qui est similaire Ã  l'API OpenAI. L'exemple est prÃ©sentÃ© ci-dessous:
```python
import random
from http import HTTPStatus
from dashscope import Generation


def call_with_messages():
    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},
                {'role': 'user', 'content': 'å¦‚ä½•åšè¥¿çº¢æŸ¿é¸¡è›‹ï¼Ÿ'}]
    gen = Generation()
    response = gen.call(
        Generation.Models.qwen_turbo,
        messages=messages,
        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set
        result_format='message',  # set the result to be "message" format.
    )
    return response


if __name__ == '__main__':
    response = call_with_messages()
    if response.status_code == HTTPStatus.OK:
        print(response)
    else:
        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (
            response.request_id, response.status_code,
            response.code, response.message
        ))
```
Pour d'autres utilisations, veuillez consulter le site web officiel pour plus de dÃ©tails.

### API OpenAI

Nous fournissons des mÃ©thodes pour dÃ©ployer une API locale basÃ©e sur l'API OpenAI (merci Ã  @hanpenggit). Avant de commencer, installez les paquets nÃ©cessaires:

```bash
pip install fastapi uvicorn openai "pydantic>=2.3.0" sse_starlette
```

ExÃ©cutez ensuite la commande pour dÃ©ployer votre API:

```bash
python openai_api.py
```

Vous pouvez modifier vos arguments, par exemple, `-c` pour le nom ou le chemin du poids, `--cpu-only` pour le dÃ©ploiement CPU, etc. Si vous rencontrez des problÃ¨mes lors du lancement du dÃ©ploiement de l'API, la mise Ã  jour des paquets vers la derniÃ¨re version peut probablement les rÃ©soudre.

L'utilisation de l'API est simple. Voir l'exemple ci-dessous:

```python
import openai
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "none"

# create a request activating streaming response
for chunk in openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ],
    stream=True 
    # Specifying stop words in streaming output format is not yet supported and is under development.
):
    if hasattr(chunk.choices[0].delta, "content"):
        print(chunk.choices[0].delta.content, end="", flush=True)

# create a request not activating streaming response
response = openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ],
    stream=False,
    stop=[] # You can add custom stop words here, e.g., stop=["Observation:"] for ReAct prompting.
)
print(response.choices[0].message.content)
```

<p align="center">
    <br>
    <img src="assets/openai_api.gif" width="600" />
    <br>
<p>

**Function calling** est aussi supportÃ© (mais seulement quand `stream=False` pour le moment). Voir [l'exemple d'utilisation](examples/function_call_examples.py) ici.
<br><br>


## Utilisation des outils

Qwen-Chat a Ã©tÃ© optimisÃ© pour l'utilisation d'outils et les capacitÃ©s d'appel de fonctions. Les utilisateurs peuvent dÃ©velopper des agents, des applications LangChain, et mÃªme augmenter Qwen avec un Code Interpreter.

Nous fournissons une documentation sur la maniÃ¨re d'implÃ©menter les appels d'outils basÃ©s sur le principe de ReAct Prompting, veuillez vous rÃ©fÃ©rer Ã  [l'exemple ReAct](examples/react_prompt.md). Sur la base de ce principe, nous fournissons un support pour function calling dans [openai_api.py](openai_api.py).

Nous avons testÃ© les capacitÃ©s d'appel d'outil du modÃ¨le sur notre benchmark d'Ã©valuation chinois Ã  source ouverte et nous avons constatÃ© que Qwen-Chat obtient systÃ©matiquement de bons rÃ©sultats:

<table>
    <tr>
        <th colspan="4" align="center">Chinese Tool-Use Benchmark</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selection (Acc.â†‘)</th><th align="center">Tool Input (Rouge-Lâ†‘)</th><th align="center">False Positive Errorâ†“</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">95%</td><td align="center">0.90</td><td align="center">15.0%</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">85%</td><td align="center">0.88</td><td align="center">75.0%</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">98%</td><td align="center">0.91</td><td align="center">7.3%</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">98%</td><td align="center">0.93</td><td align="center">2.4%</td>
    </tr>
</table>

Pour Ã©valuer la capacitÃ© de Qwen Ã  utiliser l'interprÃ©teur de code Python pour des tÃ¢ches telles que la rÃ©solution de problÃ¨mes mathÃ©matiques, la visualisation de donnÃ©es et d'autres tÃ¢ches gÃ©nÃ©rales telles que la manipulation de fichiers et l'exploration du Web, nous avons crÃ©Ã© et mis en libre accÃ¨s un test de rÃ©fÃ©rence spÃ©cialement conÃ§u pour Ã©valuer ces capacitÃ©s. Vous pouvez trouver le benchmark sur ce [lien](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark).

Nous avons observÃ© que Qwen est performant en termes d'exÃ©cutabilitÃ© du code et de prÃ©cision des rÃ©sultats lors de la gÃ©nÃ©ration du code:

<table>
    <tr>
        <th colspan="4" align="center">Executable Rate of Generated Code (%)</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Mathâ†‘</th><th align="center">Visualizationâ†‘</th><th align="center">Generalâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">91.9</td><td align="center">85.9</td><td align="center">82.8</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">89.2</td><td align="center">65.0</td><td align="center">74.1</td>
    </tr>
    <tr>
        <td>LLaMA2-7B-Chat</td>
        <td align="center">41.9</td>
        <td align="center">33.1</td>
        <td align="center">24.1 </td>
    </tr>
    <tr>
        <td>LLaMA2-13B-Chat</td>
        <td align="center">50.0</td>
        <td align="center">40.5</td>
        <td align="center">48.3 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-7B-Instruct</td>
        <td align="center">85.1</td>
        <td align="center">54.0</td>
        <td align="center">70.7 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-13B-Instruct</td>
        <td align="center">93.2</td>
        <td align="center">55.8</td>
        <td align="center">74.1 </td>
    </tr>
    <tr>
        <td>InternLM-7B-Chat-v1.1</td>
        <td align="center">78.4</td>
        <td align="center">44.2</td>
        <td align="center">62.1 </td>
    </tr>
    <tr>
        <td>InternLM-20B-Chat</td>
        <td align="center">70.3</td>
        <td align="center">44.2</td>
        <td align="center">65.5 </td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td>
        <td align="center">82.4</td>
        <td align="center">64.4</td>
        <td align="center">67.2 </td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td>
        <td align="center">89.2</td>
        <td align="center">84.1</td>
        <td align="center">65.5</td>
    </tr>
</table>

<table>
    <tr>
        <th colspan="4" align="center">Accuracy of Code Execution Results (%)</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Mathâ†‘</th><th align="center">Visualization-Hardâ†‘</th><th align="center">Visualization-Easyâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">82.8</td><td align="center">66.7</td><td align="center">60.8</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">47.3</td><td align="center">33.3</td><td align="center">55.7</td>
    </tr>
    <tr>
        <td>LLaMA2-7B-Chat</td>
        <td align="center">3.9</td>
        <td align="center">14.3</td>
        <td align="center">39.2 </td>
    </tr>
    <tr>
        <td>LLaMA2-13B-Chat</td>
        <td align="center">8.3</td>
        <td align="center">8.3</td>
        <td align="center">40.5 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-7B-Instruct</td>
        <td align="center">14.3</td>
        <td align="center">26.2</td>
        <td align="center">60.8 </td>
    </tr>
    <tr>
        <td>CodeLLaMA-13B-Instruct</td>
        <td align="center">28.2</td>
        <td align="center">27.4</td>
        <td align="center">62.0 </td>
    </tr>
    <tr>
        <td>InternLM-7B-Chat-v1.1</td>
        <td align="center">28.5</td>
        <td align="center">4.8</td>
        <td align="center">40.5 </td>
    </tr>
    <tr>
        <td>InternLM-20B-Chat</td>
        <td align="center">34.6</td>
        <td align="center">21.4</td>
        <td align="center">45.6 </td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td>
        <td align="center">41.9</td>
        <td align="center">40.5</td>
        <td align="center">54.4 </td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td>
        <td align="center">58.4</td>
        <td align="center">53.6</td>
        <td align="center">59.5</td>
    </tr>
</table>

<p align="center">
    <br>
    <img src="assets/code_interpreter_showcase_001.jpg" />
    <br>
<p>

En outre, nous fournissons Ã©galement des rÃ©sultats expÃ©rimentaux dÃ©montrant que notre modÃ¨le est capable d'agir en tant qu'agent Hugging Face. Pour plus d'informations, veuillez vous rÃ©fÃ©rer Ã  la [documentation de l'exemple](examples/transformers_agent.md). Les performances du modÃ¨le sur l'ensemble des donnÃ©es d'Ã©valuation fournies par Hugging Face sont les suivantes:

<table>
    <tr>
        <th colspan="4" align="center">HuggingFace Agent Benchmark- Run Mode</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selectionâ†‘</th><th align="center">Tool Usedâ†‘</th><th align="center">Codeâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">100</td><td align="center">100</td><td align="center">97.4</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">95.4</td><td align="center">96.3</td><td align="center">87.0</td>
    </tr>
    <tr>
        <td>StarCoder-Base-15B</td><td align="center">86.1</td><td align="center">87.0</td><td align="center">68.9</td>
    </tr>
    <tr>
        <td>StarCoder-15B</td><td align="center">87.0</td><td align="center">88.0</td><td align="center">68.9</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">87.0</td><td align="center">87.0</td><td align="center">71.5</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">93.5</td><td align="center">94.4</td><td align="center">87.0</td>
    </tr>
</table>

<table>
    <tr>
        <th colspan="4" align="center">HuggingFace Agent Benchmark - Chat Mode</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selectionâ†‘</th><th align="center">Tool Usedâ†‘</th><th align="center">Codeâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">98.5</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">97.3</td><td align="center">96.8</td><td align="center">89.6</td>
    </tr>
    <tr>
        <td>StarCoder-Base-15B</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">91.1</td>
    </tr>
    <tr>
        <td>StarCoder-15B</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">89.6</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">94.7</td><td align="center">94.7</td><td align="center">85.1</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">95.5</td>
    </tr>
</table>

<br>

## ComprÃ©hension du contexte long

Pour Ã©tendre la longueur du contexte et briser le goulot d'Ã©tranglement de la longueur de la sÃ©quence d'entraÃ®nement, nous introduisons plusieurs techniques, y compris l'interpolation consciente de NTK, l'attention de fenÃªtre, et l'Ã©chelle d'attention LogN, pour Ã©tendre la longueur du contexte de Qwen-7B/14B de 2k Ã  plus de 8k tokens, et Qwen-7B de 8k Ã  32k tokens. Nous menons des expÃ©riences de modÃ©lisation du langage sur l'ensemble de donnÃ©es arXiv avec l'Ã©valuation PPL et nous constatons que Qwen peut atteindre des performances exceptionnelles dans le scÃ©nario d'un contexte long. Les rÃ©sultats sont prÃ©sentÃ©s ci-dessous :

<table>
    <tr>
        <th rowspan="2">Model</th><th colspan="6" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th><th align="center">16384</th><th align="center">32768</th>
    </tr>
     <tr>
        <td>Qwen-7B (original)</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">39.35</td><td align="center">469.81</td><td align="center">2645.09</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.59</td><td align="center">3.66</td><td align="center">5.71</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.58</td><td align="center">3.56</td><td align="center">4.62</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.58</td><td align="center">3.49</td><td align="center">4.32</td><td align="center">-</td>
    </tr>
    <tr>
    <tr>
        <td>Qwen-7B</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.31</b></td><td align="center">7.27</td><td align="center">181.49</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.33</b></td><td align="center"><b>3.22</b></td><td align="center"><b>3.17</b></td>
    </tr>
    <tr>
        <td>Qwen-14B</td><td align="center"><b>-</b></td><td align="center"><b>3.46</b></td><td align="center">22.79</td><td align="center">334.65</td><td align="center">3168.35</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>-</b></td><td align="center"><b>3.46</b></td><td align="center"><b>3.29</b></td><td align="center"><b>3.18</b></td><td align="center">3.42</td><td align="center">-</td>
    </tr>
</table>


## Tokenizer

Notre tokenizer basÃ© sur tiktoken est diffÃ©rent des autres tokenizers, par exemple le tokenizer sentencepiece. Vous devez faire attention aux tokens spÃ©ciaux, en particulier lors de la mise au point. Pour des informations plus dÃ©taillÃ©es sur le tokenizer et son utilisation dans le cadre du finetuning, veuillez vous rÃ©fÃ©rer Ã  la [documentation](tokenization_note.md).
<br><br>

## Reproduction

Pour reproduire les performances du modÃ¨le sur des ensembles de donnÃ©es de rÃ©fÃ©rence, nous fournissons des scripts permettant de reproduire les rÃ©sultats. Consultez [eval/EVALUATION.md](eval/EVALUATION.md) pour plus d'informations. Notez que la reproduction peut entraÃ®ner de lÃ©gÃ¨res diffÃ©rences par rapport Ã  nos rÃ©sultats.
<br><br>

## FAQ

Si vous rencontrez des problÃ¨mes, veuillez vous rÃ©fÃ©rer Ã  la [FAQ](FAQ.md) et aux problÃ¨mes pour trouver une solution avant de lancer un nouveau problÃ¨me.
<br><br>

## Citation
Si vous trouvez notre travail utile, n'hÃ©sitez pas Ã  nous citer.

```
@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
```
<br>

## Accord de Licence

Les chercheurs et les dÃ©veloppeurs sont libres d'utiliser les codes et les poids des modÃ¨les de Qwen et de Qwen-Chat. Nous autorisons Ã©galement leur utilisation commerciale. Consultez notre licence Ã  [LICENSE](LICENSE) pour plus de dÃ©tails. Si vous avez des exigences en matiÃ¨re d'utilisation commerciale, veuillez remplir le formulaire ([7B](https://dashscope.console.aliyun.com/openModelApply/qianwen), [14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat)) pour en faire la demande.
<br><br>

## Contactez-nous

Si vous souhaitez laisser un message Ã  notre Ã©quipe de recherche ou Ã  notre Ã©quipe produit, rejoignez nos groupes Discord ou WeChat! N'hÃ©sitez pas non plus Ã  envoyer un courriel Ã  qianwen_opensource@alibabacloud.com.

