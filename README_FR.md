<p align="left">
    <a href="README_CN.md">ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>&nbsp ï½œ &nbsp<a href="README_JA.md">æ—¥æœ¬èª</a>&nbsp ï½œ &nbspFranÃ§ais ï½œ &nbsp<a href="README_ES.md">EspaÃ±ol</a>
</p>
<br><br>

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg" width="400"/>
<p>
<br>

<p align="center">
        ğŸ¤— <a href="https://huggingface.co/Qwen">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href="https://modelscope.cn/organization/qwen">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href="https://arxiv.org/abs/2309.16609">Paper</a> &nbsp&nbsp ï½œ &nbsp&nbspğŸ–¥ï¸ <a href="https://modelscope.cn/studios/qwen/Qwen-72B-Chat-Demo/summary">Demo</a>
<br>
<a href="assets/wechat.png">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp&nbsp ï½œ  &nbsp&nbsp<a href="https://dashscope.aliyun.com">API</a> 
</p>
<br><br>

> [!Important]
> Qwen2 est lÃ  ! Vous Ãªtes invitÃ© Ã  suivre [QwenLM/Qwen2](https://github.com/QwenLM/Qwen2) et Ã  partager vos expÃ©riences lÃ -bas.
>
> Ce repo ([QwenLM/Qwen](https://github.com/QwenLM/Qwen)) n'est plus activement maintenu, en raison de diffÃ©rences substantielles dans le code source.
<br>

|     |                                                              Qwen-Chat                                                               |                                                                Qwen-Chat (Int4)                                                                |                        Qwen-Chat (Int8)                         |                                                            Qwen                                                            |
|-----|:------------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------------------------------------------------------:|:---------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------:|
| 1.8B  |  <a href="https://modelscope.cn/models/qwen/Qwen-1_8B-Chat/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-1_8B-Chat">ğŸ¤—</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int4/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int4">ğŸ¤—</a>  | <a href="https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int8/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-1_8B-Chat-Int8">ğŸ¤—</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-1_8B/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-1_8B">ğŸ¤—</a>  |
| 7B  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat">ğŸ¤—</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int4">ğŸ¤—</a>  | <a href="https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B-Chat-Int8">ğŸ¤—</a>  |  <a href="https://modelscope.cn/models/qwen/Qwen-7B/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-7B">ğŸ¤—</a>  |
| 14B | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int4">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B-Chat-Int8">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-14B/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-14B">ğŸ¤—</a> |
| 72B | <a href="https://modelscope.cn/models/qwen/Qwen-72B-Chat/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-72B-Chat">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int4/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-72B-Chat-Int4">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int8/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-72B-Chat-Int8">ğŸ¤—</a> | <a href="https://modelscope.cn/models/qwen/Qwen-72B/summary">ğŸ¤–</a>  <a href="https://huggingface.co/Qwen/Qwen-72B">ğŸ¤—</a> |



Nous ouvrons notre sÃ©rie **Qwen**, qui comprend dÃ©sormais **Qwen**, les modÃ¨les de langue de base, Ã  savoir **Qwen-7B** et **Qwen-14B**, ainsi que **Qwen-Chat**, les modÃ¨les de chat, Ã  savoir **Qwen-7B-Chat** et **Qwen-14B-Chat**. Les liens se trouvent dans le tableau ci-dessus. Cliquez dessus et consultez les fiches des modÃ¨les. Nous publions Ã©galement le **[rapport technique](https://arxiv.org/abs/2309.16609)**. Cliquez sur le lien du document et consultez-le !

En bref, nous disposons de modÃ¨les linguistiques solides, qui ont Ã©tÃ© prÃ©-entraÃ®nÃ© de maniÃ¨re stable pour 3 000 milliards de tokens de donnÃ©es multilingues avec une large couverture de domaines, de langues (en particulier le chinois et l'anglais), etc. Ils sont capables d'atteindre des performances compÃ©titives sur des ensembles de donnÃ©es de rÃ©fÃ©rence. En outre, nous disposons de modÃ¨les de chat alignÃ©s sur les prÃ©fÃ©rences humaines basÃ©es sur SFT et RLHF (pas encore publiÃ©s), qui sont capables de chatter, de crÃ©er du contenu, d'extraire des informations, de rÃ©sumer, de traduire, de coder, de rÃ©soudre des problÃ¨mes mathÃ©matiques, etc. et d'utiliser des outils, de jouer le rÃ´le d'agents ou mÃªme code interpreter, etc.

| ModÃ¨le    | Date de sortie | Longueur maximale | AmÃ©lioration de l'invite du systÃ¨me | # de tokens prÃ©-formÃ©s | Utilisation minimale de la mÃ©moire du GPU pour Finetuning (Q-Lora) | Utilisation minimale du GPU pour gÃ©nÃ©rer 2048 jetons (Int4) | Utilisation des outils |
|:----------|:--------------:|:-----------------:|:-----------------------------------:|:----------------------:|:------------------------------------------------------------------:|:-----------------------------------------------------------:|:----------------------:|
| Qwen-1.8B |    23.11.30    |        32K        |                  âœ…                  |          2.2T          |                               5.8GB                                |                            2.9GB                            |           âœ…            |  
| Qwen-7B   |    23.08.03    |        32K        |                  â                  |          2.4T          |                               11.5GB                               |                            8.2GB                            |           âœ…            |   
| Qwen-14B  |    23.09.25    |        8K         |                  â                  |          3.0T          |                               18.7GB                               |                           13.0GB                            |           âœ…            |
| Qwen-72B  |    23.11.30    |        32K        |                  âœ…                  |          3.0T          |                               61.4GB                               |                           48.9GB                            |           âœ…            |   


Dans la repo, vous pouvez trouver:

* Comment utiliser Qwen, et profiter de l'infÃ©rence simple.
* DÃ©tails sur les modÃ¨les de quantization, y compris GPTQ et la quantization de KV cache.
* Statistiques sur les performances de l'infÃ©rence, y compris la vitesse et la mÃ©moire.
* Tutoriels sur le finetuning, y compris le finetuning de paramÃ¨tres complets, LoRA, et Q-LoRA.
* Instructions de dÃ©ploiement, avec l'exemple de vLLM et FastChat.
* Instructions sur la crÃ©ation de dÃ©mos, y compris WebUI, dÃ©mo CLI, etc.
* Introduction au service API de DashScope, ainsi que les instructions pour construire une API de type OpenAI pour votre modÃ¨le.
* Informations sur Qwen pour l'utilisation d'outils, d'agents et code interpreter.
* Statistiques de l'Ã©valuation de la comprÃ©hension du contexte long.
* Contrat de licence.
* ...

En outre, si vous rencontrez des problÃ¨mes, consultez d'abord la [FAQ](FAQ.md) pour obtenir de l'aide. Vous vous sentez toujours en difficultÃ© ? N'hÃ©sitez pas Ã  nous envoyer des questions (de prÃ©fÃ©rence en anglais pour que plus de gens puissent vous comprendre) ! Si vous souhaitez nous aider, envoyez-nous des demandes d'extension sans hÃ©sitation ! Nous sommes toujours enthousiastes Ã  propos des relations publiques ! 

Vous voulez discuter avec nous ou prendre un cafÃ© avec nous ? Bienvenue sur notre Discord ou WeChat !
<br><br>

## Nouvelles et mises Ã  jour

* 2023.11.30 ğŸ”¥ Nous publions **Qwen-72B** et **Qwen-72B-Chat**, qui sont entraÃ®nÃ©s sur des tokens 3T et prennent en charge 32k contextes, ainsi que **Qwen-1.8B** et **Qwen-1.8B-Chat**, sur ModelScope et Hugging Face. Nous avons Ã©galement renforcÃ© les capacitÃ©s de l'invite systÃ¨me du Qwen-72B-Chat et du Qwen-1.8B-Chat, voir la [documentation d'exemple](examples/system_prompt.md). De plus, nous supportons l'infÃ©rence sur **Ascend 910** et **Hygon DCU**. Consultez `ascend-support` et `dcu-support` pour plus de dÃ©tails.
* 2023.10.17 Nous publions le modÃ¨le quantifiÃ© Int8 **Qwen-7B-Chat-Int8** et **Qwen-14B-Chat-Int8**.
* 2023.9.25 ğŸ”¥ Nous publions **Qwen-14B** et **Qwen-14B-Chat** sur ModelScope et Hugging Face, ainsi que [qwen.cpp](https://github.com/QwenLM/qwen.cpp) et [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent). Les codes et les poids de **Qwen-7B** et **Qwen-7B-Chat** ont Ã©galement Ã©tÃ© mis Ã  jour. **S'IL VOUS PLAÃT, TIREZ LA DERNIÃˆRE VERSION!**
    - Par rapport Ã  **Qwen-7B** (original), **Qwen-7B** utilise davantage de jetons d'entraÃ®nement, passant de 2,2 Ã  2,4T de jetons, tandis que la longueur du contexte passe de 2048 Ã  8192. La connaissance du chinois et la capacitÃ© de codage de **Qwen-7B** ont Ã©tÃ© encore amÃ©liorÃ©es.
* 2023.9.12 Nous prenons dÃ©sormais en charge le finetuning sur les modÃ¨les Qwen-7B, y compris le finetuning de tous les paramÃ¨tres, LoRA et Q-LoRA.
* 2023.8.21 Nous publions le modÃ¨le quantifiÃ© Int4 pour Qwen-7B-Chat, **Qwen-7B-Chat-Int4**, qui nÃ©cessite de faibles coÃ»ts de mÃ©moire mais permet d'amÃ©liorer la vitesse d'infÃ©rence. En outre, il n'y a pas de dÃ©gradation significative des performances lors de l'Ã©valuation de rÃ©fÃ©rence.
* 2023.8.3 Nous publions **Qwen-7B** et **Qwen-7B-Chat** sur ModelScope et Hugging Face. Nous fournissons Ã©galement un mÃ©mo technique pour plus de dÃ©tails sur le modÃ¨le, y compris les dÃ©tails de l'entraÃ®nement et les performances du modÃ¨le.
<br>

## Performance

Les modÃ¨les Qwen surpassent les modÃ¨les de base de taille similaire sur une sÃ©rie de donnÃ©es de rÃ©fÃ©rence, par exemple MMLU, C-Eval, GSM8K, MATH, HumanEval, MBPP, BBH, etc., qui Ã©valuent les capacitÃ©s des modÃ¨les sur la comprÃ©hension du langage naturel, la rÃ©solution de problÃ¨mes mathÃ©matiques, le codage, etc. Qwen-72B obtient de meilleures performances que LLaMA2-70B dans toutes les tÃ¢ches et surpasse GPT-3.5 dans 7 tÃ¢ches sur 10.

<p align="left">
    <img src="assets/radar_72b.jpg" width=600px/>
<p>
<br>

| Model             |   MMLU   |  C-Eval  |  GSM8K   |   MATH   | HumanEval |   MBPP   |   BBH    |  CMMLU   |
|:------------------|:--------:|:--------:|:--------:|:--------:|:---------:|:--------:|:--------:|:--------:|
|                   |  5-shot  |  5-shot  |  8-shot  |  4-shot  |  0-shot   |  3-shot  |  3-shot  |  5-shot  |
| LLaMA2-7B         |   46.8   |   32.5   |   16.7   |   3.3    |   12.8    |   20.8   |   38.2   |   31.8   |
| LLaMA2-13B        |   55.0   |   41.4   |   29.6   |   5.0    |   18.9    |   30.3   |   45.6   |   38.4   |
| LLaMA2-34B        |   62.6   |    -     |   42.2   |   6.2    |   22.6    |   33.0   |   44.1   |    -     |
| ChatGLM2-6B       |   47.9   |   51.7   |   32.4   |   6.5    |     -     |    -     |   33.7   |    -     |
| InternLM-7B       |   51.0   |   53.4   |   31.2   |   6.3    |   10.4    |   14.0   |   37.0   |   51.8   |
| InternLM-20B      |   62.1   |   58.8   |   52.6   |   7.9    |   25.6    |   35.6   |   52.5   |   59.0   |
| Baichuan2-7B      |   54.7   |   56.3   |   24.6   |   5.6    |   18.3    |   24.2   |   41.6   |   57.1   |
| Baichuan2-13B     |   59.5   |   59.0   |   52.8   |   10.1   |   17.1    |   30.2   |   49.0   |   62.0   |
| Yi-34B      	  	  |   76.3   |   81.8   |   67.9   |   15.9   |   26.2    |   38.2   |   66.4   |   82.6   |
| XVERSE-65B      	 |   70.8   |   68.6   |   60.3   |    -     |   26.3    |    -     |    -     |    -     |
| **Qwen-1.8B**     |   45.3   |   56.1   |   32.3   |   2.3    |   15.2    |   14.2   |   22.3   |   52.1   |
| **Qwen-7B**       |   58.2   |   63.5   |   51.7   |   11.6   |   29.9    |   31.6   |   45.0   |   62.2   |
| **Qwen-14B**      |   66.3   |   72.1   |   61.3   |   24.8   |   32.3    |   40.8   |   53.4   |   71.0   |
| **Qwen-72B**      | **77.4** | **83.3** | **78.9** | **35.2** | **35.4**  | **52.2** | **67.7** | **83.6** |

Pour tous les modÃ¨les comparÃ©s, nous indiquons les meilleurs scores entre leurs rÃ©sultats officiels et [OpenCompass] (https://opencompass.org.cn/leaderboard-llm). 

Pour plus de rÃ©sultats expÃ©rimentaux (performances dÃ©taillÃ©es des modÃ¨les sur d'autres ensembles de donnÃ©es de rÃ©fÃ©rence) et de dÃ©tails, veuillez vous rÃ©fÃ©rer Ã  notre rapport technique en cliquant [ici](https://qianwen-res.oss-cn-beijing.aliyuncs.com/QWEN_TECHNICAL_REPORT.pdf).
<br><br>

## Besoins

* python 3.8 et plus
* pytorch 1.12 et plus, 2.0 et plus sont recommandÃ©s
* transformers 4.32 et plus
* CUDA 11.4 et plus sont recommandÃ©s (pour les utilisateurs de GPU, les utilisateurs de flash, etc.)
<br>

## DÃ©marrage Rapide

Ci-dessous, nous fournissons des exemples simples pour montrer comment utiliser Qwen-Chat avec ğŸ¤– ModelScope et ğŸ¤— Transformers.

Vous pouvez utiliser nos images docker prÃ©-construites pour sauter la plupart des Ã©tapes de configuration de l'environnement, voir la section ["Utiliser des images docker prÃ©-construites"](#-docker) pour plus de dÃ©tails. 

Si vous n'utilisez pas Docker, assurez-vous d'avoir configurÃ© l'environnement et installÃ© les paquets requis. Assurez-vous de rÃ©pondre aux exigences ci-dessus, puis installez les bibliothÃ¨ques dÃ©pendantes.

```bash
pip install -r requirements.txt
```

Si votre appareil supporte fp16 ou bf16, nous vous recommandons d'installer [flash-attention](https://github.com/Dao-AILab/flash-attention) (**nous supportons flash-attention 2 maintenant.**) pour une meilleure efficacitÃ© et une moindre utilisation de la mÃ©moire. (**flash-attention est optionnel et le projet peut fonctionner normalement sans l'installer**)

```bash
git clone https://github.com/Dao-AILab/flash-attention
cd flash-attention && pip install .
# Below are optional. Installing them might be slow.
# pip install csrc/layer_norm
# pip install csrc/rotary
```

Vous pouvez maintenant commencer avec ModelScope ou Transformers.

### ğŸ¤— Transformers

Pour utiliser Qwen-Chat pour l'infÃ©rence, il vous suffit de saisir quelques lignes de code, comme indiquÃ© ci-dessous. N'oubliez pas de transmettre les noms de modÃ¨les ou les chemins corrects, tels que "Qwen/Qwen-7B-Chat" et "Qwen/Qwen-14B-Chat". Cependant, **veuillez vous assurer que vous utilisez le code le plus rÃ©cent**.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# Model names: "Qwen/Qwen-7B-Chat", "Qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
# use auto mode, automatically select precision based on the device.
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat",
    device_map="auto",
    trust_remote_code=True
).eval()

# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)

# 1st dialogue turn
response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
# ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚

# 2nd dialogue turn
response, history = model.chat(tokenizer, "ç»™æˆ‘è®²ä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚", history=history)
print(response)
# è¿™æ˜¯ä¸€ä¸ªå…³äºä¸€ä¸ªå¹´è½»äººå¥‹æ–—åˆ›ä¸šæœ€ç»ˆå–å¾—æˆåŠŸçš„æ•…äº‹ã€‚
# æ•…äº‹çš„ä¸»äººå…¬å«ææ˜ï¼Œä»–æ¥è‡ªä¸€ä¸ªæ™®é€šçš„å®¶åº­ï¼Œçˆ¶æ¯éƒ½æ˜¯æ™®é€šçš„å·¥äººã€‚ä»å°ï¼Œææ˜å°±ç«‹ä¸‹äº†ä¸€ä¸ªç›®æ ‡ï¼šè¦æˆä¸ºä¸€åæˆåŠŸçš„ä¼ä¸šå®¶ã€‚
# ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œææ˜å‹¤å¥‹å­¦ä¹ ï¼Œè€ƒä¸Šäº†å¤§å­¦ã€‚åœ¨å¤§å­¦æœŸé—´ï¼Œä»–ç§¯æå‚åŠ å„ç§åˆ›ä¸šæ¯”èµ›ï¼Œè·å¾—äº†ä¸å°‘å¥–é¡¹ã€‚ä»–è¿˜åˆ©ç”¨è¯¾ä½™æ—¶é—´å»å®ä¹ ï¼Œç§¯ç´¯äº†å®è´µçš„ç»éªŒã€‚
# æ¯•ä¸šåï¼Œææ˜å†³å®šå¼€å§‹è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–å¼€å§‹å¯»æ‰¾æŠ•èµ„æœºä¼šï¼Œä½†å¤šæ¬¡éƒ½è¢«æ‹’ç»äº†ã€‚ç„¶è€Œï¼Œä»–å¹¶æ²¡æœ‰æ”¾å¼ƒã€‚ä»–ç»§ç»­åŠªåŠ›ï¼Œä¸æ–­æ”¹è¿›è‡ªå·±çš„åˆ›ä¸šè®¡åˆ’ï¼Œå¹¶å¯»æ‰¾æ–°çš„æŠ•èµ„æœºä¼šã€‚
# æœ€ç»ˆï¼Œææ˜æˆåŠŸåœ°è·å¾—äº†ä¸€ç¬”æŠ•èµ„ï¼Œå¼€å§‹äº†è‡ªå·±çš„åˆ›ä¸šä¹‹è·¯ã€‚ä»–æˆç«‹äº†ä¸€å®¶ç§‘æŠ€å…¬å¸ï¼Œä¸“æ³¨äºå¼€å‘æ–°å‹è½¯ä»¶ã€‚åœ¨ä»–çš„é¢†å¯¼ä¸‹ï¼Œå…¬å¸è¿…é€Ÿå‘å±•èµ·æ¥ï¼Œæˆä¸ºäº†ä¸€å®¶æˆåŠŸçš„ç§‘æŠ€ä¼ä¸šã€‚
# ææ˜çš„æˆåŠŸå¹¶ä¸æ˜¯å¶ç„¶çš„ã€‚ä»–å‹¤å¥‹ã€åšéŸ§ã€å‹‡äºå†’é™©ï¼Œä¸æ–­å­¦ä¹ å’Œæ”¹è¿›è‡ªå·±ã€‚ä»–çš„æˆåŠŸä¹Ÿè¯æ˜äº†ï¼Œåªè¦åŠªåŠ›å¥‹æ–—ï¼Œä»»ä½•äººéƒ½æœ‰å¯èƒ½å–å¾—æˆåŠŸã€‚

# 3rd dialogue turn
response, history = model.chat(tokenizer, "ç»™è¿™ä¸ªæ•…äº‹èµ·ä¸€ä¸ªæ ‡é¢˜", history=history)
print(response)
# ã€Šå¥‹æ–—åˆ›ä¸šï¼šä¸€ä¸ªå¹´è½»äººçš„æˆåŠŸä¹‹è·¯ã€‹
```

L'exÃ©cution du modÃ¨le prÃ©-entraÃ®nÃ© de Qwen est Ã©galement simple.

<details>
  <summary>Running Qwen</summary>

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

# Model names: "Qwen/Qwen-7B", "Qwen/Qwen-14B" 
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)
# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B", device_map="cpu", trust_remote_code=True).eval()
# use auto mode, automatically select precision based on the device.
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B",
    device_map="auto",
    trust_remote_code=True
).eval()

# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.
# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B", trust_remote_code=True)

inputs = tokenizer('è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯', return_tensors='pt')
inputs = inputs.to(model.device)
pred = model.generate(**inputs)
print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))
# è’™å¤å›½çš„é¦–éƒ½æ˜¯ä¹Œå…°å·´æ‰˜ï¼ˆUlaanbaatarï¼‰\nå†°å²›çš„é¦–éƒ½æ˜¯é›·å…‹é›…æœªå…‹ï¼ˆReykjavikï¼‰\nåŸƒå¡ä¿„æ¯”äºšçš„é¦–éƒ½æ˜¯äºšçš„æ–¯äºšè´å·´ï¼ˆAddis Ababaï¼‰...
```

</details>

En cas de problÃ¨me de rÃ©seau lors de la tentative de tÃ©lÃ©chargement des poids et des codes du modÃ¨le Ã  partir de HuggingFace, une autre approche consiste Ã  rÃ©cupÃ©rer le point de contrÃ´le Ã  partir de ModelScope, puis Ã  le charger Ã  partir du rÃ©pertoire local, comme indiquÃ© ci-dessous:

```python
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer

# Downloading model checkpoint to a local dir model_dir
# model_dir = snapshot_download('qwen/Qwen-7B')
# model_dir = snapshot_download('qwen/Qwen-7B-Chat')
# model_dir = snapshot_download('qwen/Qwen-14B')
model_dir = snapshot_download('qwen/Qwen-14B-Chat')

# Loading local checkpoints
# trust_remote_code is still set as True since we still load codes from local dir instead of transformers
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    device_map="auto",
    trust_remote_code=True
).eval()
```

### ğŸ¤– ModelScope

ModelScope est une plateforme opensource pour Model-as-a-Service (MaaS), qui fournit un service de modÃ¨le flexible et rentable aux dÃ©veloppeurs d'IA. De mÃªme, vous pouvez exÃ©cuter les modÃ¨les avec ModelScope comme indiquÃ© ci-dessous:

```python
from modelscope import AutoModelForCausalLM, AutoTokenizer
from modelscope import GenerationConfig

# Model names: "qwen/Qwen-7B-Chat", "qwen/Qwen-14B-Chat"
tokenizer = AutoTokenizer.from_pretrained("qwen/Qwen-7B-Chat", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚

response, history = model.chat(tokenizer, "ä½ å¥½", history=None)
print(response)
response, history = model.chat(tokenizer, "æµ™æ±Ÿçš„çœä¼šåœ¨å“ªé‡Œï¼Ÿ", history=history) 
print(response)
response, history = model.chat(tokenizer, "å®ƒæœ‰ä»€ä¹ˆå¥½ç©çš„æ™¯ç‚¹", history=history)
print(response)
```

### InfÃ©rence par lots
Qwen prend en charge l'infÃ©rence par lots. Lorsque flash attention est activÃ©e, l'utilisation de l'infÃ©rence par lots peut entraÃ®ner une accÃ©lÃ©ration de 40 %. Le code d'exemple est prÃ©sentÃ© ci-dessous:
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import GenerationConfig
from qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids

tokenizer = AutoTokenizer.from_pretrained(
    './',
    pad_token='<|extra_0|>',
    eos_token='<|endoftext|>',
    padding_side='left',
    trust_remote_code=True
)
model = AutoModelForCausalLM.from_pretrained(
    './',
    pad_token_id=tokenizer.pad_token_id,
    device_map="auto",
    trust_remote_code=True
).eval()
model.generation_config = GenerationConfig.from_pretrained('./', pad_token_id=tokenizer.pad_token_id)

all_raw_text = ["æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚", "ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹", "æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°"]
batch_raw_text = []
for q in all_raw_text:
    raw_text, _ = make_context(
        tokenizer,
        q,
        system="You are a helpful assistant.",
        max_window_size=model.generation_config.max_window_size,
        chat_format=model.generation_config.chat_format,
    )
    batch_raw_text.append(raw_text)

batch_input_ids = tokenizer(batch_raw_text, padding='longest')
batch_input_ids = torch.LongTensor(batch_input_ids['input_ids']).to(model.device)
batch_out_ids = model.generate(
    batch_input_ids,
    return_dict_in_generate=False,
    generation_config=model.generation_config
)
padding_lens = [batch_input_ids[i].eq(tokenizer.pad_token_id).sum().item() for i in range(batch_input_ids.size(0))]

batch_response = [
    decode_tokens(
        batch_out_ids[i][padding_lens[i]:],
        tokenizer,
        raw_text_len=len(batch_raw_text[i]),
        context_length=(batch_input_ids[i].size(0)-padding_lens[i]),
        chat_format="chatml",
        verbose=False,
        errors='replace'
    ) for i in range(len(all_raw_text))
]
print(batch_response)

response, _ = model.chat(tokenizer, "æˆ‘æƒ³å¬ä½ è¯´çˆ±æˆ‘ã€‚", history=None)
print(response)

response, _ = model.chat(tokenizer, "ä»Šå¤©æˆ‘æƒ³åƒç‚¹å•¥ï¼Œç”œç”œçš„ï¼Œæ¨èä¸‹", history=None)
print(response)

response, _ = model.chat(tokenizer, "æˆ‘é©¬ä¸Šè¿Ÿåˆ°äº†ï¼Œæ€ä¹ˆåšæ‰èƒ½ä¸è¿Ÿåˆ°", history=None)
print(response)
```

### CPU

Pour dÃ©ployer nos modÃ¨les sur CPU, nous vous conseillons vivement d'utiliser [qwen.cpp](https://github.com/QwenLM/qwen.cpp), qui est une implÃ©mentation purement C++ de Qwen et de tiktoken. Consultez le repo pour plus de dÃ©tails!

Il est simple d'exÃ©cuter directement le modÃ¨le sur le CPU, ce qui nÃ©cessite la spÃ©cification de votre appareil:

```python
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()
```

Cependant, il est probable que vous souffriez d'une efficacitÃ© d'infÃ©rence extrÃªmement faible.

### Plusieurs GPU

Si vous souffrez d'un manque de mÃ©moire GPU et que vous souhaitez exÃ©cuter le modÃ¨le sur plus d'un GPU, vous pouvez utiliser directement la mÃ©thode de chargement par dÃ©faut, qui est maintenant supportÃ©e par Transformers. La mÃ©thode prÃ©cÃ©dente basÃ©e sur `utils.py` est obsolÃ¨te.

Cependant, bien que cette mÃ©thode soit simple, l'efficacitÃ© du parallÃ©lisme natif du pipeline est faible. Nous vous conseillons d'utiliser vLLM avec FastChat et de lire la section relative au dÃ©ploiement.


### DashScope

Le moyen le plus simple d'utiliser Qwen via les API est le service API DashScope via Alibaba Cloud. Nous prÃ©sentons une introduction Ã  l'utilisation. De plus, nous fournissons un script pour vous permettre de dÃ©ployer une API de type OpenAI sur vos propres serveurs.

DashScope est le service API de grands modÃ¨les linguistiques fourni par Alibaba Cloud, qui prend dÃ©sormais en charge Qwen. Notez que les modÃ¨les derriÃ¨re DashScope sont des versions internes temporairement sans dÃ©tails fournis. Les services comprennent `qwen-turbo` et `qwen-plus`, le premier fonctionnant plus rapidement et le second atteignant de meilleures performances. Pour plus d'informations, consultez la documentation [ici] (https://dashscope.aliyun.com).

Veuillez vous rendre sur le site officiel [lien](https://help.aliyun.com/zh/dashscope/developer-reference/activate-dashscope-and-create-an-api-key?spm=a2c4g.11186623.0.0.6c2774fahtfXdn) pour crÃ©er un compte DashScope et obtenir la clÃ© API (AK). Nous recommandons de dÃ©finir l'AK Ã  l'aide d'une variable d'environnement:
```bash
export DASHSCOPE_API_KEY="YOUR_DASHSCOPE_API_KEY"
```
Installez ensuite les paquets et cliquez sur [ici](https://help.aliyun.com/zh/dashscope/developer-reference/install-dashscope-sdk) pour obtenir la documentation. Si vous utilisez Python, vous pouvez installer DashScope avec pip:
```bash
pip install dashscope
```
Si vous utilisez JAVA SDK, vous pouvez l'installer de cette maniÃ¨re:
```xml
<!-- https://mvnrepository.com/artifact/com.alibaba/dashscope-sdk-java -->
<dependency>
    <groupId>com.alibaba</groupId>
    <artifactId>dashscope-sdk-java</artifactId>
    <version>the-latest-version</version>
</dependency>
```
La maniÃ¨re la plus simple d'utiliser DashScope est l'utilisation de messages, qui est similaire Ã  l'API OpenAI. L'exemple est prÃ©sentÃ© ci-dessous:
```python
import random
from http import HTTPStatus
from dashscope import Generation


def call_with_messages():
    messages = [{'role': 'system', 'content': 'You are a helpful assistant.'},
                {'role': 'user', 'content': 'å¦‚ä½•åšè¥¿çº¢æŸ¿é¸¡è›‹ï¼Ÿ'}]
    gen = Generation()
    response = gen.call(
        Generation.Models.qwen_turbo,
        messages=messages,
        seed=random.randint(1, 10000),  # set the random seed, optional, default to 1234 if not set
        result_format='message',  # set the result to be "message" format.
    )
    return response


if __name__ == '__main__':
    response = call_with_messages()
    if response.status_code == HTTPStatus.OK:
        print(response)
    else:
        print('Request id: %s, Status code: %s, error code: %s, error message: %s' % (
            response.request_id, response.status_code,
            response.code, response.message
        ))
```
Pour d'autres utilisations, veuillez consulter le site web officiel pour plus de dÃ©tails.
<br><br>

## Quantization

### GPTQ

Nous proposons une solution basÃ©e sur [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), et publions les modÃ¨les quantifiÃ©s Int4 et Int8, qui permettent d'obtenir des effets de modÃ¨le presque sans perte mais des performances amÃ©liorÃ©es en termes de coÃ»ts de mÃ©moire et de vitesse d'infÃ©rence.

Nous dÃ©montrons ici comment utiliser les modÃ¨les quantifiÃ©s que nous fournissons pour l'infÃ©rence. Avant de commencer, assurez-vous que vous rÃ©pondez aux exigences d'auto-gptq (par exemple, torch 2.0 et plus, transformers 4.32.0 et plus, etc.) et installez les paquets requis:

```bash
pip install auto-gptq optimum
```

Si vous rencontrez des problÃ¨mes pour installer `auto-gptq`, nous vous conseillons de consulter le [repo](https://github.com/PanQiWei/AutoGPTQ) officiel pour trouver une roue.

> Note : Les paquets `auto-gptq` prÃ©compilÃ©s dÃ©pendent fortement de la version de `torch` et de sa version CUDA. De plus, en raison d'une rÃ©cente mise Ã  jour,
> vous pouvez aussi rencontrer des erreurs de version non supportÃ©e avec `transformers`, `optimum`, ou `peft`.
> Nous recommandons d'utiliser les derniÃ¨res versions rÃ©pondant aux exigences suivantes :
> - torch==2.1 auto-gptq>=0.5.1 transformers>=4.35.0 optimum>=1.14.0 peft>=0.6.1
> - torch>=2.0,<2.1 auto-gptq<0.5.0 transformers<4.35.0 optimum<1.14.0 peft>=0.5.0,<0.6.0

Vous pouvez ensuite charger facilement le modÃ¨le quantifiÃ© et lancer l'infÃ©rence comme d'habitude:

```python
# Model names: "Qwen/Qwen-7B-Chat-Int4", "Qwen/Qwen-14B-Chat-Int4"
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat-Int4",
    device_map="auto",
    trust_remote_code=True
).eval()
response, history = model.chat(tokenizer, "Hi", history=None)
```

Nous illustrons les performances des modÃ¨les BF16, Int8 et Int4 sur le benchmark, et nous constatons que le modÃ¨le quantifiÃ© ne souffre pas d'une dÃ©gradation significative des performances. Les rÃ©sultats sont prÃ©sentÃ©s ci-dessous:

| Quantization         | MMLU | CEval (val) | GSM8K | Humaneval |
|----------------------|:----:|:-----------:|:-----:|:---------:|
| Qwen-1.8B-Chat (BF16)| 43.3 |    55.6     | 33.7  |   26.2    |
| Qwen-1.8B-Chat (Int8)| 43.1 |    55.8     | 33.0  |   27.4    |
| Qwen-1.8B-Chat (Int4)| 42.9 |    52.8     | 31.2  |   25.0    |
| Qwen-7B-Chat (BF16)  | 55.8 |    59.7     | 50.3  |   37.2    |
| Qwen-7B-Chat (Int8)  | 55.4 |    59.4     | 48.3  |   34.8    |
| Qwen-7B-Chat (Int4)  | 55.1 |    59.2     | 49.7  |   29.9    |
| Qwen-14B-Chat (BF16) | 64.6 |    69.8     | 60.1  |   43.9    |
| Qwen-14B-Chat (Int8) | 63.6 |    68.6     | 60.0  |   48.2    |
| Qwen-14B-Chat (Int4) | 63.3 |    69.0     | 59.8  |   45.7    |
| Qwen-72B-Chat (BF16) | 74.4 |    80.1     | 76.4  |   64.6    |
| Qwen-72B-Chat (Int8) | 73.5 |    80.1     | 73.5  |   62.2    |
| Qwen-72B-Chat (Int4) | 73.4 |    80.1     | 75.3  |   61.6    |

### Quantization du cache KV

> NOTE : Veuillez noter qu'en raison du mÃ©canisme interne de Hugging Face, les fichiers de support pour cette fonctionnalitÃ© 
> (i.e., `cache_autogptq_cuda_256.cpp` et `cache_autogptq_cuda_kernel_256.cu`) peuvent Ãªtre manquants. 
> Veuillez les tÃ©lÃ©charger manuellement manuellement depuis le Hugging Face Hub et placez-les dans le mÃªme dossier que les autres fichiers du module.

Le cache KV de l'attention peut Ãªtre quantifiÃ© et compressÃ© pour le stockage, afin d'obtenir un dÃ©bit d'Ã©chantillonnage plus Ã©levÃ©. Les arguments `use_cache_quantization` et `use_cache_kernel` dans `config.json` sont fournis pour activer la quantification du cache KV. 
La mÃ©thode d'utilisation spÃ©cifique est la suivante:

```python
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-7B-Chat",
     device_map="auto",
     trust_remote_code=True,
     use_cache_quantization=True,
     use_cache_kernel=True,
     use_flash_attn=False
)
```
Attention : Actuellement, la quantification du cache KV et flash attention ne peuvent pas Ãªtre utilisÃ©es en mÃªme temps.
Si vous activez la quantification du cache KV et flash attention en mÃªme temps (`use_flash_attn=True`, `use_cache_quantization=True`, `use_cache_kernel=True`), `use_flash_attn` est dÃ©sactivÃ© par dÃ©faut (`use_flash_attn=false`).

Nous avons vÃ©rifiÃ© que l'utilisation du modÃ¨le int8-kvcache quantifiÃ© ne souffre pas d'une dÃ©gradation significative des performances dans l'Ã©valuation en aval. Dans ce qui suit, nous nous concentrons sur le profilage de son empreinte mÃ©moire dans diffÃ©rentes conditions. 
Le profilage s'exÃ©cute sur un seul GPU A100-SXM4-80G avec PyTorch 2.0.1 et CUDA 11.4. 
Nous utilisons des modÃ¨les BF16 pour gÃ©nÃ©rer 1024 jetons par dÃ©faut, et "OOM" indique une erreur de mÃ©moire insuffisante.

Avec la quantification du cache KV, le modÃ¨le peut infÃ©rer avec une taille de lot (bs) plus grande.

| Utilisation du cache KV |  bs=1  |  bs=4  | bs=16  | bs=32  | bs=64  | bs=100 |
|--------------|:------:|:------:|:------:|:------:|:------:|:------:|
| Non          | 16.3GB | 24.1GB | 31.7GB | 48.7GB |  OOM   |  OOM   |
| Oui          | 15.5GB | 17.2GB | 22.3GB | 30.2GB | 48.2GB | 72.4GB |

Avec la quantification du cache KV, le modÃ¨le peut Ã©conomiser plus de mÃ©moire lorsqu'il gÃ©nÃ¨re des sÃ©quences plus longues (`sl`, se rÃ©fÃ©rant au nombre de jetons gÃ©nÃ©rÃ©s) Ã  l'Ã©tape de l'infÃ©rence.

| Utilisation du cache KV | sl=512 | sl=1024 | sl=2048 | sl=4096 | sl=8192 |
|-------------------------|:------:|:-------:|:-------:|:-------:|:-------:|
| Non                     | 15.2GB | 16.3GB  | 17.6GB  | 19.5GB  | 23.2GB  |
| Oui                     | 15.0GB | 15.5GB  | 15.8GB  | 16.6GB  | 17.6GB  |

Le modÃ¨le avec quantification du cache KV convertira le format de `layer_past` de float Ã  int8, et pendant ce temps le `layer-past` quantifiÃ© stockera Ã©galement les paramÃ¨tres de quantification.

Les Ã©tapes spÃ©cifiques sont les suivantes:

1. Quantifier clÃ©/valeur
```
    qv,scale,zero_point=quantize_cache_v(v)
```
2. Stocker dans `layer_past`

Voici le format de `layer_past` quantifiÃ©:
```
    layer_past=((q_key,key_scale,key_zero_point),
                (q_value,value_scale,value_zero_point))
```

Le format original de `layer_past` est illustrÃ© ci-dessous:
```
    layer_past=(key,value)
```

Si vous souhaitez utiliser l'attention KV qui est quantifiÃ©e, vous pouvez utiliser l'opÃ©ration de dÃ©quantification pour reconvertir la clÃ©/valeur int8 au format float comme suit 
vous pouvez utiliser l'opÃ©ration de dÃ©quantification pour reconvertir la clÃ©/valeur int8 au format float comme suit:
```
    v=dequantize_cache_torch(qv,scale,zero_point)
```
<br>


## Performance de l'infÃ©rence

Cette section fournit les statistiques de vitesse et de mÃ©moire des modÃ¨les dans diffÃ©rentes prÃ©cisions. Le profilage de la vitesse et de la mÃ©moire est effectuÃ© Ã  l'aide de [ce script] (https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py).

Nous avons mesurÃ© la vitesse moyenne d'infÃ©rence (tokens/s) et l'utilisation de la mÃ©moire GPU pour gÃ©nÃ©rer 2048 avec les modÃ¨les en BF16, Int8 et Int4.

<table>
    <tr>
        <td>Model Size</td>
        <td>Quantization</td>
        <td>Speed (Tokens/s)</td>
        <td>GPU Memory Usage</td>
    </tr>
    <tr>
        <td rowspan="3">1.8B</td>
        <td>BF16</td>
        <td>54.09</td>
        <td>4.23GB</td>
    </tr>
    <tr>
        <td>Int8</td>
        <td>55.56</td>
        <td>3.48GB</td>
    </tr>
    <tr>
        <td>Int4</td>
        <td>71.07</td>
        <td>2.91GB</td>
    </tr>
    <tr>
        <td rowspan="3">7B</td>
        <td>BF16</td>
        <td>40.93</td>
        <td>16.99GB</td>
    </tr>
    <tr>
        <td>Int8</td>
        <td>37.47</td>
        <td>11.20GB</td>
    </tr>
    <tr>
        <td>Int4</td>
        <td>50.09</td>
        <td>8.21GB</td>
    </tr>
    <tr>
        <td rowspan="3">14B</td>
        <td>BF16</td>
        <td>32.22</td>
        <td>30.15GB</td>
    </tr>
    <tr>
        <td>Int8</td>
        <td>29.28</td>
        <td>18.81GB</td>
    </tr>
    <tr>
        <td>Int4</td>
        <td>38.72</td>
        <td>13.01GB</td>
    </tr>
    <tr>
        <td rowspan="3">72B</td>
        <td>BF16</td>
        <td>8.48</td>
        <td>144.69GB (2xA100)</td>
    </tr>
    <tr>
        <td>Int8</td>
        <td>9.05</td>
        <td>81.27GB (2xA100)</td>
    </tr>
    <tr>
        <td>Int4</td>
        <td>11.32</td>
        <td>48.86GB</td>
    </tr>
    <tr>
        <td>72B + vLLM</td>
        <td>BF16</td>
        <td>17.60</td>
        <td>2xA100</td>
    </tr>
</table>

Le profilage s'exÃ©cute sur un seul GPU A100-SXM4-80G (sauf si 2xA100 est mentionnÃ©) avec PyTorch 2.0.1, CUDA 11.8, et Flash-Attention 2. (72B + vLLM utilise PyTorch 2.1.0 et Cuda 11.8.) La vitesse d'infÃ©rence est calculÃ©e en moyenne sur les tokens encodÃ©s et gÃ©nÃ©rÃ©s.

Note : La vitesse de gÃ©nÃ©ration des modÃ¨les Int4/Int8 mentionnÃ©s ci-dessus est fournie par la bibliothÃ¨que autogptq. La vitesse actuelle du modÃ¨le chargÃ© en utilisant ``AutoModelForCausalLM.from_pretrained`` sera environ 20% plus lente. Nous avons signalÃ© ce problÃ¨me Ã  l'Ã©quipe HuggingFace et nous le mettrons Ã  jour rapidement si une solution est disponible.

Nous mesurons Ã©galement la vitesse d'infÃ©rence et l'utilisation de la mÃ©moire du GPU avec diffÃ©rents paramÃ¨tres de contexte et de longueur de gÃ©nÃ©ration, version Flash-Attention. Vous pouvez trouver les rÃ©sultats dans les cartes modÃ¨les correspondantes sur Hugging Face ou ModelScope.


## Finetuning

### Utilisation
Nous fournissons maintenant le script d'entraÃ®nement officiel, `finetune.py`, pour que les utilisateurs puissent ajuster le modÃ¨le prÃ©-entraÃ®nÃ© pour les applications en aval de maniÃ¨re simple. De plus, nous fournissons des scripts shell pour lancer le finetune sans soucis. Ce script prend en charge l'entraÃ®nement avec [DeepSpeed](https://github.com/microsoft/DeepSpeed) et [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/). Les scripts que nous fournissons utilisent DeepSpeed (Note : il peut y avoir des conflits avec la derniÃ¨re version de pydantic et vous devriez utiliser make sure `pydantic<2.0`) et Peft. Vous pouvez les installer en procÃ©dant comme suit :
```bash
pip install "peft<0.8.0" deepspeed
```

Pour prÃ©parer vos donnÃ©es d'entraÃ®nement, vous devez rassembler tous les Ã©chantillons dans une liste et l'enregistrer dans un fichier json. Chaque Ã©chantillon est un dictionnaire composÃ© d'un identifiant et d'une liste de conversation. Voici un exemple simple de liste avec 1 Ã©chantillon :
```json
[
  {
    "id": "identity_0",
    "conversations": [
      {
        "from": "user",
        "value": "ä½ å¥½"
      },
      {
        "from": "assistant",
        "value": "æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œæˆ‘å«é€šä¹‰åƒé—®ã€‚"
      }
    ]
  }
]
```

AprÃ¨s la prÃ©paration des donnÃ©es, vous pouvez utiliser les scripts shell fournis pour lancer le finetuning. N'oubliez pas de spÃ©cifier le chemin d'accÃ¨s au fichier de donnÃ©es, `$DATA`.

Les scripts de finetuning vous permettent d'effectuer les opÃ©rations suivantes
- Finetuning de tous les paramÃ¨tres
- LoRA
- Q-LoRA

Le finetuning de tous les paramÃ¨tres nÃ©cessite la mise Ã  jour de tous les paramÃ¨tres au cours de l'ensemble du processus de formation. Pour lancer votre formation, exÃ©cutez le script suivant:

```bash
# Distributed training. We do not provide single-GPU training script as the insufficient GPU memory will break down the training.
bash finetune/finetune_ds.sh
```

N'oubliez pas de spÃ©cifier le nom ou le chemin d'accÃ¨s au modÃ¨le, le chemin d'accÃ¨s aux donnÃ©es, ainsi que le rÃ©pertoire de sortie dans les scripts shell. Une autre chose Ã  noter est que nous utilisons DeepSpeed ZeRO 3 dans ce script. Si vous voulez faire des changements, il suffit de supprimer l'argument `--deepspeed` ou de faire des changements dans le fichier json de configuration de DeepSpeed en fonction de vos besoins. De plus, ce script supporte l'entraÃ®nement en prÃ©cision mixte, et donc vous pouvez utiliser `--bf16 True` ou `--fp16 True`. N'oubliez pas d'utiliser DeepSpeed lorsque vous utilisez fp16 en raison de l'entraÃ®nement de prÃ©cision mixte. Empiriquement, nous vous conseillons d'utiliser bf16 pour rendre votre apprentissage cohÃ©rent avec notre prÃ©-entraÃ®nement et notre alignement si votre machine supporte bf16, et nous l'utilisons donc par dÃ©faut.

Pour exÃ©cuter LoRA, utilisez un autre script Ã  exÃ©cuter comme indiquÃ© ci-dessous. Avant de commencer, assurez-vous que vous avez installÃ© `peft`. Vous devez spÃ©cifier les chemins d'accÃ¨s Ã  votre modÃ¨le, Ã  vos donnÃ©es et Ã  vos rÃ©sultats. Nous vous conseillons d'utiliser des chemins absolus pour votre modÃ¨le prÃ©-entraÃ®nÃ©. En effet, LoRA ne sauvegarde que l'adaptateur et le chemin absolu dans le fichier json de configuration de l'adaptateur est utilisÃ© pour trouver le modÃ¨le prÃ©-entraÃ®nÃ© Ã  charger. De plus, ce script supporte Ã  la fois bf16 et fp16.

```bash
# Single GPU training
bash finetune/finetune_lora_single_gpu.sh
# Distributed training
bash finetune/finetune_lora_ds.sh
```

Par rapport au finetuning de tous les paramÃ¨tres, LoRA ([paper](https://arxiv.org/abs/2106.09685)) ne met Ã  jour que les paramÃ¨tres des couches d'adaptateurs, tout en gelant les couches originales du grand modÃ¨le de langage. Cela permet de rÃ©duire considÃ©rablement les coÃ»ts de mÃ©moire et donc les coÃ»ts de calcul.

Notez que si vous utilisez LoRA pour affiner le modÃ¨le linguistique de base, par exemple Qwen-7B, au lieu des modÃ¨les de chat, par exemple Qwen-7B-Chat, le script change automatiquement l'intÃ©gration et la couche de sortie en tant que paramÃ¨tres entraÃ®nables. En effet, le modÃ¨le linguistique de base n'a aucune connaissance des jetons spÃ©ciaux apportÃ©s par le format ChatML. Ces couches doivent donc Ãªtre mises Ã  jour pour que le modÃ¨le comprenne et prÃ©dise les jetons. En d'autres termes, si votre formation apporte des tokens spÃ©ciaux dans LoRA, vous devez dÃ©finir les couches comme des paramÃ¨tres entraÃ®nables en dÃ©finissant `modules_to_save` Ã  l'intÃ©rieur du code. De plus, si ces paramÃ¨tres sont entraÃ®nables, il n'est pas possible d'utiliser ZeRO 3, et c'est pourquoi nous utilisons ZeRO 2 par dÃ©faut dans le script. Si vous n'avez pas de nouveaux paramÃ¨tres entraÃ®nables, vous pouvez passer Ã  ZeRO 3 en modifiant le fichier de configuration de DeepSpeed. En outre, nous constatons qu'il existe un Ã©cart important entre l'empreinte mÃ©moire de LoRA avec et sans ces paramÃ¨tres d'entraÃ®nement. Par consÃ©quent, si vous avez des problÃ¨mes de mÃ©moire, nous vous conseillons d'affiner les modÃ¨les de chat de LoRA. Consultez le profil ci-dessous pour plus d'informations.

Si vous souffrez toujours d'un manque de mÃ©moire, vous pouvez envisager Q-LoRA ([paper](https://arxiv.org/abs/2305.14314)), qui utilise le modÃ¨le de langage quantifiÃ© et d'autres techniques telles que l'attention paginÃ©e pour rÃ©duire encore les coÃ»ts de mÃ©moire.

Note : pour exÃ©cuter l'entraÃ®nement Q-LoRA sur un seul GPU, vous pouvez avoir besoin d'installer `mpi4py` via `pip` ou `conda`.

Pour lancer Q-LoRA, exÃ©cutez directement le script suivant :

```bash
# Single GPU training
bash finetune/finetune_qlora_single_gpu.sh
# Distributed training
bash finetune/finetune_qlora_ds.sh
```

Pour Q-LoRA, nous vous conseillons de charger le modÃ¨le quantifiÃ© que nous fournissons, par exemple Qwen-7B-Chat-Int4. Vous **NE DEVRIEZ PAS** utiliser les modÃ¨les bf16. Contrairement au finetuning de tous les paramÃ¨tres et Ã  la LoRA, seul le modÃ¨le fp16 est pris en charge pour la Q-LoRA. Pour l'entraÃ®nement sur un seul GPU, nous devons utiliser DeepSpeed pour l'entraÃ®nement en prÃ©cision mixte en raison de notre observation des erreurs causÃ©es par torch amp. En outre, pour Q-LoRA, les problÃ¨mes avec les jetons spÃ©ciaux dans LoRA existent toujours. Cependant, comme nous ne fournissons que les modÃ¨les Int4 pour les modÃ¨les de chat, ce qui signifie que le modÃ¨le de langage a appris les tokens spÃ©ciaux du format ChatML, vous n'avez pas Ã  vous soucier des couches. Notez que les couches du modÃ¨le Int4 ne doivent pas Ãªtre entraÃ®nables, et donc si vous introduisez des tokens spÃ©ciaux dans votre entraÃ®nement, Q-LoRA risque de ne pas fonctionner.

> NOTE : Veuillez noter qu'en raison des mÃ©canismes internes de Hugging Face, certains fichiers non-Python (par exemple, `*.cpp` et `*.cu`) 
> peuvent Ãªtre absents du point de contrÃ´le sauvegardÃ©. Vous devrez peut-Ãªtre les copier manuellement dans le rÃ©pertoire contenant les autres fichiers.

Contrairement au finetuning des paramÃ¨tres complets, l'entraÃ®nement de LoRA et de Q-LoRA n'enregistre que les paramÃ¨tres de l'adaptateur. Supposons que votre entraÃ®nement commence Ã  partir de Qwen-7B, vous pouvez charger le modÃ¨le finalisÃ© pour l'infÃ©rence comme indiquÃ© ci-dessous:

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()
```

Si vous souhaitez fusionner les adaptateurs et enregistrer le modÃ¨le affinÃ© en tant que modÃ¨le autonome (vous ne pouvez le faire qu'avec LoRA, et vous **NE POUVEZ PAS** fusionner les paramÃ¨tres de Q-LoRA), vous pouvez exÃ©cuter les codes suivants :

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()

merged_model = model.merge_and_unload()
# max_shard_size and safe serialization are not necessary. 
# They respectively work for sharding checkpoint and save the model to safetensors
merged_model.save_pretrained(new_model_directory, max_shard_size="2048MB", safe_serialization=True)
```

Note : Pour l'entraÃ®nement multi-GPU, vous devez spÃ©cifier les hyperparamÃ¨tres appropriÃ©s pour l'entraÃ®nement distribuÃ© en fonction de votre machine. De plus, nous vous conseillons de spÃ©cifier votre longueur maximale de sÃ©quence avec l'argument `--model_max_length`, en fonction de votre considÃ©ration des donnÃ©es, de l'empreinte mÃ©moire, et de la vitesse d'apprentissage.


### Profilage de la mÃ©moire et de la vitesse
Nous profilons la mÃ©moire du GPU et la vitesse d'apprentissage de LoRA (LoRA (emb) se rÃ©fÃ¨re Ã  l'apprentissage de la couche d'intÃ©gration et de sortie, tandis que LoRA n'a pas de couche d'intÃ©gration et de sortie pouvant Ãªtre entraÃ®nÃ©e) et de Q-LoRA dans la configuration de l'apprentissage sur un seul GPU. Dans ce test, nous expÃ©rimentons sur un seul GPU A100-SXM4-80G, et nous utilisons CUDA 11.8 et Pytorch 2.0. Flash attention 2 est appliquÃ©. Nous utilisons uniformÃ©ment une taille de lot de 1 et une accumulation de gradient de 8. Nous profilons la mÃ©moire (GB) et la vitesse (s/iter) des entrÃ©es de diffÃ©rentes longueurs, Ã  savoir 256, 512, 1024, 2048, 4096, et 8192. Nous prÃ©sentons Ã©galement les statistiques du rÃ©glage fin de tous les paramÃ¨tres avec Qwen-7B sur 2 GPU A100. Nous ne prÃ©sentons que les statistiques de 256, 512 et 1024 jetons en raison de la limitation de la mÃ©moire du GPU. 

Pour Qwen-72B, nous expÃ©rimentons de deux maniÃ¨res : 1) Lora fintuning + DeepSpeed ZeRO 3 sur 4 GPU A100-SXM4-80G et 2) QLora (int4) fintuning sur un seul GPU A100-SXM4-80G. Notez que l'OOM se produit sur 4 GPUs A100-SXM4-80G Ã  la fois avec le rÃ©glage fin LoRA (emb) et le rÃ©glage fin LoRA sans Deepspeed ZeRO 3 (vous pouvez passer `--deepspeed finetune/ds_config_zero3.json` Ã  [`finetune/finetune_lora_ds.sh`](finetune/finetune_lora_ds.sh) afin d'activer DeepSpeed ZeRO 3).

Les statistiques sont listÃ©es ci-dessous :

<table>
    <tr>
      <th rowspan="2">Model Size</th><th rowspan="2">Method</th><th colspan="6" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">256</th><th align="center">512</th><th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th>
    </tr>
    </tr>
    </tr>
		<tr>
        <th rowspan="4">1.8B</th><td>LoRA</td><td align="center">6.7G / 1.0s/it</td><td align="center">7.4G / 1.0s/it</td><td align="center">8.4G / 1.1s/it</td><td align="center">11.0G / 1.7s/it</td><td align="center">16.2G / 3.3s/it</td><td align="center">21.8G / 6.8s/it</td>
    </tr>
    <tr>
        <td>LoRA (emb)</td><td align="center">13.7G / 1.0s/it</td><td align="center">14.0G / 1.0s/it</td><td align="center">14.0G / 1.1s/it</td><td align="center">15.1G / 1.8s/it</td><td align="center">19.7G / 3.4s/it</td><td align="center">27.7G / 7.0s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td align="center">5.8G / 1.4s/it</td><td align="center">6.0G / 1.4s/it</td><td align="center">6.6G / 1.4s/it</td><td align="center">7.8G / 2.0s/it</td><td align="center">10.2G / 3.4s/it</td><td align="center">15.8G / 6.5s/it</td>
    </tr>
    <tr>
        <td>Full-parameter</td><td align="center">43.5G / 2.1s/it</td><td align="center">43.5G / 2.2s/it</td><td align="center">43.5G / 2.2s/it</td><td align="center">43.5G / 2.3s/it</td><td align="center">47.1G / 2.8s/it</td><td align="center">48.3G / 5.6s/it</td>
    </tr>
    <tr>
        <th rowspan="4">7B</th><td>LoRA</td><td align="center">20.1G / 1.2s/it</td><td align="center">20.4G / 1.5s/it</td><td align="center">21.5G / 2.8s/it</td><td align="center">23.8G / 5.2s/it</td><td align="center">29.7G / 10.1s/it</td><td align="center">36.6G / 21.3s/it</td>
    </tr>
    <tr>
        <td>LoRA (emb)</td><td align="center">33.7G / 1.4s/it</td><td align="center">34.1G / 1.6s/it</td><td align="center">35.2G / 2.9s/it</td><td align="center">35.1G / 5.3s/it</td><td align="center">39.2G / 10.3s/it</td><td align="center">48.5G / 21.7s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td align="center">11.5G / 3.0s/it</td><td align="center">11.5G / 3.0s/it</td><td align="center">12.3G / 3.5s/it</td><td align="center">13.9G / 7.0s/it</td><td align="center">16.9G / 11.6s/it</td><td align="center">23.5G / 22.3s/it</td>
    </tr>
    <tr>
        <td>Full-parameter</td><td align="center">139.2G / 4.0s/it</td><td align="center">148.0G / 4.0s/it</td><td align="center">162.0G / 4.5s/it</td><td align="center">-</td><td align="center">-</td><td align="center">-</td>
    </tr>
    <tr>
        <th rowspan="3">14B</th><td>LoRA</td><td align="center">34.6G / 1.6s/it</td><td align="center">35.1G / 2.4s/it</td><td align="center">35.3G / 4.4s/it</td><td align="center">37.4G / 8.4s/it</td><td align="center">42.5G / 17.0s/it</td><td align="center">55.2G / 36.0s/it</td>
    </tr>
    <tr>
        <td>LoRA (emb)</td><td align="center">51.2 / 1.7s/it</td><td align="center">51.1G / 2.6s/it</td><td align="center">51.5G / 4.6s/it</td><td align="center">54.1G / 8.6s/it</td><td align="center">56.8G / 17.2s/it</td><td align="center">67.7G / 36.3s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td align="center">18.7G / 5.3s/it</td><td align="center">18.4G / 6.3s/it</td><td align="center">18.9G / 8.2s/it</td><td align="center">19.9G / 11.8s/it</td><td align="center">23.0G / 20.1s/it</td><td align="center">27.9G / 38.3s/it</td>
    </tr>
	<tr>
        <th rowspan="2">72B</th><td>LoRA + Deepspeed Zero3</td><td align="center">215.4G / 17.6s/it</td><td align="center">217.7G / 20.5s/it</td><td align="center">222.6G / 29.4s/it</td><td align="center">228.8G / 45.7s/it</td><td align="center">249.0G / 83.4s/it</td><td align="center">289.2G / 161.5s/it</td>
    </tr>
    <tr>
        <td>Q-LoRA</td><td align="center">61.4G / 27.4s/it</td><td align="center">61.4G / 31.5s/it</td><td align="center">62.9G / 41.4s/it</td><td align="center">64.1G / 59.5s/it</td><td align="center">68.0G / 97.7s/it</td><td align="center">75.6G / 179.8s/it</td>
    </tr>
</table>
<br>

## DÃ©ploiement

### vLLM 
Pour le dÃ©ploiement et l'infÃ©rence rapide, nous suggÃ©rons d'utiliser vLLM avec FastChat. Installez d'abord les paquets:
```bash
pip install vllm
pip install "fschat[model_worker,webui]"
```
Ou vous pouvez les installer Ã  partir des sources par `git clone` et `pip install -e .`. Nous vous conseillons de lire leurs documents si vous rencontrez des problÃ¨mes lors de l'installation.

Pour faire fonctionner Qwen avec vLLM et FastChat, vous devez d'abord lancer un contrÃ´leur par:
```bash
python -m fastchat.serve.controller
```

Ensuite, vous pouvez lancer le travailleur de modÃ¨le, ce qui signifie charger votre modÃ¨le pour l'infÃ©rence. Pour l'infÃ©rence sur un seul GPU, vous pouvez directement lancer:
```bash
python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code
```
Cependant, si vous souhaitez exÃ©cuter le modÃ¨le sur plusieurs GPU pour une infÃ©rence plus rapide ou une mÃ©moire plus importante, vous pouvez utiliser le parallÃ©lisme tensoriel pris en charge par vLLM. Supposons que vous exÃ©cutiez le modÃ¨le sur 4 GPU, la commande est prÃ©sentÃ©e ci-dessous:
```bash
python -m fastchat.serve.vllm_worker --model-path $model_path --trust-remote-code --tensor-parallel-size 4
```

AprÃ¨s avoir lancÃ© votre model worker, vous pouvez lancer :

* DÃ©monstration de l'interface web
```bash
python -m fastchat.serve.gradio_web_server
```

* API OpenAI
```bash
python -m fastchat.serve.openai_api_server --host localhost --port 8000
```

Cependant, si vous avez des difficultÃ©s Ã  utiliser vLLM et FastChat, vous pouvez essayer nos mÃ©thodes les plus simples pour dÃ©ployer une dÃ©mo web, une dÃ©mo CLI et une API.

### Interface Web

Nous fournissons du code pour que les utilisateurs puissent construire une dÃ©mo d'interface web (merci Ã  @wysaid). Avant de commencer, assurez-vous d'installer les paquets suivants:

```
pip install -r requirements_web_demo.txt
```

ExÃ©cutez ensuite la commande ci-dessous et cliquez sur le lien gÃ©nÃ©rÃ©:

```bash
python web_demo.py
```

<p align="center">
    <br>
    <img src="assets/web_demo.gif" width="600" />
    <br>
<p>

### DÃ©mo CLI

Nous fournissons un exemple de dÃ©monstration CLI dans `cli_demo.py`, qui prend en charge la sortie en continu pour la gÃ©nÃ©ration. Les utilisateurs peuvent interagir avec Qwen-7B-Chat en saisissant des invites, et le modÃ¨le renvoie les sorties du modÃ¨le en mode streaming. ExÃ©cutez la commande ci-dessous:

```bash
python cli_demo.py
```

<p align="center">
    <br>
    <img src="assets/cli_demo.gif" width="600" />
    <br>
<p>
<br>

### API

Nous fournissons des mÃ©thodes pour dÃ©ployer une API locale basÃ©e sur l'API OpenAI (merci Ã  @hanpenggit). Avant de commencer, installez les paquets nÃ©cessaires:

```bash
pip install fastapi uvicorn "openai<1.0" pydantic sse_starlette
```

ExÃ©cutez ensuite la commande pour dÃ©ployer votre API:

```bash
python openai_api.py
```

Vous pouvez modifier vos arguments, par exemple, `-c` pour le nom ou le chemin du poids, `--cpu-only` pour le dÃ©ploiement CPU, etc. Si vous rencontrez des problÃ¨mes lors du lancement du dÃ©ploiement de l'API, la mise Ã  jour des paquets vers la derniÃ¨re version peut probablement les rÃ©soudre.

L'utilisation de l'API est simple. Voir l'exemple ci-dessous:

```python
import openai
openai.api_base = "http://localhost:8000/v1"
openai.api_key = "none"

# create a request activating streaming response
for chunk in openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ],
    stream=True 
    # Specifying stop words in streaming output format is not yet supported and is under development.
):
    if hasattr(chunk.choices[0].delta, "content"):
        print(chunk.choices[0].delta.content, end="", flush=True)

# create a request not activating streaming response
response = openai.ChatCompletion.create(
    model="Qwen",
    messages=[
        {"role": "user", "content": "ä½ å¥½"}
    ],
    stream=False,
    stop=[] # You can add custom stop words here, e.g., stop=["Observation:"] for ReAct prompting.
)
print(response.choices[0].message.content)
```

<p align="center">
    <br>
    <img src="assets/openai_api.gif" width="600" />
    <br>
<p>

**Function calling** est aussi supportÃ© (mais seulement quand `stream=False` pour le moment). Voir [l'exemple d'utilisation](examples/function_call_examples.py) ici.
<br><br>


## ğŸ³ Docker

Pour simplifier le processus de dÃ©ploiement, nous fournissons des images docker avec des environnements prÃ©construits : [qwenllm/qwen] (https://hub.docker.com/r/qwenllm/qwen). Il vous suffit d'installer le pilote et de tÃ©lÃ©charger les fichiers de modÃ¨le pour lancer les dÃ©monstrations, dÃ©ployer l'API OpenAI et affiner le modÃ¨le.

### PrÃ©paration

1. Installez la version correcte du pilote Nvidia en fonction de l'image Ã  utiliser :
  - `qwenllm/qwen:cu117` (**recommandÃ©**): `>= 515.48.07`
  - `qwenllm/qwen:cu114` (w/o flash-attention): `>= 470.82.01`
  - `qwenllm/qwen:cu121`: `>= 530.30.02`
  - `qwenllm/qwen:latest`: mÃªme que `qwenllm/qwen:cu117`

2. Installer et configurer [docker](https://docs.docker.com/engine/install/) et [nvidia-container-toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) :

```bash
# configure docker
sudo systemctl start docker
# test if docker is correctly installed
sudo docker run hello-world

# configure nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
# test if nvidia-container-toolkit is correctly installed
sudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi
```

3. TÃ©lÃ©chargez les checkpoints et les codes du modÃ¨le dans votre environnement (voir [ici](#DownloadModel)).

### DÃ©ploiement

Nous utilisons ici Qwen-7B-Chat comme exemple. Avant de lancer une dÃ©mo web ou une API, vous pouvez Ã©tablir la configuration comme indiquÃ© ci-dessous :

```bash
IMAGE_NAME=qwenllm/qwen:cu117
PORT=8901
CHECKPOINT_PATH=/path/to/Qwen-7B-Chat   # Path to downloaded model checkpoints and codes
```
Les scripts suivants peuvent vous aider Ã  construire :

* API OpenAI
```bash
bash docker/docker_openai_api.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}
```

* Interface Web
```bash
bash docker/docker_web_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH} --port ${PORT}
```

* DÃ©mo CLI
```bash
bash docker/docker_cli_demo.sh -i ${IMAGE_NAME} -c ${CHECKPOINT_PATH}
```

Les commandes ci-dessus tÃ©lÃ©chargeront automatiquement l'image requise et lanceront une dÃ©mo d'interface Web en arriÃ¨re-plan (le service redÃ©marrera automatiquement). Vous pouvez ouvrir `http://localhost:${PORT}` sur l'hÃ´te pour utiliser la dÃ©mo.

La dÃ©mo est lancÃ©e avec succÃ¨s si vous obtenez le rÃ©sultat suivant :

```text
Successfully started web demo. Open '...' to try!
Run `docker logs ...` to check demo status.
Run `docker rm -f ...` to stop and remove the demo.
```

Si vous voulez vÃ©rifier le statut de la dÃ©mo, vous pouvez utiliser `docker logs qwen` pour afficher les rÃ©sultats.

Vous pouvez utiliser `docker rm -f qwen` pour arrÃªter le service et supprimer le conteneur.


### Finetuning

La mÃ©thode de finetuning utilisant l'image Docker prÃ©construite est fondamentalement la mÃªme que [le chapitre ci-dessus](#Finetuning) (nous avons dÃ©jÃ  installÃ© les dÃ©pendances dans l'image) :

Voici un exemple de LoRA Ã  une seule GPU :
```bash
IMAGE_NAME=qwenllm/qwen:cu117
CHECKPOINT_PATH=/path/to/Qwen-7B                # Path to downloaded model checkpoints and codes
#CHECKPOINT_PATH=/path/to/Qwen-7B-Chat-Int4     # Path to downloaded model checkpoints and codes (Q-LoRA)
DATA_PATH=/path/to/data/root                    # Prepare finetune data at ${DATA_PATH}/example.json
OUTPUT_PATH=/path/to/output/checkpoint          # Path to finetune outputs

# Use all host devices by default
DEVICE=all
# If you need to specify GPUs for training, set device as follow (NOTE: internal quotation marks cannot be omitted)
#DEVICE='"device=0,1,2,3"'

mkdir -p ${OUTPUT_PATH}

# Single-GPU LoRA finetuning
docker run --gpus ${DEVICE} --rm --name qwen \
    --mount type=bind,source=${CHECKPOINT_PATH},target=/data/shared/Qwen/Qwen-7B \
    --mount type=bind,source=${DATA_PATH},target=/data/shared/Qwen/data \
    --mount type=bind,source=${OUTPUT_PATH},target=/data/shared/Qwen/output_qwen \
    --shm-size=2gb \
    -it ${IMAGE_NAME} \
    bash finetune/finetune_lora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B/ -d /data/shared/Qwen/data/example.json
```

Pour faire un changement vers Q-LoRA Ã  GPU unique par exemple, il suffit de modifier la commande bash Ã  l'intÃ©rieur de `docker run` :
```bash
bash finetune/finetune_qlora_single_gpu.sh -m /data/shared/Qwen/Qwen-7B-Chat-Int4/ -d /data/shared/Qwen/data/example.json
```
<br>

## ğŸ”¥ Invite du systÃ¨me
Qwen-1.8-Chat et Qwen-72B-Chat ont Ã©tÃ© entiÃ¨rement formÃ©s Ã  diverses invites de systÃ¨me avec plusieurs sÃ©ries d'interactions complexes, de sorte qu'ils peuvent suivre une variÃ©tÃ© d'invites de systÃ¨me et rÃ©aliser la personnalisation du modÃ¨le dans le contexte, amÃ©liorant ainsi l'Ã©volutivitÃ© de Qwen-chat.

GrÃ¢ce aux messages-guides du systÃ¨me, Qwen-Chat peut **jouer avec enthousiasme**, **transfÃ©rer le style de langage**, **fixer des tÃ¢ches** et **fixer des comportements**.

![](assets/system_prompt_language_style.png)

![](assets/system_prompt_role_play_en.png)

Pour plus d'informations, veuillez vous rÃ©fÃ©rer Ã  la [documentation d'exemple](examples/system_prompt.md).


## Utilisation des outils

Qwen-Chat a Ã©tÃ© optimisÃ© pour l'utilisation d'outils et les capacitÃ©s d'appel de fonctions. Les utilisateurs peuvent dÃ©velopper des agents, des applications LangChain, et mÃªme augmenter Qwen avec un Code Interpreter.

Nous fournissons une documentation sur la maniÃ¨re d'implÃ©menter les appels d'outils basÃ©s sur le principe de ReAct Prompting, veuillez vous rÃ©fÃ©rer Ã  [l'exemple ReAct](examples/react_prompt.md). Sur la base de ce principe, nous fournissons un support pour function calling dans [openai_api.py](openai_api.py).

Nous avons testÃ© les capacitÃ©s d'appel d'outil du modÃ¨le sur notre benchmark d'Ã©valuation chinois Ã  source ouverte et nous avons constatÃ© que Qwen-Chat obtient systÃ©matiquement de bons rÃ©sultats:

<table>
    <tr>
        <th colspan="4" align="center">Chinese Tool-Use Benchmark (Version 20231206)</th>
    </tr>
    <tr>
        <th align="center">Model</th><th align="center">Tool Selection (Acc.â†‘)</th><th align="center">Tool Input (Rouge-Lâ†‘)</th><th align="center">False Positive Errorâ†“</th>
    </tr>
    <tr>
        <td>GPT-4</td><td align="center">98.0%</td><td align="center">0.953</td><td align="center">23.9%</td>
    </tr>
    <tr>
        <td>GPT-3.5</td><td align="center">74.5%</td><td align="center">0.807</td><td align="center">80.6%</td>
    </tr>
    <tr>
        <td>Qwen-1_8B-Chat</td><td align="center">85.0%</td><td align="center">0.839</td><td align="center">27.6%</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td><td align="center">95.5%</td><td align="center">0.900</td><td align="center">11.6%</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td><td align="center">96.9%</td><td align="center">0.917</td><td align="center">5.6%</td>
    </tr>
    <tr>
        <td>Qwen-72B-Chat</td><td align="center">98.2%</td><td align="center">0.927</td><td align="center">1.1%</td>
    </tr>
</table>

Pour Ã©valuer la capacitÃ© de Qwen Ã  utiliser l'interprÃ©teur de code Python pour des tÃ¢ches telles que la rÃ©solution de problÃ¨mes mathÃ©matiques, la visualisation de donnÃ©es et d'autres tÃ¢ches gÃ©nÃ©rales telles que la manipulation de fichiers et l'exploration du Web, nous avons crÃ©Ã© et mis en libre accÃ¨s un test de rÃ©fÃ©rence spÃ©cialement conÃ§u pour Ã©valuer ces capacitÃ©s. Vous pouvez trouver le benchmark sur ce [lien](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark).

Nous avons observÃ© que Qwen est performant en termes d'exÃ©cutabilitÃ© du code et de prÃ©cision des rÃ©sultats lors de la gÃ©nÃ©ration du code:

<table>
    <tr>
        <th colspan="5" align="center">Code Interpreter Benchmark (Version 20231206)</th>
    </tr>
    <tr>
        <th rowspan="2" align="center">Model</th>
        <th colspan="3" align="center">Accuracy of Code Execution Results (%)</th>
        <th colspan="1" align="center">Executable Rate of Code (%)</th>
    </tr>
    <tr>
        <th align="center">Mathâ†‘</th><th align="center">Visualization-Hardâ†‘</th><th align="center">Visualization-Easyâ†‘</th><th align="center">Generalâ†‘</th>
    </tr>
    <tr>
        <td>GPT-4</td>
        <td align="center">82.8</td>
        <td align="center">66.7</td>
        <td align="center">60.8</td>
        <td align="center">82.8</td>
    </tr>
    <tr>
        <td>GPT-3.5</td>
        <td align="center">47.3</td>
        <td align="center">33.3</td>
        <td align="center">55.7</td>
        <td align="center">74.1</td>
    </tr>
    <tr>
        <td>LLaMA2-13B-Chat</td>
        <td align="center">8.3</td>
        <td align="center">1.2</td>
        <td align="center">15.2</td>
        <td align="center">48.3</td>
    </tr>
    <tr>
        <td>CodeLLaMA-13B-Instruct</td>
        <td align="center">28.2</td>
        <td align="center">15.5</td>
        <td align="center">21.5</td>
        <td align="center">74.1</td>
    </tr>
    <tr>
        <td>InternLM-20B-Chat</td>
        <td align="center">34.6</td>
        <td align="center">10.7</td>
        <td align="center">25.1</td>
        <td align="center">65.5</td>
    </tr>
    <tr>
        <td>ChatGLM3-6B</td>
        <td align="center">54.2</td>
        <td align="center">4.8</td>
        <td align="center">15.2</td>
        <td align="center">67.1</td>
    </tr>
    <tr>
        <td>Qwen-1.8B-Chat</td>
        <td align="center">25.6</td>
        <td align="center">21.4</td>
        <td align="center">22.8</td>
        <td align="center">65.5</td>
    </tr>
    <tr>
        <td>Qwen-7B-Chat</td>
        <td align="center">41.9</td>
        <td align="center">23.8</td>
        <td align="center">38.0</td>
        <td align="center">67.2</td>
    </tr>
    <tr>
        <td>Qwen-14B-Chat</td>
        <td align="center">58.4</td>
        <td align="center">31.0</td>
        <td align="center">45.6</td>
        <td align="center">65.5</td>
    </tr>
    <tr>
        <td>Qwen-72B-Chat</td>
        <td align="center">72.7</td>
        <td align="center">41.7</td>
        <td align="center">43.0</td>
        <td align="center">82.8</td>
    </tr>
</table>

<p align="center">
    <br>
    <img src="assets/code_interpreter_showcase_001.jpg" />
    <br>
<p>

<br>

## ComprÃ©hension du Contexte Long

Pour augmenter la longueur du contexte et Ã©liminer le goulot d'Ã©tranglement que constitue la longueur de la sÃ©quence d'entraÃ®nement, nous introduisons plusieurs techniques, notamment l'interpolation tenant compte des NTK, l'attention par fenÃªtre et la mise Ã  l'Ã©chelle de l'attention LogN, afin d'augmenter la longueur du contexte de Qwen-14B de 2K Ã  plus de 8K tokens, et de Qwen-1.8B/7B de 8K Ã  32K tokens. 

Pour Qwen-72B, nous adaptons RoPE Ã  des contextes plus longs avec une base rotative plus importante. Qwen-72B prend en charge la longueur de contexte maximale de 32K tokens.

Nous menons des expÃ©riences de modÃ©lisation du langage sur l'ensemble de donnÃ©es arXiv avec l'Ã©valuation PPL et nous constatons que Qwen peut atteindre des performances exceptionnelles dans le scÃ©nario d'un contexte long. Les rÃ©sultats sont prÃ©sentÃ©s ci-dessous :

<table>
    <tr>
        <th rowspan="2">Model</th><th colspan="6" align="center">Sequence Length</th>
    </tr>
    <tr>
        <th align="center">1024</th><th align="center">2048</th><th align="center">4096</th><th align="center">8192</th><th align="center">16384</th><th align="center">32768</th>
    </tr>
     <tr>
        <td>Qwen-7B (original)</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">39.35</td><td align="center">469.81</td><td align="center">2645.09</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.59</td><td align="center">3.66</td><td align="center">5.71</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.58</td><td align="center">3.56</td><td align="center">4.62</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center">4.23</td><td align="center">3.78</td><td align="center">3.58</td><td align="center">3.49</td><td align="center">4.32</td><td align="center">-</td>
    </tr>
    <tr>
    <tr>
        <td>Qwen-1.8B</td><td align="center"><b>5.00</b></td><td align="center"><b>4.48</b></td><td align="center"><b>4.13</b></td><td align="center"><b>3.89</b></td><td align="center">17.42</td><td align="center">433.85</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>5.00</b></td><td align="center"><b>4.48</b></td><td align="center"><b>4.14</b></td><td align="center"><b>3.93</b></td><td align="center"><b>3.82</b></td><td align="center"><b>3.83</b></td>
    </tr>
    <tr>
        <td>Qwen-7B</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.31</b></td><td align="center">7.27</td><td align="center">181.49</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>4.23</b></td><td align="center"><b>3.81</b></td><td align="center"><b>3.52</b></td><td align="center"><b>3.33</b></td><td align="center"><b>3.22</b></td><td align="center"><b>3.17</b></td>
    </tr>
    <tr>
        <td>Qwen-14B</td><td align="center"><b>-</b></td><td align="center"><b>3.46</b></td><td align="center">22.79</td><td align="center">334.65</td><td align="center">3168.35</td><td align="center">-</td>
    </tr>
    <tr>
        <td>+ dynamic_ntk + logn + window_attn</td><td align="center"><b>-</b></td><td align="center"><b>3.46</b></td><td align="center"><b>3.29</b></td><td align="center"><b>3.18</b></td><td align="center">3.42</td><td align="center">-</td>
    </tr>
    <tr>
        <td>Qwen-72B</td><td align="center"><b>-</b></td><td align="center"><b>-</b></td><td align="center">-</td><td align="center"><b>2.83</b></td><td align="center"><b>2.73</b></td><td align="center"><b>2.72</b></td>
    </tr>
    </tr>
</table>

En outre, pour vÃ©rifier la capacitÃ© de Qwen-72B-Chat Ã  comprendre des textes longs, nous l'avons testÃ© sur [L-Eval] (https://arxiv.org/abs/2307.11088) (tÃ¢ches fermÃ©es). Les rÃ©sultats sont les suivants :

| Model             | Input Length | Average   |  Coursera  |    GSM     |   QuALITY  |    TOEFL   |   CodeU    |  SFcition  |
|:------------------|:------------:|:---------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|
| ChatGPT-3.5-16k   |     16K      |   60.73   | **63.51**  | **84.00**  |   61.38    |    78.43   | **12.22**  |    64.84   |
| **Qwen-72B-Chat** |     32K      | **62.30** |   58.13    |   76.00    | **77.22**  |  **86.24** |    6.66    |  **69.53** |

Nous avons rÃ©alisÃ© l'expÃ©rience de "l'aiguille dans une botte de foin" (l'idÃ©e vient de [@Greg Kamradt](https://twitter.com/GregKamradt/status/1727018183608193393)) pour tester si le modÃ¨le peut rÃ©cupÃ©rer des informations Ã  diffÃ©rentes positions dans les entrÃ©es de diffÃ©rentes longueurs, le rÃ©sultat est le suivant :

![](assets/qwen_72b_needle_in_a_haystack.png)

Les rÃ©sultats ci-dessus montrent que Qwen-72B-Chat peut rÃ©cupÃ©rer avec prÃ©cision des informations placÃ©es dans diffÃ©rentes positions dans une longueur d'entrÃ©e de 32K, ce qui prouve ses excellentes capacitÃ©s de comprÃ©hension de textes longs.


## Tokenizer

Notre tokenizer basÃ© sur tiktoken est diffÃ©rent des autres tokenizers, par exemple le tokenizer sentencepiece. Vous devez faire attention aux tokens spÃ©ciaux, en particulier lors de la mise au point. Pour des informations plus dÃ©taillÃ©es sur le tokenizer et son utilisation dans le cadre du finetuning, veuillez vous rÃ©fÃ©rer Ã  la [documentation](tokenization_note.md).
<br><br>

## Reproduction

Pour reproduire les performances du modÃ¨le sur des ensembles de donnÃ©es de rÃ©fÃ©rence, nous fournissons des scripts permettant de reproduire les rÃ©sultats. Consultez [eval/EVALUATION.md](eval/EVALUATION.md) pour plus d'informations. Notez que la reproduction peut entraÃ®ner de lÃ©gÃ¨res diffÃ©rences par rapport Ã  nos rÃ©sultats.
<br><br>

## FAQ

Si vous rencontrez des problÃ¨mes, veuillez vous rÃ©fÃ©rer Ã  la [FAQ](FAQ.md) et aux problÃ¨mes pour trouver une solution avant de lancer un nouveau problÃ¨me.
<br><br>

## Citation
Si vous trouvez notre travail utile, n'hÃ©sitez pas Ã  nous citer.

```
@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
```
<br>

## Accord de Licence

Le code source fourni Ã  l'adresse <https://github.com/QwenLM/Qwen> est soumis Ã  la licence [Apache 2.0 License](./LICENSE) qui se trouve dans le rÃ©pertoire racine.

Les chercheurs et les dÃ©veloppeurs sont libres d'utiliser les codes et les poids des modÃ¨les de Qwen et de Qwen-Chat. Pour leur utilisation commerciale, veuillez consulter l'accord de licence accompagnant chaque modÃ¨le.

- Qwen-72B, Qwen-14B et Qwen-7B sont sous licence [Tongyi Qianwen LICENSE AGREEMENT](./Tongyi%20Qianwen%20LICENSE%20AGREEMENT) que l'on peut trouver dans les dÃ©pÃ´ts HuggingFace et ModelScope correspondants. Pour une utilisation commerciale, veuillez remplir le formulaire ([72B](https://dashscope.console.aliyun.com/openModelApply/Qwen-72B-Chat), [14B](https://dashscope.console.aliyun.com/openModelApply/Qwen-14B-Chat), et [7B](https://dashscope.console.aliyun.com/openModelApply/qianwen)) pour en faire la demande.

- Qwen-1.8B est sous licence [Tongyi Qianwen RESEARCH LICENSE AGREEMENT](./Tongyi%20Qianwen%20RESEARCH%20LICENSE%20AGREEMENT) qui peut Ãªtre trouvÃ© dans les dÃ©pÃ´ts HuggingFace et ModelScope correspondants. Pour une utilisation commerciale, veuillez nous contacter.
<br><br>

## Contactez-nous

Si vous souhaitez laisser un message Ã  notre Ã©quipe de recherche ou Ã  notre Ã©quipe produit, rejoignez nos groupes Discord ou WeChat! N'hÃ©sitez pas non plus Ã  envoyer un courriel Ã  qianwen_opensource@alibabacloud.com.

